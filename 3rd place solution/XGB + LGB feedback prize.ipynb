{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(\"/kaggle/input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from cuml import ForestInference\n",
    "\n",
    "\n",
    "discourses = ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement', 'Counterclaim', 'Rebuttal']\n",
    "xgb_models, lgb_models = dict(), dict()\n",
    "\n",
    "ensemble_weights = {\"Rebuttal\": 0.65,\n",
    "                    \"Counterclaim\": 0.75,\n",
    "                    \"Concluding Statement\": 0.60,\n",
    "                    \"Claim\": 0.65,\n",
    "                    \"Evidence\": 0.60,\n",
    "                    \"Position\": 0.75,\n",
    "                    \"Lead\": 0.70\n",
    "                    }\n",
    "\n",
    "\n",
    "thresholds = {'Lead': 0.66,\n",
    " 'Position': 0.56,\n",
    " 'Evidence': 0.57,\n",
    " 'Claim': 0.54,\n",
    " 'Concluding Statement': 0.56,\n",
    " 'Counterclaim': 0.7,\n",
    " 'Rebuttal': 0.74}\n",
    "\n",
    "features_dict = {'Lead': [i for i in range(34)],\n",
    " 'Position': [i for i in range(34)],\n",
    " 'Evidence': [i for i in range(20)],\n",
    " 'Claim': [i for i in range(20)],\n",
    " 'Concluding Statement': [i for i in range(34)],\n",
    " 'Counterclaim': [i for i in range(17)] + [i for i in range(27, 34)],\n",
    " 'Rebuttal': [i for i in range(17)]}\n",
    "\n",
    "N_XGB_FOLDS = 5\n",
    "\n",
    "for d in discourses:\n",
    "    model_list = []\n",
    "    for f in range(N_XGB_FOLDS):\n",
    "        xgb_model = ForestInference.load(f\"../input/student-writing-7322/xgb_{d}_{f}.json\", output_class=True, model_type=\"xgboost_json\")\n",
    "        model_list.append(xgb_model)\n",
    "    xgb_models[d] = model_list\n",
    "\n",
    "    model_list = []\n",
    "    for f in range(N_XGB_FOLDS):\n",
    "        lgb_model = ForestInference.load(f\"../input/student-writing-7322/lgb_{d}_{f}.txt\", output_class=True, model_type=\"lightgbm\")\n",
    "        model_list.append(lgb_model)\n",
    "    lgb_models[d] = model_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_tp_prob(testDs, disc_type):\n",
    "\n",
    "    if testDs.features.shape[0] == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    pred = np.mean([clf.predict_proba(testDs.features[:, features_dict[disc_type]].astype(\"float32\"))[:,1] for clf in xgb_models[disc_type]], axis=0)/2\n",
    "    pred += np.mean([clf.predict_proba(testDs.features[:, features_dict[disc_type]].astype(\"float32\"))[:, 1] for clf in lgb_models[disc_type]], axis=0)/2\n",
    "\n",
    "    return pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# DECLARE HOW MANY GPUS YOU WISH TO USE.\n",
    "# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu\n",
    "\n",
    "# IF VARIABLE IS NONE, THEN NOTEBOOK COMPUTES TOKENS\n",
    "# OTHERWISE NOTEBOOK LOADS TOKENS FROM PATH\n",
    "LOAD_TOKENS_FROM = '../input/py-bigbird-v26'\n",
    "\n",
    "# IF VARIABLE IS NONE, THEN NOTEBOOK TRAINS A NEW MODEL\n",
    "# OTHERWISE IT LOADS YOUR PREVIOUSLY TRAINED MODEL\n",
    "LOAD_MODEL_FROM = '../input/fp-test78'\n",
    "\n",
    "# IF FOLLOWING IS NONE, THEN NOTEBOOK\n",
    "# USES INTERNET AND DOWNLOADS HUGGINGFACE\n",
    "# CONFIG, TOKENIZER, AND MODEL\n",
    "DOWNLOADED_MODEL_PATH = '../input/deberta-xlarge'\n",
    "\n",
    "\n",
    "# A cache of the BigBird predictions for the validation/sequence training set and the corresponding sequence dataset\n",
    "KAGGLE_CACHE = '../input/feedbackcache2'\n",
    "\n",
    "N_FEATURES=34\n",
    "\n",
    "TEST_PERCENT = None\n",
    "\n",
    "cache = 'cache'\n",
    "cacheExists = os.path.exists(cache)\n",
    "if not cacheExists:\n",
    "  os.makedirs(cache)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "config = {'model_name': '',\n",
    "         'max_length': 2048,\n",
    "         'train_batch_size':4,\n",
    "         'valid_batch_size':4,\n",
    "         'epochs':5,\n",
    "         'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n",
    "         'max_grad_norm':10,\n",
    "         'device': 'cuda' if cuda.is_available() else 'cpu'}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np, os\n",
    "from scipy import stats\n",
    "import pandas as pd, gc\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AdamW\n",
    "from transformers import *\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.cuda import amp\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', '.*__floordiv__ is deprecated.*',)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n",
    "test_names, test_texts = [], []\n",
    "for f in list(os.listdir('../input/feedback-prize-2021/test')):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    test_texts.append(open('../input/feedback-prize-2021/test/' + f, 'r').read())\n",
    "test_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\n",
    "\n",
    "if TEST_PERCENT is not None:\n",
    "    print(f\"testing and submitting with only {TEST_PERCENT} of test data\")\n",
    "    np.random.seed(2022)\n",
    "    test_select=np.arange(len(test_texts))\n",
    "    np.random.shuffle(test_select)\n",
    "    test_texts=test_texts.iloc[test_select[:int(TEST_PERCENT*len(test_texts))]].reset_index()\n",
    "\n",
    "#sort by length of texts to minimize padding in each batch\n",
    "test_texts['len']=test_texts['text'].apply(lambda x:len(x.split()))\n",
    "test_texts=test_texts.sort_values(by=['len']).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_texts\n",
    "\n",
    "SUBMISSION = True\n",
    "if len(test_names) > 5:\n",
    "      SUBMISSION = True\n",
    "\n",
    "test_texts.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convert Train Text to NER Labels\n",
    "We will now convert all text words into NER labels and save in a dataframe."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CREATE DICTIONARIES THAT WE CAN USE DURING TRAIN AND INFER\n",
    "output_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim',\n",
    "          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n",
    "\n",
    "labels_to_ids = {v:k for k,v in enumerate(output_labels)}\n",
    "ids_to_labels = {k:v for k,v in enumerate(output_labels)}\n",
    "disc_type_to_ids = {'Evidence':(11,12),'Claim':(5,6),'Lead':(1,2),'Position':(3,4),'Counterclaim':(7,8),'Rebuttal':(9,10),'Concluding Statement':(13,14)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_to_ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Define the dataset function\n",
    "\n",
    "Below is our PyTorch dataset function. It always outputs tokens and attention. During training it also provides labels. And during inference it also provides word ids to help convert token predictions into word predictions.\n",
    "\n",
    "Note that we use text.split() and is_split_into_words=True when we convert train text to labeled train tokens. This is how the HugglingFace tutorial does it. However, this removes characters like \\n new paragraph. If you want your model to see new paragraphs, then we need to map words to tokens ourselves using return_offsets_mapping=True.\n",
    "\n",
    "See my TensorFlow notebook here for an example: https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-617\n",
    "\n",
    "Some of the following code comes from the example at HuggingFace here. https://huggingface.co/docs/transformers/custom_datasets#tok_ner\n",
    "\n",
    "However I think the code at that link is wrong. The HuggingFace original code is here: https://github.com/huggingface/transformers/blob/86b40073e9aee6959c8c85fcba89e47b432c4f4d/examples/pytorch/token-classification/run_ner.py#L371\n",
    "\n",
    "With the flag LABEL_ALL we can either label just the first subword token (when one word has more than one subword token). Or we can label all the subword tokens (with the word's label). In this notebook version, we label all the tokens.\n",
    "\n",
    "There is a Kaggle discussion here: https://www.kaggle.com/c/feedback-prize-2021/discussion/296713"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Return an array that maps character index to index of word in list of split() words\n",
    "def split_mapping(unsplit):\n",
    "    splt = unsplit.split()\n",
    "    offset_to_wordidx = np.full(len(unsplit),-1)\n",
    "    txt_ptr = 0\n",
    "    for split_index, full_word in enumerate(splt):\n",
    "        while unsplit[txt_ptr:txt_ptr + len(full_word)] != full_word:\n",
    "            txt_ptr += 1\n",
    "        offset_to_wordidx[txt_ptr:txt_ptr + len(full_word)] = split_index\n",
    "        txt_ptr += len(full_word)\n",
    "    return offset_to_wordidx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "  def __init__(self, dataframe, tokenizer, max_len, get_wids):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.get_wids = get_wids # for validation\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        # GET TEXT AND WORD LABELS\n",
    "        text = self.data.text[index]\n",
    "        word_labels = self.data.entities[index] if not self.get_wids else None\n",
    "\n",
    "        # TOKENIZE TEXT\n",
    "        encoding = self.tokenizer(text,\n",
    "                             return_offsets_mapping=True,\n",
    "                             padding=False,\n",
    "                             truncation=True,\n",
    "                             max_length=self.max_len)\n",
    "\n",
    "        word_ids = encoding.word_ids()\n",
    "        split_word_ids = np.full(len(word_ids),-1)\n",
    "        offset_to_wordidx = split_mapping(text)\n",
    "        offsets = encoding['offset_mapping']\n",
    "\n",
    "        # CREATE TARGETS AND MAPPING OF TOKENS TO SPLIT() WORDS\n",
    "        label_ids = []\n",
    "        # Iterate in reverse to label whitespace tokens until a Begin token is encountered\n",
    "        for token_idx, word_idx in reversed(list(enumerate(word_ids))):\n",
    "\n",
    "            if word_idx is None:\n",
    "                if not self.get_wids: label_ids.append(-100)\n",
    "            else:\n",
    "                if offsets[token_idx][0] != offsets[token_idx][1]:\n",
    "                    #Choose the split word that shares the most characters with the token if any\n",
    "                    split_idxs = offset_to_wordidx[offsets[token_idx][0]:offsets[token_idx][1]]\n",
    "                    split_index = stats.mode(split_idxs[split_idxs != -1]).mode[0] if len(np.unique(split_idxs)) > 1 else split_idxs[0]\n",
    "\n",
    "                    if split_index != -1:\n",
    "                        if not self.get_wids: label_ids.append( labels_to_ids[word_labels[split_index]] )\n",
    "                        split_word_ids[token_idx] = split_index\n",
    "                    else:\n",
    "                        # Even if we don't find a word, continue labeling 'I' tokens until a 'B' token is found\n",
    "                        if label_ids and label_ids[-1] != -100 and ids_to_labels[label_ids[-1]][0] == 'I':\n",
    "                            split_word_ids[token_idx] = split_word_ids[token_idx + 1]\n",
    "                            if not self.get_wids: label_ids.append(label_ids[-1])\n",
    "                        else:\n",
    "                            if not self.get_wids: label_ids.append(-100)\n",
    "                else:\n",
    "                    if not self.get_wids: label_ids.append(-100)\n",
    "\n",
    "        encoding['labels'] = list(reversed(label_ids))\n",
    "\n",
    "        # CONVERT TO TORCH TENSORS\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        if self.get_wids:\n",
    "            item['wids'] = torch.as_tensor(split_word_ids)\n",
    "\n",
    "        return item\n",
    "\n",
    "  def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class CustomCollate:\n",
    "    def __init__(self,tokenizer,sliding_window=None):\n",
    "        self.tokenizer=tokenizer\n",
    "        self.sliding_window=sliding_window\n",
    "\n",
    "    def __call__(self,data):\n",
    "        \"\"\"\n",
    "        need to collate: input_ids, attention_mask, labels\n",
    "        input_ids is padded with 1, attention_mask 0, labels -100\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        bs=len(data)\n",
    "        lengths=[]\n",
    "        for i in range(bs):\n",
    "            lengths.append(len(data[i]['input_ids']))\n",
    "            # print(data[i]['input_ids'].shape)\n",
    "            # print(data[i]['attention_mask'].shape)\n",
    "            # print(data[i]['labels'].shape)\n",
    "        max_len=max(lengths)\n",
    "        if self.sliding_window is not None and max_len > self.sliding_window:\n",
    "            max_len= int((np.floor(max_len/self.sliding_window-1e-6)+1)*self.sliding_window)\n",
    "        #always pad the right side\n",
    "        input_ids, attention_mask, labels, BIO_labels, discourse_labels=[],[],[],[],[]\n",
    "        #if np.random.uniform()>0.5:\n",
    "        #print(data[0].keys())\n",
    "        #print(max_len)\n",
    "        if 'wids' in data[0]:\n",
    "            get_wids=True\n",
    "        else:\n",
    "            get_wids=False\n",
    "        #print(get_wids)\n",
    "        wids = []\n",
    "            #wids.append(torch.nn.functional.pad(data[i]['wids'],(0,max_len-lengths[i]),value=-1))\n",
    "        for i in range(bs):\n",
    "            input_ids.append(torch.nn.functional.pad(data[i]['input_ids'],(0,max_len-lengths[i]),value=self.tokenizer.pad_token_id))\n",
    "            attention_mask.append(torch.nn.functional.pad(data[i]['attention_mask'],(0,max_len-lengths[i]),value=0))\n",
    "            #labels.append(torch.nn.functional.pad(data[i]['labels'],(0,max_len-lengths[i]),value=-100))\n",
    "            #BIO_labels.append(torch.nn.functional.pad(data[i]['BIO_labels'],(0,max_len-lengths[i]),value=-100))\n",
    "            #discourse_labels.append(torch.nn.functional.pad(data[i]['discourse_labels'],(0,max_len-lengths[i]),value=-100))\n",
    "            if get_wids:\n",
    "                wids.append(torch.nn.functional.pad(data[i]['wids'],(0,max_len-lengths[i]),value=-1))\n",
    "        # else:\n",
    "        #     for i in range(bs):\n",
    "        #         input_ids.append(torch.nn.functional.pad(data[i]['input_ids'],(max_len-lengths[i],0),value=1))\n",
    "        #         attention_mask.append(torch.nn.functional.pad(data[i]['attention_mask'],(max_len-lengths[i],0),value=0))\n",
    "        #         labels.append(torch.nn.functional.pad(data[i]['labels'],(max_len-lengths[i],0),value=-100))\n",
    "\n",
    "        input_ids=torch.stack(input_ids)\n",
    "        attention_mask=torch.stack(attention_mask)\n",
    "        #labels=torch.stack(labels)\n",
    "        #BIO_labels=torch.stack(BIO_labels)\n",
    "        #discourse_labels=torch.stack(discourse_labels)\n",
    "        if get_wids:\n",
    "            wids=torch.stack(wids)\n",
    "        #exit()\n",
    "        if get_wids:\n",
    "            return {\"input_ids\":input_ids,\"attention_mask\":attention_mask,\n",
    "            \"labels\":labels,\"BIO_labels\":BIO_labels,\"discourse_labels\":discourse_labels,\n",
    "            \"wids\":wids}\n",
    "        else:\n",
    "            return {\"input_ids\":input_ids,\"attention_mask\":attention_mask,\n",
    "            \"labels\":labels,\"BIO_labels\":BIO_labels,\"discourse_labels\":discourse_labels}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_params = {'batch_size': config['valid_batch_size'],\n",
    "                'shuffle': False,\n",
    "                'num_workers': 2,\n",
    "                'pin_memory':True\n",
    "                }\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DOWNLOADED_MODEL_PATH)\n",
    "\n",
    "\n",
    "# TEST DATASET\n",
    "test_texts_set = dataset(test_texts, tokenizer, config['max_length'], True)\n",
    "test_texts_loader = DataLoader(test_texts_set, **test_params,collate_fn=CustomCollate(tokenizer,512))\n",
    "\n",
    "tokenizer_longformer = AutoTokenizer.from_pretrained(\"../input/pytorch-longformer-large\")\n",
    "test_texts_set_longformer = dataset(test_texts, tokenizer_longformer, config['max_length'], True)\n",
    "test_texts_loader_longformer = DataLoader(test_texts_set_longformer, **test_params,collate_fn=CustomCollate(tokenizer))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "rearrange_indices=[14, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
    "class ResidualLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model,rnn):\n",
    "        super(ResidualLSTM, self).__init__()\n",
    "        self.downsample=nn.Linear(d_model,d_model//2)\n",
    "        if rnn=='GRU':\n",
    "            self.LSTM=nn.GRU(d_model//2, d_model//2, num_layers=2, bidirectional=False, dropout=0.2)\n",
    "        else:\n",
    "            self.LSTM=nn.LSTM(d_model//2, d_model//2, num_layers=2, bidirectional=False, dropout=0.2)\n",
    "        self.dropout1=nn.Dropout(0.2)\n",
    "        self.norm1= nn.LayerNorm(d_model//2)\n",
    "        self.linear1=nn.Linear(d_model//2, d_model*4)\n",
    "        self.linear2=nn.Linear(d_model*4, d_model)\n",
    "        self.dropout2=nn.Dropout(0.2)\n",
    "        self.norm2= nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res=x\n",
    "        x=self.downsample(x)\n",
    "        x, _ = self.LSTM(x)\n",
    "        x=self.dropout1(x)\n",
    "        x=self.norm1(x)\n",
    "        x=F.relu(self.linear1(x))\n",
    "        x=self.linear2(x)\n",
    "        x=self.dropout2(x)\n",
    "        x=res+x\n",
    "        return self.norm2(x)\n",
    "\n",
    "\n",
    "class ConvLSTMHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvLSTMHead, self).__init__()\n",
    "        self.downsample=nn.Sequential(nn.Linear(1024,256))\n",
    "        self.conv1=  nn.Sequential(nn.Conv1d(256,256,3,padding=1),\n",
    "                                  nn.ReLU())\n",
    "        self.norm1 = nn.LayerNorm(256)\n",
    "        self.conv2=  nn.Sequential(nn.Conv1d(256,256,3,padding=1),\n",
    "                                  nn.ReLU())\n",
    "        self.norm2 = nn.LayerNorm(256)\n",
    "        #self.lstm=nn.LSTM(256,256,2,bidirectional=True)\n",
    "        self.lstm=ResidualLSTM(256)\n",
    "        self.upsample=nn.Sequential(nn.Linear(256,1024),nn.ReLU())\n",
    "        self.classification_head=nn.Sequential(nn.Linear(1024,15))\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x=self.downsample(x)\n",
    "        res=x\n",
    "        x=self.conv1(x.permute(0,2,1))\n",
    "        x=self.norm1(x.permute(0,2,1)).permute(0,2,1)\n",
    "        x=self.conv2(x)\n",
    "        x=self.norm1(x.permute(0,2,1))\n",
    "        x=x+res\n",
    "        x=self.lstm(x.permute(1,0,2))\n",
    "        x=x.permute(1,0,2)\n",
    "        x=self.upsample(x)\n",
    "        x=self.classification_head(x)\n",
    "        #print(x.shape)\n",
    "        #exit()\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self,DOWNLOADED_MODEL_PATH, rnn='LSTM'):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        config_model = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'/config.json')\n",
    "\n",
    "        self.backbone=AutoModel.from_pretrained(\n",
    "                           DOWNLOADED_MODEL_PATH+'/pytorch_model.bin',config=config_model)\n",
    "\n",
    "        self.lstm=ResidualLSTM(1024,rnn)\n",
    "        self.classification_head=nn.Linear(1024,15)\n",
    "        #self.head=nn.Sequential(nn.Linear(1024,15))\n",
    "\n",
    "        # self.downsample=nn.Sequential(nn.Linear(1024,256))\n",
    "        # self.conv1d=nn.Sequential(nn.Conv1d(256,256,3,padding=0),\n",
    "        #                           nn.ReLU(),\n",
    "        #                           nn.LayerNorm(256),\n",
    "        #                           nn.Conv1d(256,256,3,padding=1),\n",
    "        #                           nn.ReLU(),\n",
    "        #                           nn.LayerNorm(256))\n",
    "\n",
    "        #self.BIO_head=nn.Sequential(nn.Linear(1024,3))\n",
    "\n",
    "    def forward(self,x,attention_mask):\n",
    "        x=self.backbone(input_ids=x,attention_mask=attention_mask,return_dict=False)[0]\n",
    "\n",
    "        x=self.lstm(x.permute(1,0,2)).permute(1,0,2)\n",
    "        x=self.classification_head(x)\n",
    "        # x=x.permute(0,2,1)\n",
    "        # x=self.conv1d(x)\n",
    "        # print(x.shape)\n",
    "        # exit()\n",
    "        # classification_output=self.classification_head(x)\n",
    "        #BIO_output=self.BIO_head(x[0])\n",
    "        # print(x.shape)\n",
    "        # exit()\n",
    "        return [x[:,:,rearrange_indices]]#, BIO_output\n",
    "\n",
    "class SlidingWindowTransformerModel(nn.Module):\n",
    "    def __init__(self,DOWNLOADED_MODEL_PATH, rnn, window_size=512, edge_len=64):\n",
    "        super(SlidingWindowTransformerModel, self).__init__()\n",
    "        config_model = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'/config.json')\n",
    "\n",
    "        self.backbone=AutoModel.from_pretrained(\n",
    "                           DOWNLOADED_MODEL_PATH+'/pytorch_model.bin',config=config_model)\n",
    "\n",
    "        self.lstm=ResidualLSTM(1024,rnn)\n",
    "        self.classification_head=nn.Linear(1024,15)\n",
    "        self.window_size=window_size\n",
    "        self.edge_len=edge_len\n",
    "        self.inner_len=window_size-edge_len*2\n",
    "        #self.head=nn.Sequential(nn.Linear(1024,15))\n",
    "\n",
    "        # self.downsample=nn.Sequential(nn.Linear(1024,256))\n",
    "        # self.conv1d=nn.Sequential(nn.Conv1d(256,256,3,padding=0),\n",
    "        #                           nn.ReLU(),\n",
    "        #                           nn.LayerNorm(256),\n",
    "        #                           nn.Conv1d(256,256,3,padding=1),\n",
    "        #                           nn.ReLU(),\n",
    "        #                           nn.LayerNorm(256))\n",
    "\n",
    "        #self.BIO_head=nn.Sequential(nn.Linear(1024,3))\n",
    "\n",
    "    def forward(self,input_ids,attention_mask):\n",
    "\n",
    "        B,L=input_ids.shape\n",
    "\n",
    "        # print(L)\n",
    "        # exit()\n",
    "        #x=self.backbone(input_ids=input_ids,attention_mask=attention_mask,return_dict=False)[0]\n",
    "        if L<=self.window_size:\n",
    "            x=self.backbone(input_ids=input_ids,attention_mask=attention_mask,return_dict=False)[0]\n",
    "            #pass\n",
    "        else:\n",
    "            #print(\"####\")\n",
    "            #print(input_ids.shape)\n",
    "            segments=(L-self.window_size)//self.inner_len\n",
    "            if (L-self.window_size)%self.inner_len>self.edge_len:\n",
    "                segments+=1\n",
    "            elif segments==0:\n",
    "                segments+=1\n",
    "            x=self.backbone(input_ids=input_ids[:,:self.window_size],attention_mask=attention_mask[:,:self.window_size],return_dict=False)[0]\n",
    "            for i in range(1,segments+1):\n",
    "                start=self.window_size-self.edge_len+(i-1)*self.inner_len\n",
    "                end=self.window_size-self.edge_len+(i-1)*self.inner_len+self.window_size\n",
    "                end=min(end,L)\n",
    "                x_next=input_ids[:,start:end]\n",
    "                mask_next=attention_mask[:,start:end]\n",
    "                x_next=self.backbone(input_ids=x_next,attention_mask=mask_next,return_dict=False)[0]\n",
    "                #L_next=x_next.shape[1]-self.edge_len,\n",
    "                if i==segments:\n",
    "                    x_next=x_next[:,self.edge_len:]\n",
    "                else:\n",
    "                    x_next=x_next[:,self.edge_len:self.edge_len+self.inner_len]\n",
    "                #print(x_next.shape)\n",
    "                x=torch.cat([x,x_next],1)\n",
    "\n",
    "                #print(start,end)\n",
    "        #print(x.shape)\n",
    "        x=self.lstm(x.permute(1,0,2)).permute(1,0,2)\n",
    "        x=self.classification_head(x)\n",
    "\n",
    "        # x=x.permute(0,2,1)\n",
    "        # x=self.conv1d(x)\n",
    "        # print(x.shape)\n",
    "        # exit()\n",
    "        # classification_output=self.classification_head(x)\n",
    "        #BIO_output=self.BIO_head(x[0])\n",
    "        # print(x.shape)\n",
    "        # exit()\n",
    "        #return x\n",
    "        return [x[:,:,rearrange_indices]]#, BIO_output\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Returns per-word, mean class prediction probability over all tokens corresponding to each word\n",
    "def inference(data_loader, model_ids, model, path):\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ensemble_preds = np.zeros((len(data_loader.dataset), config['max_length'], len(labels_to_ids)), dtype=np.float32)\n",
    "    wids = np.full((len(data_loader.dataset), config['max_length']), -100)\n",
    "    for model_i, model_id in enumerate(model_ids):\n",
    "\n",
    "        model.load_state_dict(torch.load(f'{path}/fold{model_id}.pt', map_location=config['device']))\n",
    "\n",
    "        # put model in training mode\n",
    "        model.eval()\n",
    "        for batch_i, batch in tqdm(enumerate(data_loader)):\n",
    "\n",
    "            if model_i == 0: wids[batch_i*config['valid_batch_size']:(batch_i+1)*config['valid_batch_size'],:batch['wids'].shape[1]] = batch['wids'].numpy()\n",
    "\n",
    "            # MOVE BATCH TO GPU AND INFER\n",
    "            ids = batch[\"input_ids\"].to(config['device'])\n",
    "            mask = batch[\"attention_mask\"].to(config['device'])\n",
    "            with torch.no_grad():\n",
    "                #with amp.autocast():\n",
    "                outputs = model(ids, attention_mask=mask)\n",
    "            all_preds = torch.nn.functional.softmax(outputs[0], dim=2).cpu().detach().numpy()\n",
    "            ensemble_preds[batch_i*config['valid_batch_size']:(batch_i+1)*config['valid_batch_size'],:all_preds.shape[1]] += all_preds\n",
    "\n",
    "            del ids\n",
    "            del mask\n",
    "            del outputs\n",
    "            del all_preds\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    ensemble_preds /= len(model_ids)\n",
    "    predictions = []\n",
    "    # INTERATE THROUGH EACH TEXT AND GET PRED\n",
    "    for text_i in range(ensemble_preds.shape[0]):\n",
    "        token_preds = ensemble_preds[text_i]\n",
    "\n",
    "        prediction = []\n",
    "        previous_word_idx = -1\n",
    "        prob_buffer = []\n",
    "        word_ids = wids[text_i][wids[text_i] != -100]\n",
    "        for idx,word_idx in enumerate(word_ids):\n",
    "            if word_idx == -1:\n",
    "                pass\n",
    "            elif word_idx != previous_word_idx:\n",
    "                if prob_buffer:\n",
    "                    prediction.append(np.mean(prob_buffer, dtype=np.float32, axis=0))\n",
    "                    prob_buffer = []\n",
    "                prob_buffer.append(token_preds[idx])\n",
    "                previous_word_idx = word_idx\n",
    "            else:\n",
    "                prob_buffer.append(token_preds[idx])\n",
    "        prediction.append(np.mean(prob_buffer, dtype=np.float32, axis=0))\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = SlidingWindowTransformerModel(DOWNLOADED_MODEL_PATH,'GRU').to(config['device'])\n",
    "test_word_preds = inference(test_texts_loader, [0, 1, 3, 4, 6, 7], model, LOAD_MODEL_FROM)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = TransformerModel(\"../input/pytorch-longformer-large\",'GRU').to(config['device'])\n",
    "test_word_preds2 = inference(test_texts_loader_longformer, [0, 2, 3, 4, 5, 6], model, \"../input/fp-test63\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sequence Datasets\n",
    "\n",
    "We will create datasets that, instead of describing individual words or tokens, describes sequences of words. Within some heuristic constraints, every possible sub-sequence of words in a text will converted to a dataset sample with the following attributes:\n",
    "- features- sequence length, position, and various kinds of class probability predictions/statistics\n",
    "- labels- whether the sequence matches exactly a discourse instance\n",
    "- truePos- whether the sequence matches a discourse instance by competition criteria for true positive groups- the integer index of the text where the sequence is found\n",
    "- wordRanges- the start and end word index of the sequence in the text\n",
    "\n",
    "Sequence datasets are generated for each discourse type and for validation and submission datasets.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from bisect import bisect_left\n",
    "\n",
    "# Percentile code taken from https://www.kaggle.com/vuxxxx/tensorflow-longformer-ner-postprocessing\n",
    "# Thank Vu!\n",
    "#\n",
    "# Use 99.5% of the distribution of lengths for a disourse type as maximum.\n",
    "# Increasing this constraint makes this step slower but generally increases performance.\n",
    "train_df=pd.read_csv(\"../input/feedback-prize-2021/train.csv\")\n",
    "MAX_SEQ_LEN = {}\n",
    "train_df['len'] = train_df['predictionstring'].apply(lambda x:len(x.split()))\n",
    "max_lens = train_df.groupby('discourse_type')['len'].quantile(.995)\n",
    "for disc_type in disc_type_to_ids:\n",
    "    MAX_SEQ_LEN[disc_type] = int(max_lens[disc_type])\n",
    "\n",
    "#The minimum probability prediction for a 'B'egin class for which we will evaluate a word sequence\n",
    "MIN_BEGIN_PROB = {\n",
    "    'Claim': .35*0.8,\n",
    "    'Concluding Statement': .15*1.0,\n",
    "    'Counterclaim': .04*1.25,\n",
    "    'Evidence': .1*0.8,\n",
    "    'Lead': .32*1.0,\n",
    "    'Position': .25*0.8,\n",
    "    'Rebuttal': .01*1.25,\n",
    "}\n",
    "\n",
    "class SeqDataset(object):\n",
    "\n",
    "    def __init__(self, features, labels, groups, wordRanges, truePos):\n",
    "\n",
    "        self.features = np.array(features, dtype=np.float32)\n",
    "        self.labels = np.array(labels)\n",
    "        self.groups = np.array(groups, dtype=np.int16)\n",
    "        self.wordRanges = np.array(wordRanges, dtype=np.int16)\n",
    "        self.truePos = np.array(truePos)\n",
    "\n",
    "# Adapted from https://stackoverflow.com/questions/60467081/linear-interpolation-in-numpy-quantile\n",
    "# This is used to prevent re-sorting to compute quantile for every sequence.\n",
    "def sorted_quantile(array, q):\n",
    "    array = np.array(array)\n",
    "    n = len(array)\n",
    "    index = (n - 1) * q\n",
    "    left = np.floor(index).astype(int)\n",
    "    fraction = index - left\n",
    "    right = left\n",
    "    right = right + (fraction > 0).astype(int)\n",
    "    i, j = array[left], array[right]\n",
    "    return i + (j - i) * fraction\n",
    "\n",
    "def seq_dataset(disc_type, pred_indices=None, submit=False):\n",
    "    begin_class_ids = [0, 1, 3, 5, 7, 9, 11, 13]\n",
    "    word_preds = valid_word_preds if not submit else test_word_preds\n",
    "    w = ensemble_weights[disc_type]\n",
    "\n",
    "\n",
    "    window = pred_indices if pred_indices else range(len(word_preds))\n",
    "    X = np.empty((int(1e6),N_FEATURES), dtype=np.float32)\n",
    "    X_ind = 0\n",
    "    y = []\n",
    "    truePos = []\n",
    "    wordRanges = []\n",
    "    groups = []\n",
    "    for text_i in window:\n",
    "        text_preds, text_preds2 = np.array(test_word_preds[text_i]), np.array(test_word_preds2[text_i])\n",
    "\n",
    "        if len(text_preds) <= len(text_preds2):\n",
    "            text_preds = w*text_preds + (1-w)*text_preds2[:len(text_preds)]\n",
    "        else:\n",
    "            text_preds[:len(text_preds2)] = w*text_preds[:len(text_preds2)] + (1-w)*text_preds2\n",
    "\n",
    "        num_words = len(text_preds)\n",
    "\n",
    "        global_features, global_locs = [], []\n",
    "\n",
    "        for dt in disc_type_to_ids:\n",
    "            disc_begin, disc_inside = disc_type_to_ids[dt]\n",
    "\n",
    "            gmean = (text_preds[:, disc_begin] + text_preds[:, disc_inside]).mean()\n",
    "            global_features.append(gmean)\n",
    "            global_locs.append(np.argmax(text_preds[:, disc_begin])/float(num_words))\n",
    "\n",
    "        disc_begin, disc_inside = disc_type_to_ids[disc_type]\n",
    "\n",
    "        # The probability that a word corresponds to either a 'B'-egin or 'I'-nside token for a class\n",
    "        prob_or = lambda word_preds: word_preds[:,disc_begin] + word_preds[:,disc_inside]\n",
    "\n",
    "        if not submit:\n",
    "            gt_idx = set()\n",
    "            gt_arr = np.zeros(num_words, dtype=int)\n",
    "            text_gt = valid.loc[valid.id == test_dataset.id.values[text_i]]\n",
    "            disc_gt = text_gt.loc[text_gt.discourse_type == disc_type]\n",
    "\n",
    "            # Represent the discourse instance locations in a hash set and an integer array for speed\n",
    "            for row_i, row in enumerate(disc_gt.iterrows()):\n",
    "                splt = row[1]['predictionstring'].split()\n",
    "                start, end = int(splt[0]), int(splt[-1]) + 1\n",
    "                gt_idx.add((start, end))\n",
    "                gt_arr[start:end] = row_i + 1\n",
    "            gt_lens = np.bincount(gt_arr)\n",
    "\n",
    "        # Iterate over every sub-sequence in the text\n",
    "        quants = np.linspace(0,1,7)\n",
    "        prob_begins = np.copy(text_preds[:,disc_begin])\n",
    "        min_begin = MIN_BEGIN_PROB[disc_type]\n",
    "        for pred_start in range(num_words):\n",
    "            prob_begin = prob_begins[pred_start]\n",
    "            if prob_begin > min_begin:\n",
    "                begin_or_inside = []\n",
    "                for pred_end in range(pred_start+1,min(num_words+1, pred_start+MAX_SEQ_LEN[disc_type]+1)):\n",
    "\n",
    "                    new_prob = prob_or(text_preds[pred_end-1:pred_end])\n",
    "                    insert_i = bisect_left(begin_or_inside, new_prob)\n",
    "                    begin_or_inside.insert(insert_i, new_prob[0])\n",
    "\n",
    "                    # Generate features for a word sub-sequence\n",
    "\n",
    "                    # The length and position of start/end of the sequence\n",
    "                    features = [pred_end - pred_start, pred_start / float(num_words), pred_end / float(num_words)]\n",
    "\n",
    "                    # 7 evenly spaced quantiles of the distribution of relevant class probabilities for this sequence\n",
    "                    features.extend(list(sorted_quantile(begin_or_inside, quants)))\n",
    "\n",
    "                    # The probability that words on either edge of the current sub-sequence belong to the class of interest\n",
    "                    features.append(prob_or(text_preds[pred_start-1:pred_start])[0] if pred_start > 0 else 0)\n",
    "                    features.append(prob_or(text_preds[pred_end:pred_end+1])[0] if pred_end < num_words else 0)\n",
    "                    features.append(prob_or(text_preds[pred_start-2:pred_start-1])[0] if pred_start > 1 else 0)\n",
    "                    features.append(prob_or(text_preds[pred_end+1:pred_end+2])[0] if pred_end < (num_words-1) else 0)\n",
    "\n",
    "                    # The probability that the first word corresponds to a 'B'-egin token\n",
    "                    features.append(text_preds[pred_start,disc_begin])\n",
    "                    features.append(text_preds[pred_start-1,disc_begin])\n",
    "\n",
    "                    if pred_end < num_words:\n",
    "                        features.append(text_preds[pred_end, begin_class_ids].sum())\n",
    "                    else:\n",
    "                        features.append(1.0)\n",
    "\n",
    "                    s = prob_or(text_preds[pred_start:pred_end])\n",
    "                    features.append(np.argmax(s)/features[0]) # maximum point location on sequence\n",
    "                    features.append(np.argmin(s)/features[0]) # minimum point location on sequence\n",
    "                    instability = 0\n",
    "                    if len(s) > 1:\n",
    "                        instability = (np.diff(s)**2).mean()\n",
    "                    features.append(instability)\n",
    "\n",
    "                    features.extend(list(global_features))\n",
    "                    features.extend(list([loc - features[1] for loc in global_locs]))\n",
    "\n",
    "                    exact_match = (pred_start, pred_end) in gt_idx if not submit else None\n",
    "\n",
    "                    if not submit:\n",
    "                        true_pos = False\n",
    "                        for match_cand, count in Counter(gt_arr[pred_start:pred_end]).most_common(2):\n",
    "                            if match_cand != 0 and count / float(pred_end - pred_start) >= .5 and float(count) / gt_lens[match_cand] >= .5: true_pos = True\n",
    "                    else: true_pos = None\n",
    "\n",
    "                    # For efficiency, use a numpy array instead of a list that doubles in size when full to conserve constant \"append\" time complexity\n",
    "                    if X_ind >= X.shape[0]:\n",
    "                        new_X = np.empty((X.shape[0]*2,N_FEATURES), dtype=np.float32)\n",
    "                        new_X[:X.shape[0]] = X\n",
    "                        X = new_X\n",
    "                    X[X_ind] = features\n",
    "                    X_ind += 1\n",
    "\n",
    "                    y.append(exact_match)\n",
    "                    truePos.append(true_pos)\n",
    "                    wordRanges.append((np.int16(pred_start), np.int16(pred_end)))\n",
    "                    groups.append(np.int16(text_i))\n",
    "\n",
    "    return SeqDataset(X[:X_ind], y, groups, wordRanges, truePos)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# predict strings and submit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Manager\n",
    "from sklearn.model_selection import cross_val_score, GroupKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from skopt.space import Real\n",
    "from skopt import gp_minimize\n",
    "import sys\n",
    "import xgboost\n",
    "\n",
    "NUM_FOLDS = 8\n",
    "\n",
    "warnings.filterwarnings('ignore', '.*ragged nested sequences*',)\n",
    "\n",
    "prob_cache = {} # Cache each fold's probability predictions for speed\n",
    "clfs = []  # Each fold will add its classifier here\n",
    "# Predict sub-sequences for a discourse type and set of train/test texts\n",
    "def predict_strings(disc_type, probThresh, test_groups, train_ind=None, submit=False):\n",
    "    string_preds = []\n",
    "    #validSeqDs = validSeqSets[disc_type]\n",
    "    #submitSeqDs = submitSeqSets[disc_type]\n",
    "\n",
    "    # Average the probability predictions of a set of classifiers\n",
    "\n",
    "\n",
    "\n",
    "    predict_df = test_texts\n",
    "    text_df = test_texts\n",
    "\n",
    "    for text_idx in tqdm(test_groups):\n",
    "        # The probability of true positive and (start,end) of each sub-sequence in the curent text\n",
    "\n",
    "        testDs=seq_dataset(disc_type, pred_indices=[text_idx],submit=True)\n",
    "\n",
    "        prob_tp_curr = get_tp_prob(testDs, disc_type)\n",
    "        word_ranges_curr = testDs.wordRanges[testDs.groups == text_idx]\n",
    "\n",
    "        split_text = text_df.loc[text_df.id == predict_df.id.values[text_idx]].iloc[0].text.split()\n",
    "        full_preds = np.zeros(len(split_text))\n",
    "        # Include the sub-sequence predictions in order of predicted probability\n",
    "        for prob, wordRange in reversed(sorted(zip(prob_tp_curr, [tuple(wr) for wr in word_ranges_curr]))):\n",
    "\n",
    "            # Until the predicted probability is lower than the tuned threshold\n",
    "            if prob < probThresh: break\n",
    "\n",
    "            intersect = np.sum(full_preds[wordRange[0]:wordRange[1]])\n",
    "            total = wordRange[1] - wordRange[0]\n",
    "            condition = intersect/total <= 0.15\n",
    "\n",
    "            if condition:\n",
    "                full_preds[wordRange[0]:wordRange[1]] = 1\n",
    "                string_preds.append((predict_df.id.values[text_idx], disc_type, ' '.join(map(str, list(range(wordRange[0], wordRange[1]))))))\n",
    "    return string_preds\n",
    "\n",
    "def sub_df(string_preds):\n",
    "    return pd.DataFrame(string_preds, columns=['id','class','predictionstring'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the tuned probability thresholds from tuning result files, and make sub-sequence predictions!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "uniqueSubmitGroups = range(len(test_word_preds))\n",
    "\n",
    "sub = pd.concat([sub_df(predict_strings(disc_type, thresholds[disc_type],\n",
    "                                        uniqueSubmitGroups, submit=True)) for disc_type in disc_type_to_ids ]).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "sub.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}