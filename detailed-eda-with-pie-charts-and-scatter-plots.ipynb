{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**hi welcome to my notebook, I hope you are having a good day**\n\n**in this notebook, I reviewed the NBME Score Clinial Patient Notes dataset and went deeper**\n\n**I tried to give as much detail as possible**\n\n**if you have any issue, please don't hesitate to contact me**\n\n**upvotes are all appreciated!**\n","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os # operating system module\nimport plotly.express as px # visualisations and graphs\nimport math # mathematical operations\nimport nltk # natural language toolkit\nfrom nltk.corpus import stopwords # stopwords in English e.g. out, as, has etc.\nfrom collections import Counter # useful counter in order to find most common words\nimport re # regular expressions\nimport string # string operations","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-01T20:59:05.02544Z","iopub.execute_input":"2022-03-01T20:59:05.026393Z","iopub.status.idle":"2022-03-01T20:59:06.759932Z","shell.execute_reply.started":"2022-03-01T20:59:05.026282Z","shell.execute_reply":"2022-03-01T20:59:06.759249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/nbme-score-clinical-patient-notes/train.csv')\ndisplay(os.path.getsize('../input/nbme-score-clinical-patient-notes/train.csv') / (1024 ** 2), 'in megabytes') # our train_df dataset is around 0.72 megabytes, it's relatively small","metadata":{"execution":{"iopub.status.busy":"2022-03-01T20:59:06.762108Z","iopub.execute_input":"2022-03-01T20:59:06.762457Z","iopub.status.idle":"2022-03-01T20:59:06.802218Z","shell.execute_reply.started":"2022-03-01T20:59:06.762413Z","shell.execute_reply":"2022-03-01T20:59:06.80152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**we calculated the size of data above**","metadata":{}},{"cell_type":"code","source":"for i, column in enumerate(train_df.columns):\n    print(f'{i+1}. column is {column}')","metadata":{"execution":{"iopub.status.busy":"2022-03-10T10:51:05.975257Z","iopub.execute_input":"2022-03-10T10:51:05.975814Z","iopub.status.idle":"2022-03-10T10:51:06.069581Z","shell.execute_reply.started":"2022-03-10T10:51:05.975704Z","shell.execute_reply":"2022-03-10T10:51:06.068678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Columns**\n\nwe have 6 columns in our train_df dataset in total\n\n> id: stands for unique ids of patients\n\n> case_num: number of a unique case\n\n> pn_num: uniqe number of patient\n\n> feature_num: numbers of features\n\n> annotation: annotations as it refers\n\n> location: Character spans indicating the location of each annotation within the note. Multiple spans may be needed to represent an annotation, in which case the spans are delimited by a semicolon\n**Credit goes to [Joao de Oliveira](https://www.kaggle.com/jdoliveira) for help me to correct it.**\n","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T20:59:06.810772Z","iopub.execute_input":"2022-03-01T20:59:06.811259Z","iopub.status.idle":"2022-03-01T20:59:06.82853Z","shell.execute_reply.started":"2022-03-01T20:59:06.811224Z","shell.execute_reply":"2022-03-01T20:59:06.827864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-03-01T20:59:06.830517Z","iopub.execute_input":"2022-03-01T20:59:06.831065Z","iopub.status.idle":"2022-03-01T20:59:06.839663Z","shell.execute_reply.started":"2022-03-01T20:59:06.831029Z","shell.execute_reply":"2022-03-01T20:59:06.839068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**our dataset consists of two different data types..**","metadata":{}},{"cell_type":"code","source":"train_df.isnull().any().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T20:59:06.840852Z","iopub.execute_input":"2022-03-01T20:59:06.841236Z","iopub.status.idle":"2022-03-01T20:59:06.856335Z","shell.execute_reply.started":"2022-03-01T20:59:06.8412Z","shell.execute_reply":"2022-03-01T20:59:06.85512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**we don't have any type of missing data, that's quite good**","metadata":{}},{"cell_type":"code","source":"print(f'we have {train_df.shape[0]} rows and {train_df.shape[1]} columns in our dataset')","metadata":{"execution":{"iopub.status.busy":"2022-03-01T20:59:06.8575Z","iopub.execute_input":"2022-03-01T20:59:06.857853Z","iopub.status.idle":"2022-03-01T20:59:06.862065Z","shell.execute_reply.started":"2022-03-01T20:59:06.857816Z","shell.execute_reply":"2022-03-01T20:59:06.861333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient_notes_df = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\npatient_notes_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T20:59:06.863212Z","iopub.execute_input":"2022-03-01T20:59:06.863607Z","iopub.status.idle":"2022-03-01T20:59:07.279603Z","shell.execute_reply.started":"2022-03-01T20:59:06.863574Z","shell.execute_reply":"2022-03-01T20:59:07.279062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> we have another dataset named as *patient notes*\n\n**it contains patient histories about stated patient (pn_num in this example)**\n\n**now we can merge them in order to enrich our main dataset**","metadata":{}},{"cell_type":"code","source":"features_df = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\npatient_notes_df = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\ndf = train_df.append(features_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T20:59:07.280535Z","iopub.execute_input":"2022-03-01T20:59:07.281096Z","iopub.status.idle":"2022-03-01T20:59:07.690952Z","shell.execute_reply.started":"2022-03-01T20:59:07.281057Z","shell.execute_reply":"2022-03-01T20:59:07.690321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**we merged our data**","metadata":{}},{"cell_type":"code","source":"train_df = train_df.merge(patient_notes_df, on=['pn_num', 'case_num'], how='left')\ntrain_df = train_df.merge(features_df, on=['feature_num', 'case_num'], how='left')\ndisplay(train_df.head())\nnullResult = 'nope' if not train_df.isnull().any().sum() else 'yup' # still we don't have any null values\ndisplay('-' * 120)\nprint(f' +do we have null values? \\n -{nullResult}')\ndisplay('-' * 120)\ndisplay(train_df.tail())","metadata":{"execution":{"iopub.status.busy":"2022-03-01T20:59:07.691868Z","iopub.execute_input":"2022-03-01T20:59:07.692745Z","iopub.status.idle":"2022-03-01T20:59:07.753565Z","shell.execute_reply.started":"2022-03-01T20:59:07.692695Z","shell.execute_reply":"2022-03-01T20:59:07.752783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train_df.duplicated().sum())\ntrain_df.tail()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T20:59:07.754913Z","iopub.execute_input":"2022-03-01T20:59:07.755521Z","iopub.status.idle":"2022-03-01T20:59:07.802413Z","shell.execute_reply.started":"2022-03-01T20:59:07.755486Z","shell.execute_reply":"2022-03-01T20:59:07.801587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**none of our rows are duplicated, this means we won't need to have our data cleansed**\n\n**let's visualise then!**","metadata":{}},{"cell_type":"code","source":"counts_of_notes_df = patient_notes_df.groupby(\"case_num\").count()\nind = counts_of_notes_df.index\nfig = px.bar(data_frame= counts_of_notes_df, y='pn_num', x=ind, text_auto='.2s',\n            title=\"Count Distribution of Different Cases\",\n            labels={'case_num': 'Case Number', 'pn_num': 'Number of Patients'},\n            width=1100, height=700,\n            color ='pn_num',\n            color_continuous_scale='aggrnyl'\n            )\nfig.update_layout(\n    title_x=0.5,\n    xaxis = dict(\n        tickmode = 'array',\n        tickvals = list(range(0,10)),\n        ticktext = ['Case Zero', 'Case One', 'Case Two', 'Case Three', 'Case Four', 'Case Five', 'Case Six', 'Case Seven', 'Case Eight', 'Case Nine']\n    )\n)\n\nfig.show()","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-03-01T20:59:35.287481Z","iopub.execute_input":"2022-03-01T20:59:35.28866Z","iopub.status.idle":"2022-03-01T20:59:35.38139Z","shell.execute_reply.started":"2022-03-01T20:59:35.288606Z","shell.execute_reply":"2022-03-01T20:59:35.380391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**let's check average length of annotations and patient histories**","metadata":{}},{"cell_type":"code","source":"annotation_length, pn_history_length = math.ceil(train_df['annotation'].str.len().mean()), math.ceil(train_df['pn_history'].str.len().mean())\nprint(f' average length of annotations: ~{annotation_length} \\n average length of patient histories: ~{pn_history_length}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-01T20:59:08.110279Z","iopub.execute_input":"2022-03-01T20:59:08.110644Z","iopub.status.idle":"2022-03-01T20:59:08.140746Z","shell.execute_reply.started":"2022-03-01T20:59:08.110613Z","shell.execute_reply":"2022-03-01T20:59:08.139742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**now we can visualise it!**","metadata":{}},{"cell_type":"code","source":"annotation_length = []\nfor i in train_df['annotation']:\n    annotation_length.append(len(i))\n    \ndf = pd.DataFrame(annotation_length)\n\nfig = px.scatter(df,\n                 x=df[0].index,\n                 y=df[0],\n                 labels={'index': 'Annotation Index', '0': 'Annotation Length'},\n                 color=df[0],\n                 color_continuous_scale=px.colors.sequential.Viridis)\nfig.update_layout(\n    title='Length Distribution of Annotations',\n    title_x=0.5,)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T20:59:08.14418Z","iopub.execute_input":"2022-03-01T20:59:08.144565Z","iopub.status.idle":"2022-03-01T20:59:08.291675Z","shell.execute_reply.started":"2022-03-01T20:59:08.144518Z","shell.execute_reply":"2022-03-01T20:59:08.291028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**now we are gonna imply the same operation for patient histories**","metadata":{}},{"cell_type":"code","source":"pn_history_length = []\nfor i in train_df['pn_history']:\n    pn_history_length.append(len(i))\n\ndf = pd.DataFrame(pn_history_length)\n\nfig = px.scatter(df,\n                 x=df[0].index,\n                 y=df[0],\n                 labels={'index': 'History Index', '0': 'History Length'},\n                 color=df[0],\n                 color_continuous_scale=px.colors.sequential.algae)\nfig.update_layout(\n    title='Length Distribution of Patient Histories',\n    title_x=0.5,)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T20:59:08.29261Z","iopub.execute_input":"2022-03-01T20:59:08.293246Z","iopub.status.idle":"2022-03-01T20:59:08.42171Z","shell.execute_reply.started":"2022-03-01T20:59:08.293212Z","shell.execute_reply":"2022-03-01T20:59:08.421119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**all done so far!**\n\n**now we will filter our data, we will remove brackets, punctuations, stopwords etc.**","metadata":{}},{"cell_type":"code","source":"annotation_words_list =  pd.Series(' '.join(train_df['annotation']).lower().split()).value_counts()[:50]\nfiltered_words = [word for word in annotation_words_list.index if word not in stopwords.words('english')]\n#display(annotation_words_list.size)\n#display(len(filtered_words))\n\nfiltered_words = [x for x in filtered_words if not any(c.isdigit() for c in x)] # we remove digits\nfinal_filtered_annotation_words =  []\nfor doc in filtered_words:\n    doc = doc.translate(str.maketrans('', '', string.punctuation)) # we remove punctuations\n    final_filtered_annotation_words.append(doc)\ntmp_ann = []\nfor doc in annotation_words_list.index:\n    doc = doc.translate(str.maketrans('', '', string.punctuation)) # we apply same process for indices\n    tmp_ann.append(doc)\nannotation_words_list.index = tmp_ann\nfor i in final_filtered_annotation_words:\n    print(i)\ndisplay('these things are our brand new filtered most occurring words in annotations!')\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-01T20:59:08.422873Z","iopub.execute_input":"2022-03-01T20:59:08.423232Z","iopub.status.idle":"2022-03-01T20:59:08.467565Z","shell.execute_reply.started":"2022-03-01T20:59:08.423195Z","shell.execute_reply":"2022-03-01T20:59:08.466782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(final_filtered_annotation_words)\ndf = df.iloc[1: , :] # indexing\ndf['values'] = None # create new column\ndf = df.rename(columns={0: 'words'})\nfor i in range(1, len(df)):\n    #df['values'].loc[i+1] = annotation_words_list[i]\n    for j in range(len(annotation_words_list.index)):\n        if annotation_words_list.index[j] == df['words'].iloc[i]:\n            df['values'].iloc[i] = annotation_words_list[j]\n# some manual corrections\ndf['values'].iloc[0] = 509\ndf['values'].iloc[2] = 360\ndf.drop(11 ,axis=0)\ndf.drop_duplicates(subset=None, keep=\"first\", inplace=True) # drop one of two duplicates\ndf.reset_index(drop=True) # resetting our index after some drop process","metadata":{"execution":{"iopub.status.busy":"2022-03-01T20:59:08.468869Z","iopub.execute_input":"2022-03-01T20:59:08.469099Z","iopub.status.idle":"2022-03-01T20:59:08.523366Z","shell.execute_reply.started":"2022-03-01T20:59:08.46907Z","shell.execute_reply":"2022-03-01T20:59:08.522511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**now, it looks clean yet! let's visualise it then**\n\n**for this case we will be using pie chart**","metadata":{}},{"cell_type":"code","source":"fig = px.pie(df, values='values', names='words',color_discrete_sequence=px.colors.sequential.RdBu, title='Occurency of Some of Words in Annotations',\n            width=800, height=700,\n                labels={\n                     'values': \"Occurence Count\",\n                     \"words\": \"Word\"\n                 },)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T20:59:08.524579Z","iopub.execute_input":"2022-03-01T20:59:08.524811Z","iopub.status.idle":"2022-03-01T20:59:08.581527Z","shell.execute_reply.started":"2022-03-01T20:59:08.524783Z","shell.execute_reply":"2022-03-01T20:59:08.58069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**not gonna lie i like pie charts anyway**\n\n**we will apply the very similar process for our patient histories data**","metadata":{}},{"cell_type":"code","source":"pn_history_words_list =  pd.Series(' '.join(train_df['pn_history']).lower().split()).value_counts()[:50]\nfiltered_words = [word for word in pn_history_words_list.index if word not in stopwords.words('english')]\n#display(pn_history_words_list.size)\n#display(len(filtered_words))\n\nfiltered_words = [x for x in filtered_words if not any(c.isdigit() for c in x)]\nfinal_filtered_pn_history_words =  []\nfor doc in filtered_words:\n    doc = doc.translate(str.maketrans('', '', string.punctuation))\n    final_filtered_pn_history_words.append(doc)\ntmp_ann = []\nfor doc in pn_history_words_list.index:\n    doc = doc.translate(str.maketrans('', '', string.punctuation))\n    tmp_ann.append(doc)\npn_history_words_list.index = tmp_ann\n\ndf = pd.DataFrame(final_filtered_pn_history_words)\ndf = df.iloc[1: , :]\ndf['values'] = None\ndf = df.rename(columns={0: 'words'})\nfor i in range(1, len(df)):\n    #df['values'].loc[i+1] = pn_history_words_list[i]\n    for j in range(len(pn_history_words_list.index)):\n        if pn_history_words_list.index[j] == df['words'].iloc[i]:\n            df['values'].iloc[i] = pn_history_words_list[j]\ndf['values'].iloc[0] = 509\ndf['values'].iloc[2] = 360\ndf.drop(1 ,axis=0, inplace=True)\ndf.drop(4 ,axis=0, inplace=True)\ndf.drop_duplicates(subset=None, keep=\"first\", inplace=True)\ndf.reset_index(drop=True)\ndf.dropna(how='all', inplace=True)\n\ndf = pd.DataFrame(final_filtered_pn_history_words)\ndf = df.iloc[1: , :]\ndf['values'] = None\ndf = df.rename(columns={0: 'words'})\nfor i in range(1, len(df)):\n    #df['values'].loc[i+1] = pn_history_words_list[i]\n    for j in range(len(pn_history_words_list.index)):\n        if pn_history_words_list.index[j] == df['words'].iloc[i]:\n            df['values'].iloc[i] = pn_history_words_list[j]\ndf['values'].iloc[0] = 509\ndf['values'].iloc[2] = 360\ndf.drop(1 ,axis=0, inplace=True)\ndf.drop(4 ,axis=0, inplace=True)\ndf.drop_duplicates(subset=None, keep=\"first\", inplace=True)\ndf.reset_index(drop=True)\ndf.dropna(how='all', inplace=True)\nfig = px.pie(df, values='values', names='words',color_discrete_sequence=px.colors.sequential.RdBu, title='Occurency of Some of Words in Patient Histories',\n            width=800, height=700,\n                labels={\n                     'values': \"Occurence Count\",\n                     \"words\": \"Word\"\n                 },)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-01T20:59:08.583054Z","iopub.execute_input":"2022-03-01T20:59:08.583535Z","iopub.status.idle":"2022-03-01T20:59:09.610173Z","shell.execute_reply.started":"2022-03-01T20:59:08.583494Z","shell.execute_reply":"2022-03-01T20:59:09.609494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**thanks for reading.**","metadata":{}}]}