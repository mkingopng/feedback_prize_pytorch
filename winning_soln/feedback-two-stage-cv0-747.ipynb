{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This code is our ensemble cv result. If you are interested, you can change the prediction result to your own. What you need is to change your preds to preds1_mean and preds2_mean, which represent the probability that the token is the beginning and the probability of each class, Note that the order of class needs to be the same as the order of id2label. \n\ncompared to https://www.kaggle.com/wht1996/feedback-two-stage-lb0-727, lgb features less lstm and pca, These features can increase about 0.001, but the speed is relatively slow, so they are deleted here.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pickle\nimport os\nimport math\nimport re\nfrom numba import jit\nfrom tqdm import tqdm\ndata_path = '../input/feedback-prize-2021/'\n\ntrain_df = pd.read_csv('../input/feedback-prize-2021/train.csv')\nIDS = train_df.id.unique()\n\nkfold_ids = pickle.load(open('../input/feedback-two-stage-data/kfold_ids.pkl','rb'))\n\nid2label = {0:'Lead', 1:'Position', 2:'Evidence', 3:'Claim', 4:'Concluding Statement',\n             5:'Counterclaim', 6:'Rebuttal', 7:'blank'}\nlabel2id = {v:k for k,v in id2label.items()}\n\nclass CONFIG:\n    def __init__(self):\n        self.max_length = 4096\n        \nconfig = CONFIG() ","metadata":{"execution":{"iopub.status.busy":"2022-03-15T12:44:54.153183Z","iopub.execute_input":"2022-03-15T12:44:54.153479Z","iopub.status.idle":"2022-03-15T12:44:56.749869Z","shell.execute_reply.started":"2022-03-15T12:44:54.1534Z","shell.execute_reply":"2022-03-15T12:44:56.748929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### read pred data","metadata":{}},{"cell_type":"code","source":"data_pred = pickle.load(open('../input/feedback-two-stage-data/feedback-lb704.pkl','rb'))\ndata_pred = pd.DataFrame(data_pred, columns=['id', 'text', 'input_ids', 'attention_mask', 'token_label',\n       'offset_mapping', 'kfold', 'pred'])\n\ndic_off_map = data_pred[['id','offset_mapping']].set_index('id')['offset_mapping'].to_dict()\ndic_txt = data_pred[['id','text']].set_index('id')['text'].to_dict()\n\n\"\"\"\nori class order\n{'O': 0,\n 'I-Claim': 1,\n 'I-Evidence': 2,\n 'I-Position': 3,\n 'I-Concluding Statement': 4,\n 'I-Lead': 5,\n 'I-Counterclaim': 6,\n 'I-Rebuttal': 7,\n 'B-Claim': 8,\n 'B-Evidence': 9,\n 'B-Position': 10,\n 'B-Concluding Statement': 11,\n 'B-Lead': 12,\n 'B-Counterclaim': 13,\n 'B-Rebuttal': 14}\n\"\"\"\n\ndef change_label(x):\n    \"\"\"\n    change N*15 preds to N*1 + N*8 preds\n    \"\"\"\n    res1  = x[:,8:].sum(axis=1)\n    res2 = np.zeros((len(res1), 8))\n    \n    # change order, If it is in the same order as id2label, delete it\n    label_map = {0:5, 1:3, 2:2, 3:1, 4:4, 5:6, 6:7, 7:0} \n    for i in range(8):\n        if i == 7:\n            res2[:,i] = x[:,label_map[i]]\n        else:\n            res2[:,i] = x[:,[label_map[i], label_map[i]+7]].sum(axis=1)\n\n    return res1, res2\n\npreds1_mean = {}\npreds2_mean = {}\nfor irow,row in data_pred.iterrows():\n    t1, t2 = change_label(row.pred)\n    preds1_mean[row.id] = t1.astype('float64')\n    preds2_mean[row.id] = t2.astype('float64')","metadata":{"execution":{"iopub.status.busy":"2022-03-15T12:44:56.75157Z","iopub.execute_input":"2022-03-15T12:44:56.751808Z","iopub.status.idle":"2022-03-15T12:45:12.785538Z","shell.execute_reply.started":"2022-03-15T12:44:56.751775Z","shell.execute_reply":"2022-03-15T12:45:12.784646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### get recall sample","metadata":{}},{"cell_type":"code","source":"recall_thre = { \n    \"Lead\": 0.07,\n    \"Position\": 0.06,\n    \"Evidence\": 0.07,\n    \"Claim\": 0.06,\n    \"Concluding Statement\": 0.07,\n    \"Counterclaim\": 0.03,\n    \"Rebuttal\": 0.02,\n}\n\nL_k = {\n    \"Evidence\": 0.85,\n    \"Rebuttal\": 0.6,\n}\n\n\ndef deal_predictionstring(df):\n    \"\"\"\n    select sample with high boundary threshold and \n    choice 65% length with the highest probability of the current class as a new sample\n    \"\"\"\n    new_predictionstring = []\n    new_pos_list = []\n    flag_list = []\n    thre = 0.8\n    for id, typ, pos, (start, end) in df.values:\n        flag = 0\n        L = round(max(1, (pos[1]-pos[0]+1)*0.25))\n\n        pos_left = max(0, pos[0]-L)\n        pos_right = min(len(preds1_mean[id]), pos[1]+1+L)\n        \n        if start<10:\n            left_thre = 2\n        else:\n            left_thre = max(preds1_mean[id][pos[0]], 1-preds2_mean[id][pos_left:pos[0],label2id[typ]].min())\n        \n        if pos[1] >= len(preds1_mean[id])-10:\n            right_thre=2\n        else:\n            right_thre = max(preds1_mean[id][pos[1]+1:pos_right].max(), 1-preds2_mean[id][pos[1]+1:pos_right, label2id[typ]].min())\n        \n        if left_thre>thre and right_thre>thre:\n\n            L = math.ceil((pos[1]-pos[0]+1)*L_k.get(typ, 0.65))\n\n            tmp = {}\n            for i in range(pos[0], pos[1]):\n                if i+L>pos[1]:\n                    break\n                tmp[i] = np.sum(preds2_mean[id][i:i+L+1,label2id[typ]])\n            if len(tmp)==0:\n                new_pos = pos\n            else:\n                flag = min(left_thre, right_thre)\n                new_start = max(tmp.keys(), key=lambda x:tmp[x])\n                new_pos = (new_start,new_start+L)\n\n        else:\n            new_pos = pos\n\n        off_map = dic_off_map[id]\n        txt = dic_txt[id]\n        txt_max = len(txt.split())\n\n        start_word = len(txt[:off_map[new_pos[0]][0]].split())\n\n        L = len(txt[off_map[new_pos[0]][0]:off_map[new_pos[1]][1]].split())\n        end_word = min(txt_max, start_word+L) - 1\n\n        new_predictionstring.append([start_word, end_word])\n        new_pos_list.append(new_pos)\n        flag_list.append(flag)\n        \n    df_new = df.copy()\n    df_new['pos'] = new_pos_list\n    df_new['predictionstring'] = new_predictionstring\n    df_new['flag'] = flag_list\n    \n    df_new = pd.concat([df_new, df.loc[df_new[(df_new.flag>=0.8) & (df_new.flag<0.95)].index]])\n    df_new = df_new.reset_index(drop=True)\n    df_new['flag'].fillna(0,inplace=True)\n    \n    return df_new\n\n\ndef get_recall(id):\n    all_predictions = []\n\n    pred1_np = np.array(preds1_mean[id])\n    pred2_np_all = np.array(preds2_mean[id])\n\n    off_map = dic_off_map[id]\n    off_map_len = len(off_map) if off_map[-1][1] != 0 else len(off_map)-1\n    max_length = min(config.max_length, off_map_len)\n    for class_num in range(7):\n        thre = recall_thre[id2label[class_num]]\n        pred2_np = pred2_np_all[:, class_num]\n\n        i_start = 0\n        while i_start < max_length:\n            i = 0\n            if pred1_np[i_start] > thre and pred2_np[i_start:i_start+10].max() > thre: #开头\n                i = i_start + 1\n                if i>=max_length: break\n                while pred1_np[i] < (1-thre) and pred2_np[i:i+10].max() > thre: # 结束\n                    cond = any([\n                        i+1==max_length,\n                        pred1_np[i] > thre,\n                        i+1<max_length and pred2_np[i] < 0.6 and pred2_np[i] - pred2_np[i+1] > thre\n                    ])\n                    if i>i_start+1 and cond:\n                        all_predictions.append((id, id2label[class_num], [i_start, i]))\n                    i += 1\n                    if i>=max_length: break\n\n            if i != 0:\n                if i == max_length:\n                    i -=1\n\n                all_predictions.append((id, id2label[class_num], [i_start, i]))\n            i_start += 1\n                \n    df_recall = pd.DataFrame(all_predictions, columns=['id', 'class', 'pos'])\n    \n    predictionstring = []\n    for cache in df_recall.values:\n        id = cache[0]\n        pos = cache[2]\n        off_map = dic_off_map[id]\n        txt = dic_txt[id]\n        txt_max = len(txt.split())\n\n        start_word = len(txt[:off_map[pos[0]][0]].split())\n\n        L = len(txt[off_map[pos[0]][0]:off_map[pos[1]][1]].split())\n        end_word = min(txt_max, start_word+L) - 1\n\n        predictionstring.append([start_word, end_word])\n\n    df_recall['predictionstring'] = predictionstring\n\n    return deal_predictionstring(df_recall)\n#     return df_recall","metadata":{"execution":{"iopub.status.busy":"2022-03-15T12:45:12.786931Z","iopub.execute_input":"2022-03-15T12:45:12.787202Z","iopub.status.idle":"2022-03-15T12:45:12.833791Z","shell.execute_reply.started":"2022-03-15T12:45:12.787173Z","shell.execute_reply":"2022-03-15T12:45:12.833021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### lgb features","metadata":{}},{"cell_type":"code","source":"@jit(nopython=True)\ndef feat_speedup(arr):\n    r_max, r_min, r_sum = -1e5,1e5,0\n    for x in arr:\n        r_max = max(r_max, x)\n        r_min = min(r_min, x)\n        r_sum += x\n    return r_max, r_min, r_sum, r_sum/len(arr)\n\nnp_lin = np.linspace(0,1,7)\n\n@jit(nopython=True)\ndef sorted_quantile(array, q):\n    n = len(array)\n    index = (n - 1) * q\n    left = int(index)\n    fraction = index - left\n    right = left\n    right = right + int(fraction > 0)\n    i, j = array[left], array[right]\n    return i + (j - i) * fraction\n\ndef get_percentile(array):\n    x = np.sort(array)\n    n = len(x)-1\n    return x[[int(n*t) for t in np_lin[1:-1]]]\n\n\ndef tuple_map(offset_mapping,threshold):\n    paragraph_rk = []\n    rk = 0\n    last = 1\n    for token_index in offset_mapping:\n        if len(threshold) == 0:\n            paragraph_rk.append(1)\n        elif token_index[1] <= threshold[rk][1]:\n            last = max(rk+1,last)\n            paragraph_rk.append(last)\n        else: \n            last = max(rk+2,last)\n            paragraph_rk.append(last)\n            if rk + 1 < len(threshold) - 1:\n                rk += 1\n            \n    return paragraph_rk\n\n\ndef get_pos_feat(text, offset_mapping):\n\n    paragraph_cnt = len(text.split('\\n\\n')) + 1\n\n    paragraph_th = [m.span() for m in re.finditer('\\n\\n',text)]\n    paragraph_rk = tuple_map(offset_mapping,paragraph_th)\n\n    paragraph_rk_r = [paragraph_cnt-rk+1 if rk!=0 else 0 for rk in paragraph_rk]\n\n    sentence_th = []\n    for i,v in enumerate([m.span() for m in re.finditer('\\n\\n|\\.|,|\\?|\\!',text)]):\n        if i == 0:\n            sentence_th.append(list(v))\n        else:\n            if v[0]==sentence_th[-1][-1]:\n                sentence_th[-1][-1] = v[-1]\n            else:\n                sentence_th.append(list(v))\n    sentence_cnt = len(sentence_th) + 1\n\n    sentence_rk = tuple_map(offset_mapping,sentence_th)\n    sentence_rk_r = [sentence_cnt-rk+1 if rk!=0 else 0 for rk in sentence_rk]\n\n    last_garagraph_cnt = 0\n    sentence_rk_of_paragraph = []\n    for i in range(len(offset_mapping)):\n        sentence_rk_of_paragraph.append(sentence_rk[i]-last_garagraph_cnt)\n        if i+1 == len(offset_mapping) or paragraph_rk[i]!=paragraph_rk[i+1]:\n            last_garagraph_cnt = sentence_rk[i]\n\n    sentence_cnt_of_paragraph = []\n    last_max = None\n    for i in range(1,len(offset_mapping)+1):\n        if i==1 or paragraph_rk[-i] != paragraph_rk[-i+1]:\n            last_max = sentence_rk_of_paragraph[-i]\n        sentence_cnt_of_paragraph.append(last_max)\n    sentence_cnt_of_paragraph = sentence_cnt_of_paragraph[::-1]\n \n    sentence_rk_r_of_paragraph = [s_cnt-rk+1 if rk!=0 else 0 for s_cnt,rk in zip(sentence_cnt_of_paragraph,sentence_rk_of_paragraph)]\n\n    return paragraph_cnt,sentence_cnt,paragraph_rk,paragraph_rk_r,sentence_rk,sentence_rk_r, \\\n            sentence_cnt_of_paragraph,sentence_rk_of_paragraph,sentence_rk_r_of_paragraph\n\n\nlgb_columns = pickle.load(open('../input/feedback-two-stage-data/lgb_columns.pkl','rb'))\n\n\ndef fun_get_feat(id):\n    df_feat = []\n    \n    data_sub = get_recall(id)\n    txt = dic_txt[id]\n    off_map = dic_off_map[id]\n    txt_feat = get_pos_feat(txt, off_map)\n   \n    preds1_all = preds1_mean[id]\n    preds_type = preds2_mean[id].argmax(axis=-1)\n    \n    text_char_length = len(txt)\n    text_word_length = len(txt.split())\n    text_token_length = len(off_map)\n    for cache in data_sub.values:\n        id = cache[0]\n        typ = cache[1]\n        start, end = cache[2]\n        prediction = cache[3]\n\n        dic = {k:np.nan for k in lgb_columns}\n#         dic={'id': id}\n        dic['id'] = id\n        dic['pos'] = cache[2]\n        dic['class'] = label2id[typ]\n        dic['post_flag'] = cache[4]\n\n        dic['paragraph_cnt'] = txt_feat[0]\n        dic['sentence_cnt'] = txt_feat[1]\n        dic['paragraph_rk'] = txt_feat[2][start]\n        dic['paragraph_rk_r'] = txt_feat[3][end]\n        dic['sentence_rk'] = txt_feat[4][start]\n        dic['sentence_rk_r'] = txt_feat[5][end]\n        dic['sentence_cnt_of_paragraph'] = txt_feat[6][start]\n        dic['sentence_cnt_of_paragraph2'] = txt_feat[6][end]\n        dic['sentence_rk_of_paragraph'] = txt_feat[7][start]\n        dic['sentence_rk_r_of_paragraph'] = txt_feat[8][end]\n        dic['sub_paragraph_cnt'] = txt_feat[2][end] - txt_feat[2][start]\n        dic['sub_sentence_cnt'] = txt_feat[4][end] - txt_feat[4][start]\n\n        other_type = [t for t in range(8) if t != dic['class']]\n        preds2_all = preds2_mean[id][:, label2id[typ]]\n        preds4_all = preds2_mean[id][:, other_type].max(axis=-1)\n        preds1 = preds1_all[start:end+1]\n        preds2 = preds2_all[start:end+1]\n        preds4 = preds4_all[start:end+1]\n\n        word_length = prediction[-1] - prediction[0] + 1\n        \n        dic['L1'] = word_length\n        dic['L2'] = end - start + 1\n        dic['text_char_length'] = text_char_length\n        dic['text_word_length'] = text_word_length\n        dic['text_token_length'] = text_token_length\n\n        dic['word_start'] = prediction[0]\n        dic['word_end'] = prediction[-1]\n        dic['token_start'] = start\n        dic['token_start2'] = start / text_token_length\n        dic['token_end'] = end\n        dic['token_end2'] = text_token_length - end\n        dic['token_end3'] = end / text_token_length\n        \n        dic[f'head_preds1'] = preds1[0]\n        dic[f'head2_preds1'] = preds1_all[start-1:start+2].sum()\n        if len(preds1) > 1:\n            dic[f'tail_preds1'] = preds1[-1]\n            dic['max_preds1'], dic['min_preds1'], dic['sum_preds1'], dic['mean_preds1'] = feat_speedup(preds1[1:])\n      \n        sort_idx = preds1[1:].argsort()[::-1]\n        tmp = []\n        for i in range(5):\n            if i < len(sort_idx):\n                dic[f'other_preds1_{i}'] = preds1[1+sort_idx[i]]\n                dic[f'other_preds1_idx_{i}'] = (1+sort_idx[i])/len(preds1)\n                tmp.append(preds1[1+sort_idx[i]])\n        if len(tmp):\n            dic[f'other_preds1_mean'] = np.mean(tmp)\n\n        dic[f'head_preds2'] = preds2[0]\n        dic[f'tail_preds2'] = preds2[-1]\n        dic['max_preds2'], dic['min_preds2'], dic['sum_preds2'], dic['mean_preds2'] = feat_speedup(preds2)\n\n        dic[f'head_preds4'] = preds4[0]\n        dic[f'tail_preds4'] = preds4[-1]\n        dic['max_preds4'], dic['min_preds4'], dic['sum_preds4'], dic['mean_preds4'] = feat_speedup(preds4)\n        \n        sort_idx = preds2.argsort()\n        tmp = []\n        for i in range(5):\n            if i < len(sort_idx):\n                dic[f'other_preds2_{i}'] = preds2[sort_idx[i]]\n                dic[f'other_preds2_idx_{i}'] = (sort_idx[i])/len(preds2)\n                tmp.append(preds2[sort_idx[i]])\n        if len(tmp):\n            dic[f'other_preds2_mean'] = np.mean(tmp)\n            \n            \n        for i,ntile in enumerate([sorted_quantile(preds2,i) for i in np_lin]):\n            dic[f'preds2_trend{i}'] = ntile\n        for i,ntile in enumerate(get_percentile(preds2)):\n            dic[f'preds2_ntile{i}'] = ntile\n        for i,ntile in enumerate([sorted_quantile(preds4,i) for i in np_lin]):\n            dic[f'preds4_trend{i}'] = ntile\n        for i,ntile in enumerate(get_percentile(preds4)):\n            dic[f'preds4_ntile{i}'] = ntile\n            \n            \n        for i in range(1,4):\n            if start-i >= 0:\n                dic[f'before_head2_prob{i}'] = preds2_all[start-i]\n                dic[f'before_other_prob{i}'] = preds4_all[start-i]\n                dic[f'before_other_type{i}'] = preds_type[start-i]\n                \n            if end+i < len(preds1_all):\n                dic[f'after_head2_prob{i}'] = preds2_all[end+i]\n                dic[f'after_other_prob{i}'] = preds4_all[end+i]\n                dic[f'after_other_type{i}'] = preds_type[end+i]\n\n\n        for mode in ['before', 'after']:\n            for iw, extend_L in enumerate([math.ceil(word_length/2), word_length]):\n                if mode == 'before':\n                    if start-extend_L<0:\n                        continue\n                    preds1_extend = preds1_all[start-extend_L:start]\n                    preds2_extend = preds2_all[start-extend_L:start]\n                else:\n                    if end+extend_L >=len(preds1_all):\n                        continue\n                    preds1_extend = preds1_all[end+1:end+extend_L]\n                    preds2_extend = preds2_all[end+1:end+extend_L]\n                    \n                if len(preds1_extend) == 0:\n                    continue\n                dic[f'{mode}{iw}_head_preds1'] = preds1_extend[0]\n                dic[f'{mode}{iw}_max_preds1'], dic[f'{mode}{iw}_min_preds1'], \\\n                dic[f'{mode}{iw}_sum_preds1'], dic[f'{mode}{iw}_mean_preds1'] = feat_speedup(preds1_extend)\n\n                dic[f'{mode}{iw}_head_preds2'] = preds2_extend[0]\n                dic[f'{mode}{iw}_max_preds2'], dic[f'{mode}{iw}_min_preds2'], \\\n                dic[f'{mode}{iw}_sum_preds2'], dic[f'{mode}{iw}_mean_preds2'] = feat_speedup(preds2_extend)\n\n                dic[f'{mode}{iw}_sum_preds1_rate'] = dic[f'{mode}{iw}_sum_preds1'] / dic[f'sum_preds1']\n                dic[f'{mode}{iw}_sum_preds2_rate'] = dic[f'{mode}{iw}_sum_preds2'] / dic[f'sum_preds2']\n                dic[f'{mode}{iw}_max_preds1_rate'] = dic[f'{mode}{iw}_max_preds1'] / dic[f'max_preds1']\n                dic[f'{mode}{iw}_max_preds2_rate'] = dic[f'{mode}{iw}_max_preds2'] / dic[f'max_preds2']\n\n        df_feat.append(dic)\n\n    return df_feat\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-15T12:45:12.836231Z","iopub.execute_input":"2022-03-15T12:45:12.836552Z","iopub.status.idle":"2022-03-15T12:45:13.122321Z","shell.execute_reply.started":"2022-03-15T12:45:12.836525Z","shell.execute_reply":"2022-03-15T12:45:13.121565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### lgb predict and post choice","metadata":{}},{"cell_type":"code","source":"proba_thresh = { \n    \"Lead\": 0.45,\n    \"Position\": 0.4,\n    \"Evidence\": 0.45,\n    \"Claim\": 0.35,\n    \"Concluding Statement\": 0.5,\n    \"Counterclaim\": 0.35,\n    \"Rebuttal\": 0.3,\n}\n\ninter_thresh = { \n    \"Lead\": 0.15,\n    \"Position\": 0.15,\n    \"Evidence\": 0.15,\n    \"Claim\": 0.25,\n    \"Concluding Statement\": 0.15,\n    \"Counterclaim\": 0.25,\n    \"Rebuttal\": 0.25,\n}\n\n\ndef post_choice(df):\n    rtn = []\n    for k,group in df.groupby(['id','class']):\n        group = group.sort_values('lgb_prob',ascending=False)\n\n        preds_range = []\n        for irow, row in group.iterrows():\n            start = row.word_start\n            end = row.word_end\n            L1 = end-start+1\n            flag = 0\n            for pos_range in preds_range:\n                L2 = pos_range[1] - pos_range[0] + 1\n                intersection = (min(end, pos_range[1]) - max(start, pos_range[0]) + 1) / L1\n                inter_t = inter_thresh[row['class']]\n                if intersection>inter_t and (inter_t<=L1/L2<=1 or inter_t<=L2/L1<=1):\n                    flag = 1\n                    break\n\n            if flag == 0:\n                preds_range.append((start, end, row.lgb_prob))\n                rtn.append((row.id, row['class'], row.pos, row.word_start, row.word_end, row.lgb_prob))\n    rtn = pd.DataFrame(rtn, columns=['id','class','pos','start','end','lgb_prob'])\n    return rtn\n\nfold = 0\nlgb_model = pickle.load(open(f'../input/feedback-two-stage-data/lgb_fold{fold}.pkl','rb'))\n\nsub = pd.DataFrame()\nfor id in tqdm(kfold_ids[fold][1]):\n    df_feat = pd.DataFrame(fun_get_feat(id))\n    \n    lgb_preds = lgb_model.predict(df_feat.drop(['id','pos'],axis=1))\n        \n    df_final = df_feat[['id', 'class','pos', 'word_start','word_end']].copy()\n    df_final['lgb_prob'] = lgb_preds\n    df_final['class'] = df_final['class'].map(lambda x:id2label[x])\n    \n    df_final['thre'] = df_final['class'].map(lambda x: proba_thresh[x])\n    df_final = df_final[df_final.lgb_prob>=df_final.thre]\n    df_final = post_choice(df_final)\n    \n    sub = pd.concat([sub, df_final])","metadata":{"execution":{"iopub.status.busy":"2022-03-15T12:45:13.123459Z","iopub.execute_input":"2022-03-15T12:45:13.123695Z","iopub.status.idle":"2022-03-15T12:56:02.105137Z","shell.execute_reply.started":"2022-03-15T12:45:13.123664Z","shell.execute_reply":"2022-03-15T12:56:02.104119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predictionstring(df):\n    predictionstring = []\n    for cache in df.values:\n        predictionstring.append(' '.join(list(map(str, range(cache[3], cache[4]+1)))))\n    return predictionstring\n\n\nsub['predictionstring'] = get_predictionstring(sub)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T12:56:02.106296Z","iopub.execute_input":"2022-03-15T12:56:02.106478Z","iopub.status.idle":"2022-03-15T12:56:02.321487Z","shell.execute_reply.started":"2022-03-15T12:56:02.106453Z","shell.execute_reply":"2022-03-15T12:56:02.320702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### cv score","metadata":{}},{"cell_type":"code","source":"def calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(\" \"))\n    set_gt = set(row.predictionstring_gt.split(\" \"))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter / len_gt\n    overlap_2 = inter / len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp_micro(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n\n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = (\n        gt_df[[\"id\", \"discourse_type\", \"predictionstring\"]]\n        .reset_index(drop=True)\n        .copy()\n    )\n    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n    pred_df[\"pred_id\"] = pred_df.index\n    gt_df[\"gt_id\"] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(\n        gt_df,\n        left_on=[\"id\", \"class\"],\n        right_on=[\"id\", \"discourse_type\"],\n        how=\"outer\",\n        suffixes=(\"_pred\", \"_gt\"),\n    )\n    joined[\"predictionstring_gt\"] = joined[\"predictionstring_gt\"].fillna(\" \")\n    joined[\"predictionstring_pred\"] = joined[\"predictionstring_pred\"].fillna(\" \")\n\n    joined[\"overlaps\"] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined[\"overlap1\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[0])\n    joined[\"overlap2\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[1])\n\n    joined[\"potential_TP\"] = (joined[\"overlap1\"] >= 0.5) & (joined[\"overlap2\"] >= 0.5)\n    joined[\"max_overlap\"] = joined[[\"overlap1\", \"overlap2\"]].max(axis=1)\n    tp_pred_ids = (\n        joined.query(\"potential_TP\")\n        .sort_values(\"max_overlap\", ascending=False)\n        .groupby([\"id\", \"predictionstring_gt\"])\n        .first()[\"pred_id\"]\n        .values\n    )\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined[\"pred_id\"].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query(\"potential_TP\")[\"gt_id\"].unique()\n    unmatched_gt_ids = [c for c in joined[\"gt_id\"].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    # calc microf1\n    f1_score = TP / (TP + 0.5 * (FP + FN))\n    precise_score = TP / (TP+FP)\n    recall_score = TP / (TP+FN)\n    \n    return {'f1':f1_score, 'precise':precise_score, 'recall':recall_score}\n\n\ndef score_feedback_comp(pred_df, gt_df, return_class_scores=True):\n    class_scores = {}\n    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n    for discourse_type, gt_subset in gt_df.groupby(\"discourse_type\"):\n        pred_subset = (\n            pred_df.loc[pred_df[\"class\"] == discourse_type]\n            .reset_index(drop=True)\n            .copy()\n        )\n        class_scores[discourse_type] = score_feedback_comp_micro(pred_subset, gt_subset)\n    f1 = np.mean([v['f1'] for v in class_scores.values()])\n    if return_class_scores:\n        return f1, class_scores\n    return f1","metadata":{"execution":{"iopub.status.busy":"2022-03-15T12:56:02.323497Z","iopub.execute_input":"2022-03-15T12:56:02.324165Z","iopub.status.idle":"2022-03-15T12:56:02.351884Z","shell.execute_reply.started":"2022-03-15T12:56:02.324124Z","shell.execute_reply":"2022-03-15T12:56:02.350875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_oof = train_df[train_df.id.isin(kfold_ids[fold][1])]\nscore_feedback_comp(sub, train_oof)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T12:56:02.353401Z","iopub.execute_input":"2022-03-15T12:56:02.353621Z","iopub.status.idle":"2022-03-15T12:56:08.258125Z","shell.execute_reply.started":"2022-03-15T12:56:02.353593Z","shell.execute_reply":"2022-03-15T12:56:08.257404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}