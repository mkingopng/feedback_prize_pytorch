{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "https://www.kaggle.com/code/cdeotte/2nd-place-solution-cv741-public727-private740/notebook"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 2nd Place Solution - CV 741, Public 727, Private 740\n",
    "\n",
    "We (Chun Ming Lee @leecming , Udbhav Bamba @ubamba98, and Chris Deotte @cdeotte ) are excited to present our 2nd place solution to Kaggle's \"Feedback Prize - Evaluating Student Writing\" Competition. Thank you Georgia State University, The Learning Agency Lab, and Kaggle for an awesome competition.\n",
    "\n",
    "Our full solution write up is here: https://www.kaggle.com/c/feedback-prize-2021/discussion/313389\n",
    "\n",
    "The main ingredients to our solution are\n",
    "- powerful post process per model\n",
    "- huge variety of NLP models trained on NER task\n",
    "- ensemble with weighted box fusion (from ZFTurbo's GitHub here: https://github.com/ZFTurbo/Weighted-Boxes-Fusion).\n",
    "\n",
    "## Inference Script with Post Process\n",
    "The following Python script accepts a filename of a saved model, infers the test texts, applies post process, and writes a submission.csv file with an extra column of confidence scores per span (i.e. the probability that this span is correct). Click \"show hidden cell\" to see the code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%writefile generate_preds.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument('--model_paths', nargs='+', required=True)\n",
    "ap.add_argument(\"--save_name\", type=str, required=True)\n",
    "ap.add_argument(\"--max_len\", type=int, required=True)\n",
    "args = ap.parse_args()\n",
    "\n",
    "if args.save_name == \"yoso\":\n",
    "    os.system(\"cp -r ../input/hf-transformers/transformers-4.16.0 .\")\n",
    "    os.system(\"pip install -U --no-build-isolation --no-deps /kaggle/working/transformers-4.16.0\")\n",
    "\n",
    "if (\"v3\" in args.save_name)|(\"v2\" in args.save_name):\n",
    "    # https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizer\n",
    "    # The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
    "    # This must be done before importing transformers\n",
    "    import shutil\n",
    "    from pathlib import Path\n",
    "\n",
    "    transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n",
    "\n",
    "    input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n",
    "\n",
    "    convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "    conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "    if conversion_path.exists():\n",
    "        conversion_path.unlink()\n",
    "\n",
    "    shutil.copy(convert_file, transformers_path)\n",
    "    deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n",
    "\n",
    "    for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n",
    "        filepath = deberta_v2_path/filename\n",
    "        if filepath.exists():\n",
    "            filepath.unlink()\n",
    "\n",
    "        shutil.copy(input_dir/filename, filepath)\n",
    "\n",
    "if args.save_name == \"longformerwithlstm\":\n",
    "    os.system(\"cp -r ../input/longformerwithbilstmhead/model.py .\")\n",
    "    from model import LongformerForTokenClassificationwithbiLSTM\n",
    "\n",
    "if args.save_name == \"debertawithlstm\":\n",
    "    os.system(\"cp -r ../input/deberta-lstm/model.py .\")\n",
    "    from model import DebertaForTokenClassificationwithbiLSTM\n",
    "\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import multiprocessing as mp\n",
    "from scipy.special import softmax\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (AutoModelForTokenClassification,\n",
    "                          AutoTokenizer,\n",
    "                          TrainingArguments,\n",
    "                          Trainer)\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "NUM_CORES = 16\n",
    "BATCH_SIZE = 4\n",
    "MAX_SEQ_LENGTH = args.max_len\n",
    "PRETRAINED_MODEL_PATHS = args.model_paths\n",
    "if \"debertal_chris\" in args.save_name:\n",
    "    print('==> using -1 in offset mapping...')\n",
    "if (\"v3\" in args.save_name)|(\"v2\" in args.save_name):\n",
    "    print('==> using -1 in offset mapping...')\n",
    "\n",
    "AGG_FUNC = np.mean\n",
    "print('==> using span token mean...')\n",
    "\n",
    "TEST_DIR = '../input/feedback-prize-2021/test/'\n",
    "\n",
    "MIN_TOKENS = {\n",
    "    \"Lead\": 32,\n",
    "    \"Position\": 5,\n",
    "    \"Evidence\": 35,\n",
    "    \"Claim\": 7,\n",
    "    \"Concluding Statement\": 6,\n",
    "    \"Counterclaim\": 6,\n",
    "    \"Rebuttal\": 6\n",
    "}\n",
    "\n",
    "if \"chris\" not in args.save_name:\n",
    "    ner_labels = {'O': 0,\n",
    "                  'B-Lead': 1,\n",
    "                  'I-Lead': 2,\n",
    "                  'B-Position': 3,\n",
    "                  'I-Position': 4,\n",
    "                  'B-Evidence': 5,\n",
    "                  'I-Evidence': 6,\n",
    "                  'B-Claim': 7,\n",
    "                  'I-Claim': 8,\n",
    "                  'B-Concluding Statement': 9,\n",
    "                  'I-Concluding Statement': 10,\n",
    "                  'B-Counterclaim': 11,\n",
    "                  'I-Counterclaim': 12,\n",
    "                  'B-Rebuttal': 13,\n",
    "                  'I-Rebuttal': 14}\n",
    "else:\n",
    "    print(\"==> Using Chris BIO\")\n",
    "    ner_labels = {'O': 14,\n",
    "                  'B-Lead': 0,\n",
    "                  'I-Lead': 1,\n",
    "                  'B-Position': 2,\n",
    "                  'I-Position': 3,\n",
    "                  'B-Evidence': 4,\n",
    "                  'I-Evidence': 5,\n",
    "                  'B-Claim': 6,\n",
    "                  'I-Claim': 7,\n",
    "                  'B-Concluding Statement': 8,\n",
    "                  'I-Concluding Statement': 9,\n",
    "                  'B-Counterclaim': 10,\n",
    "                  'I-Counterclaim': 11,\n",
    "                  'B-Rebuttal': 12,\n",
    "                  'I-Rebuttal': 13}\n",
    "\n",
    "\n",
    "inverted_ner_labels = dict((v,k) for k,v in ner_labels.items())\n",
    "inverted_ner_labels[-100] = 'Special Token'\n",
    "\n",
    "test_files = os.listdir(TEST_DIR)\n",
    "\n",
    "# accepts file path, returns tuple of (file_ID, txt split, NER labels)\n",
    "def generate_text_for_file(input_filename):\n",
    "    curr_id = input_filename.split('.')[0]\n",
    "    with open(os.path.join(TEST_DIR, input_filename)) as f:\n",
    "        curr_txt = f.read()\n",
    "\n",
    "    return curr_id, curr_txt\n",
    "\n",
    "with mp.Pool(NUM_CORES) as p:\n",
    "    ner_test_rows = p.map(generate_text_for_file, test_files)\n",
    "\n",
    "if (\"v3\" in args.save_name)|(\"v2\" in args.save_name):\n",
    "    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
    "    tokenizer = DebertaV2TokenizerFast.from_pretrained(PRETRAINED_MODEL_PATHS[0])\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_PATHS[0])\n",
    "# Check is rust-based fast tokenizer\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "\n",
    "ner_test_rows = sorted(ner_test_rows, key=lambda x: len(tokenizer(x[1], max_length=MAX_SEQ_LENGTH, truncation=True)['input_ids']))\n",
    "\n",
    "# tokenize and store word ids\n",
    "def tokenize_with_word_ids(ner_raw_data):\n",
    "    # ner_raw_data is shaped (num_examples, 3) where cols are (ID, words, word-level labels)\n",
    "    tokenized_inputs = tokenizer([x[1] for x in ner_raw_data],\n",
    "                                 max_length=MAX_SEQ_LENGTH,\n",
    "                                 return_offsets_mapping=True,\n",
    "                                 truncation=True)\n",
    "\n",
    "    tokenized_inputs['id'] = [x[0] for x in ner_raw_data]\n",
    "    tokenized_inputs['offset_mapping'] = [tokenized_inputs['offset_mapping'][i] for i in range(len(ner_raw_data))]\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_all = tokenize_with_word_ids(ner_test_rows)\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, input_dict):\n",
    "        self.input_dict = input_dict\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {k:self.input_dict[k][index] for k in self.input_dict.keys() if k not in {'id', 'offset_mapping'}}\n",
    "\n",
    "    def get_filename(self, index):\n",
    "        return self.input_dict['id'][index]\n",
    "\n",
    "    def get_offset(self, index):\n",
    "        return self.input_dict['offset_mapping'][index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_dict['input_ids'])\n",
    "\n",
    "test_dataset = NERDataset(tokenized_all)\n",
    "\n",
    "soft_predictions = None\n",
    "hfargs = TrainingArguments(output_dir='None',\n",
    "                         log_level='warning',\n",
    "                         per_device_eval_batch_size=BATCH_SIZE)\n",
    "\n",
    "for idx, curr_path in enumerate(PRETRAINED_MODEL_PATHS):\n",
    "\n",
    "    if args.save_name == \"longformerwithlstm\":\n",
    "        model = LongformerForTokenClassificationwithbiLSTM.from_pretrained(curr_path)\n",
    "    elif args.save_name == \"debertawithlstm\":\n",
    "        model = DebertaForTokenClassificationwithbiLSTM.from_pretrained(curr_path)\n",
    "    else:\n",
    "        model = AutoModelForTokenClassification.from_pretrained(curr_path, trust_remote_code=True)\n",
    "    trainer = Trainer(model,\n",
    "                      hfargs,\n",
    "                      tokenizer=tokenizer)\n",
    "\n",
    "    curr_preds, _, _ = trainer.predict(test_dataset)\n",
    "    curr_preds = curr_preds.astype(np.float16)\n",
    "    curr_preds = softmax(curr_preds, -1)\n",
    "\n",
    "    if soft_predictions is not None:\n",
    "        soft_predictions = soft_predictions + curr_preds\n",
    "    else:\n",
    "        soft_predictions = curr_preds\n",
    "\n",
    "    del model, trainer, curr_preds\n",
    "    gc.collect()\n",
    "\n",
    "soft_predictions = soft_predictions / len(PRETRAINED_MODEL_PATHS)\n",
    "\n",
    "soft_claim_predictions = soft_predictions[:, :, 8]\n",
    "\n",
    "predictions = np.argmax(soft_predictions, axis=2)\n",
    "soft_predictions = np.max(soft_predictions, axis=2)\n",
    "\n",
    "def generate_token_to_word_mapping(txt, offset):\n",
    "    # GET WORD POSITIONS IN CHARS\n",
    "    w = []\n",
    "    blank = True\n",
    "    for i in range(len(txt)):\n",
    "        if not txt[i].isspace() and blank==True:\n",
    "            w.append(i)\n",
    "            blank=False\n",
    "        elif txt[i].isspace():\n",
    "            blank=True\n",
    "    w.append(1e6)\n",
    "\n",
    "    # MAPPING FROM TOKENS TO WORDS\n",
    "    word_map = -1 * np.ones(len(offset),dtype='int32')\n",
    "    w_i = 0\n",
    "    for i in range(len(offset)):\n",
    "        if offset[i][1]==0: continue\n",
    "        while offset[i][0]>=(w[w_i+1]-(\"debertal_chris\" in args.save_name)-(\"v3\" in args.save_name)\\\n",
    "                             -(\"v2\" in args.save_name) ): w_i += 1\n",
    "        word_map[i] = int(w_i)\n",
    "\n",
    "    return word_map\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "# Clumsy gathering of predictions at word lvl - only populate with 1st subword pred\n",
    "for curr_sample_id in range(len(test_dataset)):\n",
    "    curr_preds = []\n",
    "    sample_preds = predictions[curr_sample_id]\n",
    "    sample_offset = test_dataset.get_offset(curr_sample_id)\n",
    "    sample_txt = ner_test_rows[curr_sample_id][1]\n",
    "    sample_word_map = generate_token_to_word_mapping(sample_txt, sample_offset)\n",
    "\n",
    "    word_preds = [''] * (max(sample_word_map) + 1)\n",
    "    word_probs = dict(zip(range((max(sample_word_map) + 1)),[0]*(max(sample_word_map) + 1)))\n",
    "    claim_probs = dict(zip(range((max(sample_word_map) + 1)),[0]*(max(sample_word_map) + 1)))\n",
    "\n",
    "    for i, curr_word_id in enumerate(sample_word_map):\n",
    "        if curr_word_id != -1:\n",
    "            if word_preds[curr_word_id] == '': # only use 1st subword\n",
    "                word_preds[curr_word_id] = inverted_ner_labels[sample_preds[i]]\n",
    "                word_probs[curr_word_id] = soft_predictions[curr_sample_id, i]\n",
    "                claim_probs[curr_word_id] = soft_claim_predictions[curr_sample_id, i]\n",
    "            elif 'B-' in inverted_ner_labels[sample_preds[i]]:\n",
    "                word_preds[curr_word_id] = inverted_ner_labels[sample_preds[i]]\n",
    "                word_probs[curr_word_id] = soft_predictions[curr_sample_id, i]\n",
    "                claim_probs[curr_word_id] = soft_claim_predictions[curr_sample_id, i]\n",
    "\n",
    "    # Dict to hold Lead, Position, Concluding Statement\n",
    "    let_one_dict = dict() # K = Type, V = (Prob of start token, start, end)\n",
    "\n",
    "    # If we see tokens I-X, I-Y, I-X -> change I-Y to I-X\n",
    "    for j in range(1, len(word_preds) - 1):\n",
    "        pred_trio = [word_preds[k] for k in [j - 1, j, j + 1]]\n",
    "        splitted_trio = [x.split('-')[0] for x in pred_trio]\n",
    "        if all([x == 'I' for x in splitted_trio]) and pred_trio[0] == pred_trio[2] and pred_trio[0] != pred_trio[1]:\n",
    "            word_preds[j] = word_preds[j-1]\n",
    "\n",
    "    # B-X, ? (not B), I-X -> change ? to I-X\n",
    "    for j in range(1, len(word_preds) - 1):\n",
    "        if 'B-' in word_preds[j-1] and word_preds[j+1] == f\"I-{word_preds[j-1].split('-')[-1]}\" and word_preds[j] != word_preds[j+1] and 'B-' not in word_preds[j]:\n",
    "            word_preds[j] = word_preds[j+1]\n",
    "\n",
    "     # If we see tokens I-X, O, I-X, change center token to the same for stated discourse types\n",
    "    for j in range(1, len(word_preds) - 1):\n",
    "        if word_preds[j - 1] in ['I-Lead', 'I-Position', 'I-Concluding Statement'] and word_preds[j-1] == word_preds[j+1] and word_preds[j] == 'O':\n",
    "            word_preds[j] = word_preds[j-1]\n",
    "\n",
    "    j = 0 # start of candidate discourse\n",
    "    while j < len(word_preds):\n",
    "        cls = word_preds[j]\n",
    "        cls_splitted = cls.split('-')[-1]\n",
    "        end = j + 1 # try to extend discourse as far as possible\n",
    "\n",
    "        if word_probs[j] > 0.54:\n",
    "            # Must match suffix i.e., I- to I- only; no B- to I-\n",
    "            while end < len(word_preds) and (word_preds[end].split('-')[-1] == cls_splitted if cls_splitted in ['Lead', 'Position', 'Concluding Statement'] else word_preds[end] == f'I-{cls_splitted}'):\n",
    "                end += 1\n",
    "            # if we're here, end is not the same pred as start\n",
    "            if cls != 'O' and (end - j > MIN_TOKENS[cls_splitted] or max(word_probs[l] for l in range(j, end)) > 0.73): # needs to be longer than class-specified min\n",
    "                if cls_splitted in ['Lead', 'Position', 'Concluding Statement']:\n",
    "                    lpc_max_prob = max(word_probs[c] for c in range(j, end))\n",
    "                    if cls_splitted in let_one_dict: # Already existing, check contiguous or higher prob\n",
    "                        prev_prob, prev_start, prev_end = let_one_dict[cls_splitted]\n",
    "                        if cls_splitted in ['Lead', 'Concluding Statement'] and j - prev_end < 49: # If close enough, combine\n",
    "                            let_one_dict[cls_splitted] = (max(prev_prob, lpc_max_prob), prev_start, end)\n",
    "\n",
    "                            # Delete other preds that lie inside the joined LC discourse\n",
    "                            for l in range(len(curr_preds) - 1, 0, -1):\n",
    "                                check_span = curr_preds[l][2]\n",
    "                                check_start, check_end = int(check_span[0]), int(check_span[-1])\n",
    "                                if check_start > prev_start and check_end < end:\n",
    "                                    del curr_preds[l]\n",
    "\n",
    "                        elif lpc_max_prob > prev_prob: # Overwrite if current candidate is more likely\n",
    "                            let_one_dict[cls_splitted] = (lpc_max_prob, j, end)\n",
    "                    else: # Add to it\n",
    "                        let_one_dict[cls_splitted] = (lpc_max_prob, j, end)\n",
    "                else:\n",
    "                    # Lookback and add preceding I- tokens\n",
    "                    while j - 1 > 0 and word_preds[j-1] == cls:\n",
    "                        j = j - 1\n",
    "                    # Try to add the matching B- tag if immediately precedes the current I- sequence\n",
    "                    if j - 1 > 0 and word_preds[j-1] == f'B-{cls_splitted}':\n",
    "                        j = j - 1\n",
    "\n",
    "\n",
    "                    #############################################################\n",
    "                    # Run a bunch of adjustments to discourse predictions based on CV\n",
    "                    adj_start, adj_end = j, end + 1\n",
    "\n",
    "                    # Run some heuristics against previous discourse\n",
    "                    if len(curr_preds) > 0:\n",
    "                        prev_span = list(map(int, curr_preds[-1][2].split()))\n",
    "                        prev_start, prev_end = prev_span[0], prev_span[-1]\n",
    "\n",
    "                        # Join adjacent rebuttals\n",
    "                        if cls_splitted in 'Rebuttal':\n",
    "                            if curr_preds[-1][1] == cls_splitted and adj_start - prev_end < 32:\n",
    "                                del curr_preds[-1]\n",
    "                                combined_list = prev_span + list(range(adj_start, adj_end))\n",
    "                                curr_preds.append((test_dataset.get_filename(curr_sample_id),\n",
    "                                                   cls_splitted,\n",
    "                                                   ' '.join(map(str, combined_list)),\n",
    "                                                   AGG_FUNC([word_probs[i] for i in combined_list if i in word_probs.keys()])))\n",
    "                                j = end\n",
    "                                continue\n",
    "\n",
    "                        elif cls_splitted in 'Counterclaim':\n",
    "                            if curr_preds[-1][1] == cls_splitted and adj_start - prev_end < 24:\n",
    "                                del curr_preds[-1]\n",
    "                                combined_list = prev_span + list(range(adj_start, adj_end))\n",
    "                                curr_preds.append((test_dataset.get_filename(curr_sample_id),\n",
    "                                                   cls_splitted,\n",
    "                                                   ' '.join(map(str, combined_list)),\n",
    "                                                  AGG_FUNC([word_probs[i] for i in combined_list if i in word_probs.keys()])))\n",
    "                                j = end\n",
    "                                continue\n",
    "\n",
    "                        elif cls_splitted in 'Evidence':\n",
    "                            if curr_preds[-1][1] == cls_splitted and 8 < adj_start - prev_end < 25:\n",
    "                                if max(claim_probs[l] for l in range(prev_end+1, adj_start)) > 0.35:\n",
    "                                    claim_tokens = [str(l) for l in range(prev_end+1, adj_start) if claim_probs[l] > 0.15]\n",
    "                                    if len(claim_tokens) > 2:\n",
    "                                        curr_preds.append((test_dataset.get_filename(curr_sample_id),\n",
    "                                                           'Claim',\n",
    "                                                           ' '.join(claim_tokens),\n",
    "                                                           AGG_FUNC([word_probs[int(i)] for i in claim_tokens if int(i) in word_probs.keys()])))\n",
    "                        # If gap with discourse of same type, extend to it\n",
    "                        elif curr_preds[-1][1] == cls_splitted and adj_start - prev_end > 2:\n",
    "                            adj_start -= 1\n",
    "\n",
    "                    # Adjust discourse lengths if too long or short\n",
    "                    if cls_splitted == 'Evidence':\n",
    "                        if adj_end - adj_start < 45:\n",
    "                            adj_start -= 9\n",
    "                        else:\n",
    "                            adj_end -= 1\n",
    "                    elif cls_splitted == 'Claim':\n",
    "                        if adj_end - adj_start > 24:\n",
    "                            adj_end -= 1\n",
    "                    elif cls_splitted == 'Counterclaim':\n",
    "                        if adj_end - adj_start > 24:\n",
    "                            adj_end -= 1\n",
    "                        else:\n",
    "                            adj_start -= 1\n",
    "                            adj_end += 1\n",
    "                    elif cls_splitted == 'Rebuttal':\n",
    "                        if adj_end - adj_start > 32:\n",
    "                            adj_end -= 1\n",
    "                        else:\n",
    "                            adj_start -= 1\n",
    "                            adj_end += 1\n",
    "                    adj_start = max(0, adj_start)\n",
    "                    adj_end = min(len(word_preds) - 1, adj_end)\n",
    "                    curr_preds.append((test_dataset.get_filename(curr_sample_id),\n",
    "                                       cls_splitted,\n",
    "                                       ' '.join(map(str, list(range(adj_start, adj_end)))),\n",
    "                                       AGG_FUNC([word_probs[i] for i in range(adj_start, adj_end) if i in word_probs.keys()])))\n",
    "\n",
    "        j = end\n",
    "\n",
    "    # Add the Lead, Position, Concluding Statement\n",
    "    for k, v in let_one_dict.items():\n",
    "        pred_start = v[1]\n",
    "        pred_end = v[2]\n",
    "\n",
    "        # Lookback and add preceding I- tokens\n",
    "        while pred_start - 1 > 0 and word_preds[pred_start-1] == f'I-{k}':\n",
    "            pred_start = pred_start - 1\n",
    "        # Try to add the matching B- tag if immediately precedes the current I- sequence\n",
    "        if pred_start - 1 > 0 and word_preds[pred_start - 1] == f'B-{k}':\n",
    "            pred_start = pred_start - 1\n",
    "\n",
    "        # Extend short Leads and Concluding Statements\n",
    "        if k == 'Lead':\n",
    "            if pred_end - pred_start < 33:\n",
    "                pred_end = min(len(word_preds), pred_end + 5)\n",
    "            else:\n",
    "                pred_end -= 5\n",
    "        elif k == 'Concluding Statement':\n",
    "            if pred_end - pred_start < 23:\n",
    "                pred_start = max(0, pred_start - 1)\n",
    "                pred_end = min(len(word_preds), pred_end + 10)\n",
    "        elif k == 'Position':\n",
    "            if pred_end - pred_start < 18:\n",
    "                pred_end = min(len(word_preds), pred_end + 3)\n",
    "\n",
    "        pred_start = max(0, pred_start)\n",
    "        if pred_end - pred_start > 6:\n",
    "            curr_preds.append((test_dataset.get_filename(curr_sample_id),\n",
    "                               k,\n",
    "                               ' '.join(map(str, list(range(pred_start, pred_end)))),\n",
    "                               AGG_FUNC([word_probs[i] for i in range(pred_start, pred_end) if i in word_probs.keys()])))\n",
    "\n",
    "    all_preds.extend(curr_preds)\n",
    "\n",
    "output_df = pd.DataFrame(all_preds)\n",
    "output_df.columns = ['id', 'class', 'predictionstring', 'scores']\n",
    "output_df.to_csv(f'{args.save_name}.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Weighted Box Fusion\n",
    "The following code comes from ZFTurbo's GitHub here: https://github.com/ZFTurbo/Weighted-Boxes-Fusion\n",
    "\n",
    "First, the text predictionstring are converted to 1 dimensional boxes. Next they are ensembled with ZFTurbo's WBF code. Click \"show hidden cell\" to see the code."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Code taken and modified for 1D sequences from:\n",
    "https://github.com/ZFTurbo/Weighted-Boxes-Fusion/blob/master/ensemble_boxes/ensemble_boxes_wbf.py\n",
    "'''\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "def prefilter_boxes(boxes, scores, labels, weights, thr):\n",
    "    # Create dict with boxes stored by its label\n",
    "    new_boxes = dict()\n",
    "\n",
    "    for t in range(len(boxes)):\n",
    "\n",
    "        if len(boxes[t]) != len(scores[t]):\n",
    "            print('Error. Length of boxes arrays not equal to length of scores array: {} != {}'.format(len(boxes[t]), len(scores[t])))\n",
    "            exit()\n",
    "\n",
    "        for j in range(len(boxes[t])):\n",
    "            score = scores[t][j]\n",
    "            if score < thr:\n",
    "                continue\n",
    "            label = labels[t][j]\n",
    "            box_part = boxes[t][j]\n",
    "\n",
    "            x = float(box_part[0])\n",
    "            y = float(box_part[1])\n",
    "\n",
    "            # Box data checks\n",
    "            if y < x:\n",
    "                warnings.warn('Y < X value in box. Swap them.')\n",
    "                x, y = y, x\n",
    "\n",
    "            # [label, score, weight, model index, x, y]\n",
    "            b = [label, float(score) * weights[t], weights[t], t, x, y]\n",
    "            if label not in new_boxes:\n",
    "                new_boxes[label] = []\n",
    "            new_boxes[label].append(b)\n",
    "\n",
    "    # Sort each list in dict by score and transform it to numpy array\n",
    "    for k in new_boxes:\n",
    "        current_boxes = np.array(new_boxes[k])\n",
    "        new_boxes[k] = current_boxes[current_boxes[:, 1].argsort()[::-1]]\n",
    "\n",
    "    return new_boxes\n",
    "\n",
    "\n",
    "def get_weighted_box(boxes, conf_type='avg'):\n",
    "    \"\"\"\n",
    "    Create weighted box for set of boxes\n",
    "    :param boxes: set of boxes to fuse\n",
    "    :param conf_type: type of confidence one of 'avg' or 'max'\n",
    "    :return: weighted box (label, score, weight, model index, x, y)\n",
    "    \"\"\"\n",
    "\n",
    "    box = np.zeros(6, dtype=np.float32)\n",
    "    conf = 0\n",
    "    conf_list = []\n",
    "    w = 0\n",
    "    for b in boxes:\n",
    "        box[4:] += (b[1] * b[4:])\n",
    "        conf += b[1]\n",
    "        conf_list.append(b[1])\n",
    "        w += b[2]\n",
    "    box[0] = boxes[0][0]\n",
    "    if conf_type == 'avg':\n",
    "        box[1] = conf / len(boxes)\n",
    "    elif conf_type == 'max':\n",
    "        box[1] = np.array(conf_list).max()\n",
    "    elif conf_type in ['box_and_model_avg', 'absent_model_aware_avg']:\n",
    "        box[1] = conf / len(boxes)\n",
    "    box[2] = w\n",
    "    box[3] = -1 # model index field is retained for consistensy but is not used.\n",
    "    box[4:] /= conf\n",
    "    return box\n",
    "\n",
    "\n",
    "def find_matching_box_quickly(boxes_list, new_box, match_iou):\n",
    "    \"\"\"\n",
    "        Reimplementation of find_matching_box with numpy instead of loops. Gives significant speed up for larger arrays\n",
    "        (~100x). This was previously the bottleneck since the function is called for every entry in the array.\n",
    "\n",
    "        boxes_list: shape: (N, label, score, weight, model index, x, y)\n",
    "        new_box: shape: (label, score, weight, model index, x, y)\n",
    "    \"\"\"\n",
    "    def bb_iou_array(boxes, new_box):\n",
    "        '''\n",
    "        boxes: shape: (N, x, y)\n",
    "        new_box: shape: (x, y)\n",
    "        '''\n",
    "        # bb interesection over union\n",
    "        x_min = np.minimum(boxes[:, 0], new_box[0])\n",
    "        x_max = np.maximum(boxes[:, 0], new_box[0])\n",
    "        y_min = np.minimum(boxes[:, 1], new_box[1])+1\n",
    "        y_max = np.maximum(boxes[:, 1], new_box[1])+1\n",
    "\n",
    "        iou = np.maximum(0, (y_min-x_max)/(y_max-x_min))\n",
    "\n",
    "        return iou\n",
    "\n",
    "    if boxes_list.shape[0] == 0:\n",
    "        return -1, match_iou\n",
    "\n",
    "    # boxes = np.array(boxes_list)\n",
    "    boxes = boxes_list\n",
    "\n",
    "    ious = bb_iou_array(boxes[:, 4:], new_box[4:])\n",
    "\n",
    "    ious[boxes[:, 0] != new_box[0]] = -1\n",
    "\n",
    "    best_idx = np.argmax(ious)\n",
    "    best_iou = ious[best_idx]\n",
    "\n",
    "    if best_iou <= match_iou:\n",
    "        best_iou = match_iou\n",
    "        best_idx = -1\n",
    "\n",
    "    return best_idx, best_iou\n",
    "\n",
    "\n",
    "def weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=None, iou_thr=0.55, skip_box_thr=0.0, conf_type='avg', allows_overflow=False):\n",
    "    '''\n",
    "    :param boxes_list: list of boxes predictions from each model, each box is 2 numbers.\n",
    "     It has 3 dimensions (models_number, model_preds, 2)\n",
    "     Order of boxes: x, y.\n",
    "    :param scores_list: list of scores for each model\n",
    "    :param labels_list: list of labels for each model\n",
    "    :param weights: list of weights for each model. Default: None, which means weight == 1 for each model\n",
    "    :param iou_thr: IoU value for boxes to be a match\n",
    "    :param skip_box_thr: exclude boxes with score lower than this variable\n",
    "    :param conf_type: how to calculate confidence in weighted boxes. 'avg': average value, 'max': maximum value, 'box_and_model_avg': box and model wise hybrid weighted average, 'absent_model_aware_avg': weighted average that takes into account the absent model.\n",
    "    :param allows_overflow: false if we want confidence score not exceed 1.0\n",
    "\n",
    "    :return: boxes: boxes coordinates (Order of boxes: x, y).\n",
    "    :return: scores: confidence scores\n",
    "    :return: labels: boxes labels\n",
    "    '''\n",
    "\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(boxes_list))\n",
    "    if len(weights) != len(boxes_list):\n",
    "        print('Warning: incorrect number of weights {}. Must be: {}. Set weights equal to 1.'.format(len(weights), len(boxes_list)))\n",
    "        weights = np.ones(len(boxes_list))\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    if conf_type not in ['avg', 'max', 'box_and_model_avg', 'absent_model_aware_avg']:\n",
    "        print('Unknown conf_type: {}. Must be \"avg\", \"max\" or \"box_and_model_avg\", or \"absent_model_aware_avg\"'.format(conf_type))\n",
    "        exit()\n",
    "\n",
    "    filtered_boxes = prefilter_boxes(boxes_list, scores_list, labels_list, weights, skip_box_thr)\n",
    "    if len(filtered_boxes) == 0:\n",
    "        return np.zeros((0, 2)), np.zeros((0,)), np.zeros((0,))\n",
    "\n",
    "    overall_boxes = []\n",
    "    for label in filtered_boxes:\n",
    "        boxes = filtered_boxes[label]\n",
    "        new_boxes = []\n",
    "        weighted_boxes = np.empty((0,6)) ## [label, score, weight, model index, x, y]\n",
    "        # Clusterize boxes\n",
    "        for j in range(0, len(boxes)):\n",
    "            index, best_iou = find_matching_box_quickly(weighted_boxes, boxes[j], iou_thr)\n",
    "\n",
    "            if index != -1:\n",
    "                new_boxes[index].append(boxes[j])\n",
    "                weighted_boxes[index] = get_weighted_box(new_boxes[index], conf_type)\n",
    "            else:\n",
    "                new_boxes.append([boxes[j].copy()])\n",
    "                weighted_boxes = np.vstack((weighted_boxes, boxes[j].copy()))\n",
    "\n",
    "        # Rescale confidence based on number of models and boxes\n",
    "        for i in range(len(new_boxes)):\n",
    "            clustered_boxes = np.array(new_boxes[i])\n",
    "            if conf_type == 'box_and_model_avg':\n",
    "                # weighted average for boxes\n",
    "                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weighted_boxes[i, 2]\n",
    "                # identify unique model index by model index column\n",
    "                _, idx = np.unique(clustered_boxes[:, 3], return_index=True)\n",
    "                # rescale by unique model weights\n",
    "                weighted_boxes[i, 1] = weighted_boxes[i, 1] *  clustered_boxes[idx, 2].sum() / weights.sum()\n",
    "            elif conf_type == 'absent_model_aware_avg':\n",
    "                # get unique model index in the cluster\n",
    "                models = np.unique(clustered_boxes[:, 3]).astype(int)\n",
    "                # create a mask to get unused model weights\n",
    "                mask = np.ones(len(weights), dtype=bool)\n",
    "                mask[models] = False\n",
    "                # absent model aware weighted average\n",
    "                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / (weighted_boxes[i, 2] + weights[mask].sum())\n",
    "            elif conf_type == 'max':\n",
    "                weighted_boxes[i, 1] = weighted_boxes[i, 1] / weights.max()\n",
    "            elif not allows_overflow:\n",
    "                weighted_boxes[i, 1] = weighted_boxes[i, 1] * min(len(weights), len(clustered_boxes)) / weights.sum()\n",
    "            else:\n",
    "                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weights.sum()\n",
    "\n",
    "        # REQUIRE BBOX TO BE PREDICTED BY AT LEAST 2 MODELS\n",
    "        #for i in range(len(new_boxes)):\n",
    "        #    clustered_boxes = np.array(new_boxes[i])\n",
    "        #    if len(np.unique(clustered_boxes[:, 3])) > 1:\n",
    "        #        overall_boxes.append(weighted_boxes[i])\n",
    "\n",
    "        overall_boxes.append(weighted_boxes) # NOT NEEDED FOR \"REQUIRE TWO MODELS\" ABOVE\n",
    "    overall_boxes = np.concatenate(overall_boxes, axis=0) # NOT NEEDED FOR \"REQUIRE TWO MODELS\" ABOVE\n",
    "    #overall_boxes = np.array(overall_boxes) # NEEDED FOR \"REQUIRE TWO MODELS\" ABOVE\n",
    "    overall_boxes = overall_boxes[overall_boxes[:, 1].argsort()[::-1]]\n",
    "    boxes = overall_boxes[:, 4:]\n",
    "    scores = overall_boxes[:, 1]\n",
    "    labels = overall_boxes[:, 0]\n",
    "    return boxes, scores, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Our NLP models trained on NER task\n",
    "\n",
    "Here is a summary of our models and their individual performance:\n",
    "\n",
    "| HuggingFace Model             | CV | Public LB | Private LB | Special                         |\n",
    "|-------------------------------|----|-----------|------------|---------------------------------|\n",
    "| microsoft/deberta-large       | 706 | 710       | 721        | trained with 100% train data    |\n",
    "| microsoft/deberta-large       | 699 | 706       | 719        | add lstm, add jacard loss       |\n",
    "| microsoft/deberta-v3-large    | 705 |           |            | convert slow tokenizer to fast  |\n",
    "| microsoft/deberta-xlarge      | 708 | 704       | 713        |                                 |\n",
    "| microsoft/deberta-v2-xlarge   | 702 |           |            | convert slow tokenizere to fast |\n",
    "| allenai/longformer-large-4096 | 702 | 705       | 716        | add lstm head                   |\n",
    "| lsg converted roberta         | 703 | 702       | 714        | convert 512 roberta to 1536     |\n",
    "| funnel-transformer/large      | 688 | 689       | 708        |                                 |\n",
    "| ggoogle/bigbird-roberta-base  | 675 | 676       | 692        | train 1024 infer 1024           |\n",
    "| uw-madison/yoso-4096          | 652 | 655       | 668        | lsh_backward = False            |\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python generate_preds.py --model_paths ../input/longformerwithbilstmhead/aug-longformer-large-4096-f0/checkpoint-5500 \\\n",
    "                                        ../input/longformerwithbilstmhead/aug-longformer-large-4096-f2/checkpoint-7500 \\\n",
    "                                        ../input/longformerwithbilstmhead/aug-longformer-large-4096-f5/checkpoint-6000 \\\n",
    "                            --save_name longformerwithlstm --max_len 1536\n",
    "\n",
    "!python generate_preds.py --model_paths ../input/deberta-large-100/fold0 \\\n",
    "                                        ../input/deberta-large-100/fold1 \\\n",
    "                                        ../input/deberta-large-100/fold2 \\\n",
    "                            --save_name debertal_chris --max_len 1536\n",
    "\n",
    "!python generate_preds.py --model_paths ../input/deberta-large-v2/deberta-large-v2100-f0/checkpoint-10500 \\\n",
    "                                        ../input/deberta-large-v2/deberta-large-v2101-f1/checkpoint-11500 \\\n",
    "                                        ../input/deberta-large-v2/deberta-large-v2102-f2/checkpoint-8500 \\\n",
    "                            --save_name debertal --max_len 1536\n",
    "\n",
    "!python generate_preds.py --model_paths ../input/deberta-xlarge-1536/deberta-xlarge-v8004-f4/checkpoint-14000 \\\n",
    "                                        ../input/deberta-xlarge-1536/deberta-xlarge-v4005-f5/checkpoint-13000 \\\n",
    "                            --save_name debertaxl --max_len 1536\n",
    "\n",
    "!python generate_preds.py --model_paths ../input/deberta-v2-xlarge/deberta-v2-xlarge-v6000-f0/checkpoint-7500 \\\n",
    "                                        ../input/deberta-v2-xlarge/deberta-v2-xlarge-v6003-f3/checkpoint-9000 \\\n",
    "                            --save_name deberta_v2 --max_len 1536\n",
    "\n",
    "!python generate_preds.py --model_paths ../input/deberta-lstm-jaccard/jcl-deberta-large-f1/checkpoint-4500 \\\n",
    "                                        ../input/deberta-lstm-jaccard/jcl-deberta-large-f2/checkpoint-5000 \\\n",
    "                                        ../input/deberta-lstm-jaccard/jcl-deberta-large-f3/checkpoint-3500 \\\n",
    "                            --save_name debertawithlstm --max_len 1536\n",
    "\n",
    "!python generate_preds.py --model_paths  ../input/funnel-large-6folds/large-v628-f1/checkpoint-11500 \\\n",
    "                                         ../input/funnel-large-6folds/large-v627-f3/checkpoint-11000 \\\n",
    "                                         ../input/funnel-large-6folds/large-v623-f4/checkpoint-10500 \\\n",
    "                            --save_name funnel --max_len 1536\n",
    "\n",
    "!python generate_preds.py --model_paths  ../input/auglsgrobertalarge/lsg-roberta-large-0/checkpoint-6750 \\\n",
    "                                         ../input/auglsgrobertalarge/lsg-roberta-large-2/checkpoint-7000 \\\n",
    "                                         ../input/auglsgrobertalarge/lsg-roberta-large-5/checkpoint-6500 \\\n",
    "                             --save_name lsg --max_len 1536\n",
    "\n",
    "!python generate_preds.py --model_paths  ../input/bird-base/fold1 \\\n",
    "                                         ../input/bird-base/fold3 \\\n",
    "                                         ../input/bird-base/fold5 \\\n",
    "                            --save_name bigbird_base_chris --max_len 1024\n",
    "\n",
    "!python generate_preds.py --model_paths  ../input/feedbackyoso/yoso-4096-0/checkpoint-12500 \\\n",
    "                                         ../input/feedbackyoso/yoso-4096-2/checkpoint-11000 \\\n",
    "                                         ../input/feedbackyoso/yoso-4096-4/checkpoint-12500 \\\n",
    "                            --save_name yoso --max_len 1536"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np, os\n",
    "\n",
    "longformer_csv = pd.read_csv(\"longformerwithlstm.csv\").dropna()\n",
    "deberta_v3_csv = pd.read_csv(\"debertawithlstm.csv\").dropna()\n",
    "deberta_v2_csv = pd.read_csv(\"deberta_v2.csv\").dropna()\n",
    "debertaxl_csv = pd.read_csv(\"debertaxl.csv\").dropna()\n",
    "debertal_chris_csv = pd.read_csv(\"debertal_chris.csv\").dropna()\n",
    "debertal_csv = pd.read_csv(\"debertal.csv\").dropna()\n",
    "yoso_csv = pd.read_csv(\"yoso.csv\").dropna()\n",
    "funnel_csv = pd.read_csv(\"funnel.csv\").dropna()\n",
    "bird_base_chris_csv = pd.read_csv(\"bigbird_base_chris.csv\").dropna()\n",
    "lsg_csv = pd.read_csv(\"lsg.csv\").dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensemble Models with WBF\n",
    "We will now read in the 10 submission files generated above and apply WBF to ensemble them. After applying WBF, it is important to remove predictions with confidence score below threshold. This is explained here: https://www.kaggle.com/c/tensorflow-great-barrier-reef/discussion/307609\n",
    "\n",
    "If only 1 model out of 10 models makes a certain span prediction, that prediction will still be present in WBF's outcome. However that prediction will have a very low confidence score because that model's confidence score will be averaged with 9 zero confidence scores. We found optimal confidence scores per class by analyzing our CV OOF score. For each class, we vary the threshold and compute the corresponding class metric score.\n",
    "\n",
    "<img src=\"2nd plaace plots.png\"/>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TEST_DIR = '../input/feedback-prize-2021/test/'\n",
    "test_files = os.listdir(TEST_DIR)\n",
    "v_ids = [f.replace('.txt','') for f in test_files]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class_to_label = {\n",
    "    'Claim': 0,\n",
    "    'Evidence': 1,\n",
    "    'Lead':2,\n",
    "    'Position':3,\n",
    "    'Concluding Statement':4,\n",
    "    'Counterclaim':5,\n",
    "    'Rebuttal':6\n",
    "}\n",
    "\n",
    "# Threshold found from CV\n",
    "label_to_threshold = {\n",
    "    0 : 0.275, #Claim\n",
    "    1 : 0.375, #Evidence\n",
    "    2 : 0.325, #Lead\n",
    "    3 : 0.325, #Position\n",
    "    4 : 0.4, #Concluding Statement\n",
    "    5 : 0.275, #Counterclaim\n",
    "    6 : 0.275 #Rebuttal\n",
    "}\n",
    "\n",
    "label_to_class = {v:k for k, v in class_to_label.items()}\n",
    "\n",
    "def preprocess_for_wbf(df_list):\n",
    "    boxes_list=[]\n",
    "    scores_list=[]\n",
    "    labels_list=[]\n",
    "\n",
    "    for df in df_list:\n",
    "        scores_list.append(df['scores'].values.tolist())\n",
    "        labels_list.append(df['class'].map(class_to_label).values.tolist())\n",
    "        predictionstring = df.predictionstring.str.split().values\n",
    "        df_box_list = []\n",
    "        for bb in predictionstring:\n",
    "            df_box_list.append([int(bb[0]), int(bb[-1])])\n",
    "        boxes_list.append(df_box_list)\n",
    "    return boxes_list, scores_list, labels_list\n",
    "\n",
    "def postprocess_for_wbf(idx, boxes_list, scores_list, labels_list):\n",
    "    preds = []\n",
    "    for box, score, label in zip(boxes_list, scores_list, labels_list):\n",
    "        if score > label_to_threshold[label]:\n",
    "            start = math.ceil(box[0])\n",
    "            end = int(box[1])\n",
    "            preds.append((idx, label_to_class[label], ' '.join([str(x) for x in range(start, end+1)])))\n",
    "    return preds\n",
    "\n",
    "def generate_wbf_for_id(i):\n",
    "    df1 = debertal_csv[debertal_csv['id']==i]\n",
    "    df2 = debertal_chris_csv[debertal_chris_csv['id']==i]\n",
    "    df3 = funnel_csv[funnel_csv['id']==i]\n",
    "    df4 = debertaxl_csv[debertaxl_csv['id']==i]\n",
    "    df5 = longformer_csv[longformer_csv['id']==i]\n",
    "    df6 = deberta_v3_csv[deberta_v3_csv['id']==i]\n",
    "    df7 = yoso_csv[yoso_csv['id']==i]\n",
    "    df8 = bird_base_chris_csv[bird_base_chris_csv['id']==i]\n",
    "    df9 = lsg_csv[lsg_csv['id']==i]\n",
    "    df10 = deberta_v2_csv[deberta_v2_csv['id']==i]\n",
    "\n",
    "    boxes_list, scores_list, labels_list = preprocess_for_wbf([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10])\n",
    "    nboxes_list, nscores_list, nlabels_list = weighted_boxes_fusion(boxes_list, scores_list, labels_list, iou_thr=0.33, conf_type='avg')\n",
    "\n",
    "    return postprocess_for_wbf(i, nboxes_list, nscores_list, nlabels_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "with mp.Pool(2) as p:\n",
    "    list_of_list = p.map(generate_wbf_for_id, v_ids)\n",
    "\n",
    "preds = [x for sub_list in list_of_list for x in sub_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(preds)\n",
    "sub.columns = [\"id\", \"class\", \"predictionstring\"]\n",
    "sub.to_csv('submission.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize Test Predictions\n",
    "\n",
    "Below we visualize the test predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from spacy import displacy\n",
    "\n",
    "test_path = Path('../input/feedback-prize-2021/test')\n",
    "\n",
    "colors = {\n",
    "            'Lead': '#8000ff',\n",
    "            'Position': '#2b7ff6',\n",
    "            'Evidence': '#2adddd',\n",
    "            'Claim': '#80ffb4',\n",
    "            'Concluding Statement': 'd4dd80',\n",
    "            'Counterclaim': '#ff8042',\n",
    "            'Rebuttal': '#ff0000',\n",
    "            'Other': '#007f00',\n",
    "         }\n",
    "\n",
    "def get_test_text(ids):\n",
    "    with open(test_path/f'{ids}.txt', 'r') as file: data = file.read()\n",
    "    return data\n",
    "\n",
    "def visualize(df):\n",
    "    ids = df[\"id\"].unique()\n",
    "    for i in range(len(ids)):\n",
    "        ents = []\n",
    "        example = ids[i]\n",
    "        curr_df = df[df[\"id\"]==example]\n",
    "        text = \" \".join(get_test_text(example).split())\n",
    "        splitted_text = text.split()\n",
    "        for i, row in curr_df.iterrows():\n",
    "            predictionstring = row['predictionstring']\n",
    "            predictionstring = predictionstring.split()\n",
    "            wstart = int(predictionstring[0])\n",
    "            wend = int(predictionstring[-1])\n",
    "            ents.append({\n",
    "                             'start': len(\" \".join(splitted_text[:wstart])),\n",
    "                             'end': len(\" \".join(splitted_text[:wend+1])),\n",
    "                             'label': row['class']\n",
    "                        })\n",
    "        ents = sorted(ents, key = lambda i: i['start'])\n",
    "\n",
    "        doc2 = {\n",
    "            \"text\": text,\n",
    "            \"ents\": ents,\n",
    "            \"title\": example\n",
    "        }\n",
    "\n",
    "        options = {\"ents\": ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement', 'Counterclaim', 'Rebuttal'], \"colors\": colors}\n",
    "        displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if len(sub[\"id\"].unique())==5:\n",
    "    visualize(sub)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}