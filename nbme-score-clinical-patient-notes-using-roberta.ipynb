{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import Library.\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport os, re, ast, glob, itertools, spacy, transformers, torch\nfrom transformers import AutoTokenizer, AutoConfig, AutoModel\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nDATA_PATH = \"../input/nbme-score-clinical-patient-notes/\"\nOUT_PATH = \"../input/nbme-roberta-large/\"\nWEIGHTS_FOLDER = \"../input/nbme-roberta-large/\"\n\n\n\"\"\" ------------------------------ \"\"\"\n\"\"\" Data Preparation. \"\"\"\n\"\"\" ------------------------------ \"\"\"\n\ndef process_feature_text(text):\n    text = re.sub('I-year', '1-year', text)\n    text = re.sub('-OR-', \" or \", text)\n    text = re.sub('-', ' ', text)\n    return text\n\ndef clean_spaces(text):\n    text = re.sub('\\n', ' ', text)\n    text = re.sub('\\t', ' ', text)\n    text = re.sub('\\r', ' ', text)\n    return text\n\ndef load_and_prepare_test(root=\"\"):\n    patient_notes = pd.read_csv(root + \"patient_notes.csv\")\n    features = pd.read_csv(root + \"features.csv\")\n    df = pd.read_csv(root + \"test.csv\")\n\n    df = df.merge(features, how=\"left\", on=[\"case_num\", \"feature_num\"])\n    df = df.merge(patient_notes, how=\"left\", on=[\"case_num\", \"pn_num\"])\n\n    df[\"pn_history\"] = df[\"pn_history\"].apply(lambda x: x.strip())\n    df[\"feature_text\"] = df[\"feature_text\"].apply(process_feature_text)\n    df[\"feature_text\"] = df[\"feature_text\"].apply(clean_spaces)\n    df[\"clean_text\"] = df[\"pn_history\"].apply(clean_spaces)\n    df[\"target\"] = \"\"\n    return df\n\n\n\"\"\" ------------------------------ \"\"\"\n\"\"\" Data Processing. \"\"\"\n\"\"\" ------------------------------ \"\"\"\n\ndef token_pred_to_char_pred(token_pred, offsets):\n    char_pred = np.zeros((np.max(offsets), token_pred.shape[1]))\n    for i in range(len(token_pred)):\n        s, e = int(offsets[i][0]), int(offsets[i][1])\n        char_pred[s:e] = token_pred[i]\n        if token_pred.shape[1] == 3:\n            s += 1\n            char_pred[s: e, 1], char_pred[s: e, 2] = (np.max(char_pred[s: e, 1:], 1), np.min(char_pred[s: e, 1:], 1),)\n    return char_pred\n\ndef labels_to_sub(labels):\n    all_spans = []\n    for label in labels:\n        indices = np.where(label > 0)[0]\n        indices_grouped = [list(g) for _, g in itertools.groupby(indices, key=lambda n, c=itertools.count(): n - next(c))]\n        spans = [f\"{min(r)} {max(r) + 1}\" for r in indices_grouped]\n        all_spans.append(\";\".join(spans))\n    return all_spans\n\ndef char_target_to_span(char_target):\n    spans = []\n    start, end = 0, 0\n    for i in range(len(char_target)):\n        if char_target[i] == 1 and char_target[i - 1] == 0:\n            if end:\n                spans.append([start, end])\n            start = i\n            end = i + 1\n        elif char_target[i] == 1:\n            end = i + 1\n        else:\n            if end:\n                spans.append([start, end])\n            start, end = 0, 0\n    return spans\n\ndef post_process_spaces(target, text):\n    target = np.copy(target)\n\n    if len(text) > len(target):\n        padding = np.zeros(len(text) - len(target))\n        target = np.concatenate([target, padding])\n    else:\n        target = target[:len(text)]\n\n    if text[0] == \" \":\n        target[0] = 0\n    if text[-1] == \" \":\n        target[-1] = 0\n\n    for i in range(1, len(text) - 1):\n        if text[i] == \" \":\n            if target[i] and not target[i - 1]:\n                target[i] = 0\n\n            if target[i] and not target[i + 1]:\n                target[i] = 0\n\n            if target[i - 1] and target[i + 1]:\n                target[i] = 1\n    return target\n\n\n\"\"\" ------------------------------ \"\"\"\n\"\"\" Data Tokenization. \"\"\"\n\"\"\" ------------------------------ \"\"\"\n\ndef get_tokenizer(name, precompute=False, df=None, folder=None):\n    if folder is None:\n        tokenizer = AutoTokenizer.from_pretrained(name)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(folder)\n\n    tokenizer.name = name\n\n    tokenizer.special_tokens = {\n        \"sep\": tokenizer.sep_token_id,\n        \"cls\": tokenizer.cls_token_id,\n        \"pad\": tokenizer.pad_token_id,\n    }\n\n    if precompute:\n        tokenizer.precomputed = precompute_tokens(df, tokenizer)\n    else:\n        tokenizer.precomputed=None\n        \n    return tokenizer\n\ndef precompute_tokens(df, tokenizer):\n    feature_texts = df[\"feature_text\"].unique()\n    ids = {}\n    offsets = {}\n\n    for feature_text in feature_texts:\n        encoding = tokenizer(\n            feature_text,\n            return_token_type_ids=True,\n            return_offsets_mapping=True,\n            return_attention_mask=False,\n            add_special_tokens=False,\n        )\n        ids[feature_text] = encoding[\"input_ids\"]\n        offsets[feature_text] = encoding[\"offset_mapping\"]\n\n    texts = df[\"clean_text\"].unique()\n\n    for text in texts:\n        encoding = tokenizer(\n            text,\n            return_token_type_ids=True,\n            return_offsets_mapping=True,\n            return_attention_mask=False,\n            add_special_tokens=False,\n        )\n\n        ids[text] = encoding[\"input_ids\"]\n        offsets[text] = encoding[\"offset_mapping\"]\n        \n    return {\"ids\": ids, \"offsets\": offsets}\n\ndef encodings_from_precomputed(feature_text, text, precomputed, tokenizer, max_len=300):\n    tokens = tokenizer.special_tokens\n\n    if \"roberta\" in tokenizer.name:\n        qa_sep = [tokens[\"sep\"], tokens[\"sep\"]]\n    else:\n        qa_sep = [tokens[\"sep\"]]\n\n    input_ids = [tokens[\"cls\"]] + precomputed[\"ids\"][feature_text] + qa_sep\n    n_question_tokens = len(input_ids)\n\n    input_ids += precomputed[\"ids\"][text]\n    input_ids = input_ids[: max_len - 1] + [tokens[\"sep\"]]\n\n    if \"roberta\" not in tokenizer.name:\n        token_type_ids = np.ones(len(input_ids))\n        token_type_ids[:n_question_tokens] = 0\n        token_type_ids = token_type_ids.tolist()\n    else:\n        token_type_ids = [0] * len(input_ids)\n\n    offsets = [(0, 0)] * n_question_tokens + precomputed[\"offsets\"][text]\n    offsets = offsets[: max_len - 1] + [(0, 0)]\n\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([tokens[\"pad\"]] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        offsets = offsets + ([(0, 0)] * padding_length)\n\n    encoding = {\n        \"input_ids\": input_ids,\n        \"token_type_ids\": token_type_ids,\n        \"offset_mapping\": offsets,\n    }\n\n    return encoding\n\n\n\"\"\" ------------------------------ \"\"\"\n\"\"\" Torch Dataset. \"\"\"\n\"\"\" ------------------------------ \"\"\"\n\nclass PatientNoteDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.texts = df['clean_text'].values\n        self.feature_text = df['feature_text'].values\n        self.char_targets = df['target'].values.tolist()\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        feature_text = self.feature_text[idx]\n        char_target = self.char_targets[idx]\n\n        if self.tokenizer.precomputed is None:\n            encoding = self.tokenizer(\n                feature_text, text,\n                return_token_type_ids=True,\n                return_offsets_mapping=True,\n                return_attention_mask=False,\n                truncation=\"only_second\",\n                max_length=self.max_len,\n                padding=\"max_length\",\n            )\n            raise NotImplementedError(\"Fix issues with question offsets.\")\n        else:\n            encoding = encodings_from_precomputed(feature_text, text, self.tokenizer.precomputed, self.tokenizer, max_len=self.max_len)\n\n        return {\n            \"ids\": torch.tensor(encoding[\"input_ids\"], dtype=torch.long),\n            \"token_type_ids\": torch.tensor(encoding[\"token_type_ids\"], dtype=torch.long),\n            \"target\": torch.tensor([0], dtype=torch.float),\n            \"offsets\": np.array(encoding[\"offset_mapping\"]),\n            \"text\": text,\n        }\n\n    def __len__(self):\n        return len(self.texts)\n\n\n\"\"\" ------------------------------ \"\"\"\n\"\"\" Plot Predictions. \"\"\"\n\"\"\" ------------------------------ \"\"\"\n\ndef plot_annotation(df, pn_num):\n    options = {\"colors\": {}}\n\n    df_text = df[df[\"pn_num\"] == pn_num].reset_index(drop=True)\n\n    text = df_text[\"pn_history\"][0]\n    ents = []\n\n    for spans, feature_text, feature_num in df_text[[\"span\", \"feature_text\", \"feature_num\"]].values:\n        for s in spans:\n            ents.append({\"start\": int(s[0]), \"end\": int(s[1]), \"label\": feature_text})\n\n        options[\"colors\"][feature_text] =  f\"rgb{tuple(np.random.randint(100, 255, size=3))}\"\n\n    doc = {\"text\": text, \"ents\": sorted(ents, key=lambda i: i[\"start\"])}\n    \n    # spacy.displacy.render(doc, style=\"ent\", options=options, manual=True, jupyter=True)\n\n\n\"\"\" ------------------------------ \"\"\"\n\"\"\" Model Development. \"\"\"\n\"\"\" ------------------------------ \"\"\"\n\nclass NERTransformer(nn.Module):\n    def __init__(self, model, num_classes=1, config_file=None, pretrained=True):\n        super().__init__()\n        self.name = model\n        self.pad_idx = 1 if \"roberta\" in self.name else 0\n\n        transformers.logging.set_verbosity_error()\n\n        if config_file is None:\n            config = AutoConfig.from_pretrained(model, output_hidden_states=True)\n        else:\n            config = torch.load(config_file)\n\n        if pretrained:\n            self.transformer = AutoModel.from_pretrained(model, config=config)\n        else:\n            self.transformer = AutoModel.from_config(config)\n\n        self.nb_features = config.hidden_size\n\n        self.logits = nn.Linear(self.nb_features, num_classes)\n\n    def forward(self, tokens, token_type_ids):\n        hidden_states = self.transformer(tokens, attention_mask=(tokens != self.pad_idx).long(), token_type_ids=token_type_ids)[-1]\n\n        features = hidden_states[-1]\n\n        logits = self.logits(features)\n\n        return logits\n\n\n\"\"\" ------------------------------ \"\"\"\n\"\"\" Load Weights. \"\"\"\n\"\"\" ------------------------------ \"\"\"\n\ndef load_model_weights(model, filename, verbose=1, cp_folder=\"\", strict=True):\n    if verbose:\n        print(f\"\\n -> Loading weights from {os.path.join(cp_folder,filename)}\\n\")\n    try:\n        model.load_state_dict(torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"), strict=strict)\n    except RuntimeError:\n        model.encoder.fc = torch.nn.Linear(model.nb_ft, 1)\n        model.load_state_dict(torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"), strict=strict)\n    return model\n\n\n\"\"\" ------------------------------ \"\"\"\n\"\"\" Predict Function. \"\"\"\n\"\"\" ------------------------------ \"\"\"\n\ndef predict(model, dataset, data_config, activation=\"softmax\"):\n    model.eval()\n    loader = DataLoader(dataset, batch_size=data_config[\"val_bs\"], shuffle=False, num_workers=2, pin_memory=True)\n    preds = []\n    with torch.no_grad():\n        for data in tqdm(loader):\n            ids, token_type_ids = data[\"ids\"], data[\"token_type_ids\"]\n            y_pred = model(ids.cuda(), token_type_ids.cuda())\n            if activation == \"sigmoid\":\n                y_pred = y_pred.sigmoid()\n            elif activation == \"softmax\":\n                y_pred = y_pred.softmax(-1)\n            preds += [token_pred_to_char_pred(y, offsets) for y, offsets in zip(y_pred.detach().cpu().numpy(), data[\"offsets\"].numpy())]\n    return preds\n\n\n\"\"\" ------------------------------ \"\"\"\n\"\"\" Inference Test. \"\"\"\n\"\"\" ------------------------------ \"\"\"\n\ndef inference_test(df, exp_folder, config, cfg_folder=None):\n    preds = []\n\n    if cfg_folder is not None:\n        model_config_file = cfg_folder + config.name.split('/')[-1] + \"/config.pth\"\n        tokenizer_folder = cfg_folder + config.name.split('/')[-1] + \"/tokenizers/\"\n    else:\n        model_config_file, tokenizer_folder = None, None\n\n    tokenizer = get_tokenizer(config.name, precompute=config.precompute_tokens, df=df, folder=tokenizer_folder)\n    dataset = PatientNoteDataset(df, tokenizer, max_len=config.max_len)\n    model = NERTransformer(config.name, num_classes=config.num_classes, config_file=model_config_file, pretrained=False).cuda()\n    model.zero_grad()\n\n    weights = sorted(glob.glob(exp_folder + \"*.pt\"))\n\n    for weight in weights:\n        model = load_model_weights(model, weight)\n        pred = predict(model, dataset, data_config=config.data_config, activation=config.loss_config[\"activation\"])\n        preds.append(pred)\n    return preds\n\n\n\"\"\" ------------------------------ \"\"\"\n\"\"\" Main Code. \"\"\"\n\"\"\" ------------------------------ \"\"\"\n\nif __name__ == \"__main__\":\n    class Config:\n        # Architecture.\n        name = \"roberta-large\"\n        num_classes = 1\n        # Texts.\n        max_len = 310\n        precompute_tokens = True\n        # Training.\n        loss_config = {\"activation\": \"sigmoid\"}\n        data_config = {\"val_bs\": 16 if \"large\" in name else 32, \"pad_token\": 1 if \"roberta\" in name else 0}\n        verbose = 1\n\n    df_test = load_and_prepare_test(root=DATA_PATH)\n\n    preds = inference_test(df_test, WEIGHTS_FOLDER, Config, cfg_folder=OUT_PATH)[0]\n\n    df_test[\"preds\"] = preds\n    df_test[\"preds\"] = df_test.apply(lambda x: x[\"preds\"][:len(x[\"clean_text\"])], 1)\n    df_test[\"preds\"] = df_test[\"preds\"].apply(lambda x: (x > 0.5).flatten())\n\n    try:\n        df_test[\"span\"] = df_test[\"preds\"].apply(char_target_to_span)\n        plot_annotation(df_test, df_test[\"pn_num\"][0])\n    except:\n        pass\n\n    df_test[\"preds_pp\"] = df_test.apply(lambda x: post_process_spaces(x[\"preds\"], x[\"clean_text\"]), 1)\n\n    try:\n        df_test[\"span\"] = df_test[\"preds_pp\"].apply(char_target_to_span)\n        plot_annotation(df_test, df_test[\"pn_num\"][0])\n    except:\n        pass\n\n    # Kaggle Submission.\n    df_test['location'] = labels_to_sub(df_test[\"preds_pp\"].values)\n\n    sub = pd.read_csv(DATA_PATH + \"sample_submission.csv\")\n\n    sub = sub[[\"id\"]].merge(df_test[[\"id\", \"location\"]], how=\"left\", on=\"id\")\n\n    sub.to_csv(\"submission.csv\", index=False)\n\n    sub.head()","metadata":{"_uuid":"b01b4579-1159-477d-abf6-fd1971d34c7d","_cell_guid":"d0692a72-f1cb-426a-8162-c2f3c1320f5e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}