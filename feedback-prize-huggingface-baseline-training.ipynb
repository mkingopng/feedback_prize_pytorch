{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This one works now, so best not change it. Keep it as a reference point"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HuggingFace Training Baseline\n",
    "\n",
    "I wanted to create my own baseline for this competition, and I tried to do so \"without peeking\" at the kernels published by others. Ideally this can be used for training on a Kaggle kernel. Let's see how good we can get.\n",
    "\n",
    "This baseline is based on the following notebook by Sylvain Gugger: https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb\n",
    "\n",
    "I initially started building with Roberta - thanks to Chris Deotte for pointing to Longformer :) The evaluation code is from Rob Mulla.\n",
    "\n",
    "The notebook requires a couple of hours to run, so we'll use W&B to be able to monitor it along the way and keep the record of our experiments."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "SAMPLE = False # set True for debugging"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T22:59:40.43361Z",
     "iopub.execute_input": "2021-12-23T22:59:40.434Z",
     "iopub.status.idle": "2021-12-23T22:59:40.438896Z",
     "shell.execute_reply.started": "2021-12-23T22:59:40.433966Z",
     "shell.execute_reply": "2021-12-23T22:59:40.437857Z"
    },
    "trusted": true
   },
   "execution_count": 91,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:3fnivcsr) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b2b1ee373e44dbf8e0b4ac5de961b59"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">sandy-wildflower-66</strong>: <a href=\"https://wandb.ai/feedback_prize_michael_and_wilson/feedback_prize_pytorch/runs/3fnivcsr\" target=\"_blank\">https://wandb.ai/feedback_prize_michael_and_wilson/feedback_prize_pytorch/runs/3fnivcsr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20220304_162444-3fnivcsr/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:3fnivcsr). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.11"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/noone/Documents/GitHub/feedback_prize_pytorch/wandb/run-20220304_163515-eodi9dfg</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/feedback_prize_michael_and_wilson/feedback_prize_pytorch/runs/eodi9dfg\" target=\"_blank\">comfy-frog-67</a></strong> to <a href=\"https://wandb.ai/feedback_prize_michael_and_wilson/feedback_prize_pytorch\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/feedback_prize_michael_and_wilson/feedback_prize_pytorch/runs/eodi9dfg?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x7f1c577a4310>"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb_creds import *\n",
    "\n",
    "wandb.login(key=API_KEY)\n",
    "wandb.init(project=\"feedback_prize_pytorch\", tags=TAGS, entity=\"feedback_prize_michael_and_wilson\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# CONFIG  # mk: i put these in hf_config.py\n",
    "\n",
    "EXP_NUM = 1\n",
    "task = \"ner\"\n",
    "model_checkpoint = \"longformer-large-4096-hf\"\n",
    "max_length = 1024\n",
    "stride = 128\n",
    "min_tokens = 6\n",
    "model_path = f'{model_checkpoint.split(\"/\")[-1]}-{EXP_NUM}'\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "# TRAINING HYPERPARAMS\n",
    "BS = 1\n",
    "GRAD_ACC = 8\n",
    "LR = 5e-5\n",
    "WD = 0.01\n",
    "WARMUP = 0.1\n",
    "N_EPOCHS = 5"
   ],
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:08.872471Z",
     "iopub.execute_input": "2021-12-23T23:00:08.875384Z",
     "iopub.status.idle": "2021-12-23T23:00:09.613866Z",
     "shell.execute_reply.started": "2021-12-23T23:00:08.875328Z",
     "shell.execute_reply": "2021-12-23T23:00:09.612856Z"
    },
    "trusted": true
   },
   "execution_count": 95,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read train data\n",
    "train = pd.read_csv(os.path.join(DATA_DIR,'train.csv'))  # mk: i put this in Config of config.py\n",
    "train.head(1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:09.615125Z",
     "iopub.execute_input": "2021-12-23T23:00:09.615508Z",
     "iopub.status.idle": "2021-12-23T23:00:11.240349Z",
     "shell.execute_reply.started": "2021-12-23T23:00:09.615458Z",
     "shell.execute_reply": "2021-12-23T23:00:11.239275Z"
    },
    "trusted": true
   },
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "             id  discourse_id  discourse_start  discourse_end  \\\n0  423A1CA112E2  1.622628e+12              8.0          229.0   \n\n                                      discourse_text discourse_type  \\\n0  Modern humans today are always on their phone....           Lead   \n\n  discourse_type_num                                   predictionstring  \n0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>discourse_id</th>\n      <th>discourse_start</th>\n      <th>discourse_end</th>\n      <th>discourse_text</th>\n      <th>discourse_type</th>\n      <th>discourse_type_num</th>\n      <th>predictionstring</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>8.0</td>\n      <td>229.0</td>\n      <td>Modern humans today are always on their phone....</td>\n      <td>Lead</td>\n      <td>Lead 1</td>\n      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# check unique classes\n",
    "classes = train.discourse_type.unique().tolist()  # mk: i put this in Config of hf_config.py\n",
    "classes"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:11.245598Z",
     "iopub.execute_input": "2021-12-23T23:00:11.248663Z",
     "iopub.status.idle": "2021-12-23T23:00:12.088646Z",
     "shell.execute_reply.started": "2021-12-23T23:00:11.248611Z",
     "shell.execute_reply": "2021-12-23T23:00:12.087709Z"
    },
    "trusted": true
   },
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "['Lead',\n 'Position',\n 'Evidence',\n 'Claim',\n 'Concluding Statement',\n 'Counterclaim',\n 'Rebuttal']"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# setup label indices\n",
    "from collections import defaultdict\n",
    "\n",
    "tags = defaultdict()  # mk: i put this in Config of hf_config.py\n",
    "\n",
    "for i, c in enumerate(classes):  # mk: i put this in hf_functions.py as label_to_index\n",
    "    tags[f'B-{c}'] = i\n",
    "    tags[f'I-{c}'] = i + len(classes)\n",
    "tags[f'O'] = len(classes) * 2\n",
    "tags[f'Special'] = -100\n",
    "    \n",
    "l2i = dict(tags)\n",
    "\n",
    "i2l = defaultdict()  # mk: i put this in hf_functions as index_to label\n",
    "for k, v in l2i.items(): \n",
    "    i2l[v] = k\n",
    "i2l[-100] = 'Special'\n",
    "\n",
    "i2l = dict(i2l)\n",
    "\n",
    "N_LABELS = len(i2l) - 1  # not accounting for -100  # mk: i put this in hf_functions.py as create_n_labels"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:12.090074Z",
     "iopub.execute_input": "2021-12-23T23:00:12.090401Z",
     "iopub.status.idle": "2021-12-23T23:00:12.909927Z",
     "shell.execute_reply.started": "2021-12-23T23:00:12.090357Z",
     "shell.execute_reply": "2021-12-23T23:00:12.908979Z"
    },
    "trusted": true
   },
   "execution_count": 98,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# some helper functions\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path('data/train')  # i put this in Config of hf_config.py \n",
    "\n",
    "def get_raw_text(ids):\n",
    "    with open(path/f'{ids}.txt', 'r') as file: data = file.read()\n",
    "    return data"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:12.913651Z",
     "iopub.execute_input": "2021-12-23T23:00:12.913893Z",
     "iopub.status.idle": "2021-12-23T23:00:13.630498Z",
     "shell.execute_reply.started": "2021-12-23T23:00:12.913861Z",
     "shell.execute_reply": "2021-12-23T23:00:13.629554Z"
    },
    "trusted": true
   },
   "execution_count": 99,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# group training labels by text file\n",
    "\n",
    "df1 = train.groupby('id')['discourse_type'].apply(list).reset_index(name='classlist')  # mk: moved this to hf_functions.py\n",
    "df2 = train.groupby('id')['discourse_start'].apply(list).reset_index(name='starts')\n",
    "df3 = train.groupby('id')['discourse_end'].apply(list).reset_index(name='ends')\n",
    "df4 = train.groupby('id')['predictionstring'].apply(list).reset_index(name='predictionstrings')\n",
    "\n",
    "df = pd.merge(df1, df2, how='inner', on='id')\n",
    "df = pd.merge(df, df3, how='inner', on='id')\n",
    "df = pd.merge(df, df4, how='inner', on='id')\n",
    "df['text'] = df['id'].apply(get_raw_text)\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:13.634902Z",
     "iopub.execute_input": "2021-12-23T23:00:13.635138Z",
     "iopub.status.idle": "2021-12-23T23:00:24.829274Z",
     "shell.execute_reply.started": "2021-12-23T23:00:13.635107Z",
     "shell.execute_reply": "2021-12-23T23:00:24.828189Z"
    },
    "trusted": true
   },
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "             id                                          classlist  \\\n0  0000D23A521A  [Position, Evidence, Evidence, Claim, Counterc...   \n1  00066EA9880D  [Lead, Position, Claim, Evidence, Claim, Evide...   \n2  000E6DE9E817  [Position, Counterclaim, Rebuttal, Evidence, C...   \n3  001552828BD0  [Lead, Evidence, Claim, Claim, Evidence, Claim...   \n4  0016926B079C  [Position, Claim, Claim, Claim, Claim, Evidenc...   \n\n                                              starts  \\\n0  [0.0, 170.0, 358.0, 438.0, 627.0, 722.0, 836.0...   \n1  [0.0, 456.0, 638.0, 738.0, 1399.0, 1488.0, 231...   \n2  [17.0, 64.0, 158.0, 310.0, 438.0, 551.0, 776.0...   \n3  [0.0, 161.0, 872.0, 958.0, 1191.0, 1542.0, 161...   \n4  [0.0, 58.0, 94.0, 206.0, 236.0, 272.0, 542.0, ...   \n\n                                                ends  \\\n0  [170.0, 357.0, 438.0, 626.0, 722.0, 836.0, 101...   \n1  [455.0, 592.0, 738.0, 1398.0, 1487.0, 2219.0, ...   \n2  [56.0, 157.0, 309.0, 422.0, 551.0, 775.0, 961....   \n3  [160.0, 872.0, 957.0, 1190.0, 1541.0, 1612.0, ...   \n4  [57.0, 91.0, 150.0, 235.0, 271.0, 542.0, 650.0...   \n\n                                   predictionstrings  \\\n0  [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1...   \n1  [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1...   \n2  [2 3 4 5 6 7 8, 10 11 12 13 14 15 16 17 18 19 ...   \n3  [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1...   \n4  [0 1 2 3 4 5 6 7 8 9, 10 11 12 13 14 15, 16 17...   \n\n                                                text  \n0  Some people belive that the so called \"face\" o...  \n1  Driverless cars are exaclty what you would exp...  \n2  Dear: Principal\\n\\nI am arguing against the po...  \n3  Would you be able to give your car up? Having ...  \n4  I think that students would benefit from learn...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>classlist</th>\n      <th>starts</th>\n      <th>ends</th>\n      <th>predictionstrings</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000D23A521A</td>\n      <td>[Position, Evidence, Evidence, Claim, Counterc...</td>\n      <td>[0.0, 170.0, 358.0, 438.0, 627.0, 722.0, 836.0...</td>\n      <td>[170.0, 357.0, 438.0, 626.0, 722.0, 836.0, 101...</td>\n      <td>[0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1...</td>\n      <td>Some people belive that the so called \"face\" o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00066EA9880D</td>\n      <td>[Lead, Position, Claim, Evidence, Claim, Evide...</td>\n      <td>[0.0, 456.0, 638.0, 738.0, 1399.0, 1488.0, 231...</td>\n      <td>[455.0, 592.0, 738.0, 1398.0, 1487.0, 2219.0, ...</td>\n      <td>[0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1...</td>\n      <td>Driverless cars are exaclty what you would exp...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000E6DE9E817</td>\n      <td>[Position, Counterclaim, Rebuttal, Evidence, C...</td>\n      <td>[17.0, 64.0, 158.0, 310.0, 438.0, 551.0, 776.0...</td>\n      <td>[56.0, 157.0, 309.0, 422.0, 551.0, 775.0, 961....</td>\n      <td>[2 3 4 5 6 7 8, 10 11 12 13 14 15 16 17 18 19 ...</td>\n      <td>Dear: Principal\\n\\nI am arguing against the po...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>001552828BD0</td>\n      <td>[Lead, Evidence, Claim, Claim, Evidence, Claim...</td>\n      <td>[0.0, 161.0, 872.0, 958.0, 1191.0, 1542.0, 161...</td>\n      <td>[160.0, 872.0, 957.0, 1190.0, 1541.0, 1612.0, ...</td>\n      <td>[0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1...</td>\n      <td>Would you be able to give your car up? Having ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016926B079C</td>\n      <td>[Position, Claim, Claim, Claim, Claim, Evidenc...</td>\n      <td>[0.0, 58.0, 94.0, 206.0, 236.0, 272.0, 542.0, ...</td>\n      <td>[57.0, 91.0, 150.0, 235.0, 271.0, 542.0, 650.0...</td>\n      <td>[0 1 2 3 4 5 6 7 8 9, 10 11 12 13 14 15, 16 17...</td>\n      <td>I think that students would benefit from learn...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# debugging\n",
    "if SAMPLE: df = df.sample(n=100).reset_index(drop=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:24.831063Z",
     "iopub.execute_input": "2021-12-23T23:00:24.831421Z",
     "iopub.status.idle": "2021-12-23T23:00:25.596595Z",
     "shell.execute_reply.started": "2021-12-23T23:00:24.831375Z",
     "shell.execute_reply": "2021-12-23T23:00:25.595633Z"
    },
    "trusted": true
   },
   "execution_count": 101,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# we will use HuggingFace datasets\n",
    "from datasets import Dataset, load_metric\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "datasets = ds.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "datasets"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:25.59961Z",
     "iopub.execute_input": "2021-12-23T23:00:25.600322Z",
     "iopub.status.idle": "2021-12-23T23:00:26.415085Z",
     "shell.execute_reply.started": "2021-12-23T23:00:25.600259Z",
     "shell.execute_reply": "2021-12-23T23:00:26.413987Z"
    },
    "trusted": true
   },
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'classlist', 'starts', 'ends', 'predictionstrings', 'text', '__index_level_0__'],\n        num_rows: 14034\n    })\n    test: Dataset({\n        features: ['id', 'classlist', 'starts', 'ends', 'predictionstrings', 'text', '__index_level_0__'],\n        num_rows: 1560\n    })\n})"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:26.416852Z",
     "iopub.execute_input": "2021-12-23T23:00:26.417192Z",
     "iopub.status.idle": "2021-12-23T23:00:31.722501Z",
     "shell.execute_reply.started": "2021-12-23T23:00:26.417127Z",
     "shell.execute_reply": "2021-12-23T23:00:31.721572Z"
    },
    "trusted": true
   },
   "execution_count": 103,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file longformer-large-4096-hf/config.json\n",
      "Model config LongformerConfig {\n",
      "  \"_name_or_path\": \"longformer-large-4096-hf\",\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Didn't find file longformer-large-4096-hf/added_tokens.json. We won't load it.\n",
      "Didn't find file longformer-large-4096-hf/special_tokens_map.json. We won't load it.\n",
      "Didn't find file longformer-large-4096-hf/tokenizer_config.json. We won't load it.\n",
      "loading file longformer-large-4096-hf/vocab.json\n",
      "loading file longformer-large-4096-hf/merges.txt\n",
      "loading file longformer-large-4096-hf/tokenizer.json\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file longformer-large-4096-hf/config.json\n",
      "Model config LongformerConfig {\n",
      "  \"_name_or_path\": \"longformer-large-4096-hf\",\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Not sure if this is needed, but in case we create a span with certain class without starting token of that class,\n",
    "# let's convert the first token to be the starting token.\n",
    "\n",
    "e = [0,7,7,7,1,1,8,8,8,9,9,9,14,4,4,4]\n",
    "\n",
    "def fix_beginnings(labels):\n",
    "    for i in range(1,len(labels)):\n",
    "        curr_lab = labels[i]\n",
    "        prev_lab = labels[i-1]\n",
    "        if curr_lab in range(7,14):\n",
    "            if prev_lab != curr_lab and prev_lab != curr_lab - 7:\n",
    "                labels[i] = curr_lab -7\n",
    "    return labels\n",
    "\n",
    "fix_beginnings(e)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:31.724112Z",
     "iopub.execute_input": "2021-12-23T23:00:31.724482Z",
     "iopub.status.idle": "2021-12-23T23:00:32.494243Z",
     "shell.execute_reply.started": "2021-12-23T23:00:31.724438Z",
     "shell.execute_reply": "2021-12-23T23:00:32.49297Z"
    },
    "trusted": true
   },
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 7, 7, 7, 1, 1, 8, 8, 8, 2, 9, 9, 14, 4, 4, 4]"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# tokenize and add labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "\n",
    "    o = tokenizer(examples['text'], truncation=True, padding=True, return_offsets_mapping=True, max_length=max_length, stride=stride, return_overflowing_tokens=True)\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = o[\"overflow_to_sample_mapping\"]\n",
    "\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = o[\"offset_mapping\"]\n",
    "    \n",
    "    o[\"labels\"] = []\n",
    "\n",
    "    for i in range(len(offset_mapping)):\n",
    "                   \n",
    "        sample_index = sample_mapping[i]\n",
    "\n",
    "        labels = [l2i['O'] for i in range(len(o['input_ids'][i]))]\n",
    "\n",
    "        for label_start, label_end, label in \\\n",
    "        list(zip(examples['starts'][sample_index], examples['ends'][sample_index], examples['classlist'][sample_index])):\n",
    "            for j in range(len(labels)):\n",
    "                token_start = offset_mapping[i][j][0]\n",
    "                token_end = offset_mapping[i][j][1]\n",
    "                if token_start == label_start: \n",
    "                    labels[j] = l2i[f'B-{label}']    \n",
    "                if token_start > label_start and token_end <= label_end: \n",
    "                    labels[j] = l2i[f'I-{label}']\n",
    "\n",
    "        for k, input_id in enumerate(o['input_ids'][i]):\n",
    "            if input_id in [0,1,2]:\n",
    "                labels[k] = -100\n",
    "\n",
    "        labels = fix_beginnings(labels)\n",
    "                   \n",
    "        o[\"labels\"].append(labels)\n",
    "        \n",
    "    return o"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:32.495836Z",
     "iopub.execute_input": "2021-12-23T23:00:32.496208Z",
     "iopub.status.idle": "2021-12-23T23:00:33.263669Z",
     "shell.execute_reply.started": "2021-12-23T23:00:32.49614Z",
     "shell.execute_reply": "2021-12-23T23:00:33.262629Z"
    },
    "trusted": true
   },
   "execution_count": 105,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True, batch_size=20000, remove_columns=datasets[\"train\"].column_names)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:33.265142Z",
     "iopub.execute_input": "2021-12-23T23:00:33.265646Z",
     "iopub.status.idle": "2021-12-23T23:00:35.856612Z",
     "shell.execute_reply.started": "2021-12-23T23:00:33.265601Z",
     "shell.execute_reply": "2021-12-23T23:00:35.855589Z"
    },
    "trusted": true
   },
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52329e49a6e74589919ef19fed8662a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "02af086d03094f07b980cb0449f7df5b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tokenized_datasets"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:35.858326Z",
     "iopub.execute_input": "2021-12-23T23:00:35.858635Z",
     "iopub.status.idle": "2021-12-23T23:00:36.592654Z",
     "shell.execute_reply.started": "2021-12-23T23:00:35.85859Z",
     "shell.execute_reply": "2021-12-23T23:00:36.591606Z"
    },
    "trusted": true
   },
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'labels'],\n        num_rows: 14574\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'labels'],\n        num_rows: 1625\n    })\n})"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model and Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# we will use auto model for token classification\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=N_LABELS)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:36.59433Z",
     "iopub.execute_input": "2021-12-23T23:00:36.594634Z",
     "iopub.status.idle": "2021-12-23T23:00:40.685632Z",
     "shell.execute_reply.started": "2021-12-23T23:00:36.594593Z",
     "shell.execute_reply": "2021-12-23T23:00:40.684693Z"
    },
    "trusted": true
   },
   "execution_count": 108,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file longformer-large-4096-hf/config.json\n",
      "Model config LongformerConfig {\n",
      "  \"_name_or_path\": \"longformer-large-4096-hf\",\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\"\n",
      "  },\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file longformer-large-4096-hf/pytorch_model.bin\n",
      "Some weights of the model checkpoint at longformer-large-4096-hf were not used when initializing LongformerForTokenClassification: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing LongformerForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForTokenClassification were not initialized from the model checkpoint at longformer-large-4096-hf and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    logging_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BS,\n",
    "    per_device_eval_batch_size=BS,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    weight_decay=WD,\n",
    "    report_to='wandb', \n",
    "    gradient_accumulation_steps=GRAD_ACC,\n",
    "    warmup_ratio=WARMUP\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:40.690854Z",
     "iopub.execute_input": "2021-12-23T23:00:40.693718Z",
     "iopub.status.idle": "2021-12-23T23:00:41.535273Z",
     "shell.execute_reply.started": "2021-12-23T23:00:40.693672Z",
     "shell.execute_reply": "2021-12-23T23:00:41.534215Z"
    },
    "trusted": true
   },
   "execution_count": 109,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:41.53676Z",
     "iopub.execute_input": "2021-12-23T23:00:41.537608Z",
     "iopub.status.idle": "2021-12-23T23:00:42.282789Z",
     "shell.execute_reply.started": "2021-12-23T23:00:41.537572Z",
     "shell.execute_reply": "2021-12-23T23:00:42.281853Z"
    },
    "trusted": true
   },
   "execution_count": 110,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# this is not the competition metric, but for now this will be better than nothing...\n",
    "\n",
    "metric = load_metric(\"seqeval\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:42.284192Z",
     "iopub.execute_input": "2021-12-23T23:00:42.284501Z",
     "iopub.status.idle": "2021-12-23T23:00:43.656933Z",
     "shell.execute_reply.started": "2021-12-23T23:00:42.284458Z",
     "shell.execute_reply": "2021-12-23T23:00:43.655937Z"
    },
    "trusted": true
   },
   "execution_count": 111,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [i2l[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [i2l[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:43.658571Z",
     "iopub.execute_input": "2021-12-23T23:00:43.658881Z",
     "iopub.status.idle": "2021-12-23T23:00:44.386693Z",
     "shell.execute_reply.started": "2021-12-23T23:00:43.658824Z",
     "shell.execute_reply": "2021-12-23T23:00:44.385607Z"
    },
    "trusted": true
   },
   "execution_count": 112,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, \n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:44.388421Z",
     "iopub.execute_input": "2021-12-23T23:00:44.388744Z",
     "iopub.status.idle": "2021-12-23T23:00:45.313179Z",
     "shell.execute_reply.started": "2021-12-23T23:00:44.38869Z",
     "shell.execute_reply": "2021-12-23T23:00:45.312215Z"
    },
    "trusted": true
   },
   "execution_count": 113,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")  # new addition\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3db2817eec6b4a66930f3c8e95cc1541"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(args.num_train_epochs))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()  #\n",
    "# for epoch in range(args.num_train_epochs):\n",
    "#     trainer.train()\n",
    "#     for batch in train_dataloader:\n",
    "#           batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#           outputss = model.(**batch)\n",
    "#           loss = outputs.loss\n",
    "#           loss.backward()\n",
    "#           optimizer.step()\n",
    "#           lr_scheduler.step()\n",
    "#           optimizer.zero_grad()\n",
    "#           progress_bar.update(1)\n",
    "# wandb.log()  # new additions\n",
    "# wandb.watch(model)  # new additions\n",
    "wandb.finish()  #"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:45.314663Z",
     "iopub.execute_input": "2021-12-23T23:00:45.318411Z",
     "iopub.status.idle": "2021-12-23T23:03:13.651205Z",
     "shell.execute_reply.started": "2021-12-23T23:00:45.318345Z",
     "shell.execute_reply": "2021-12-23T23:03:13.650259Z"
    },
    "trusted": true
   },
   "execution_count": 116,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `LongformerForTokenClassification.forward` and have been ignored: overflow_to_sample_mapping, offset_mapping.\n",
      "/home/noone/anaconda3/envs/pytorch_NLP/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14574\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 9105\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='9105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/9105 : < :, Epoch 0.00/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `LongformerForTokenClassification.forward` and have been ignored: overflow_to_sample_mapping, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1625\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to longformer-large-4096-hf-finetuned-ner/checkpoint-1821\n",
      "Configuration saved in longformer-large-4096-hf-finetuned-ner/checkpoint-1821/config.json\n",
      "Model weights saved in longformer-large-4096-hf-finetuned-ner/checkpoint-1821/pytorch_model.bin\n",
      "tokenizer config file saved in longformer-large-4096-hf-finetuned-ner/checkpoint-1821/tokenizer_config.json\n",
      "Special tokens file saved in longformer-large-4096-hf-finetuned-ner/checkpoint-1821/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `LongformerForTokenClassification.forward` and have been ignored: overflow_to_sample_mapping, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1625\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to longformer-large-4096-hf-finetuned-ner/checkpoint-3642\n",
      "Configuration saved in longformer-large-4096-hf-finetuned-ner/checkpoint-3642/config.json\n",
      "Model weights saved in longformer-large-4096-hf-finetuned-ner/checkpoint-3642/pytorch_model.bin\n",
      "tokenizer config file saved in longformer-large-4096-hf-finetuned-ner/checkpoint-3642/tokenizer_config.json\n",
      "Special tokens file saved in longformer-large-4096-hf-finetuned-ner/checkpoint-3642/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `LongformerForTokenClassification.forward` and have been ignored: overflow_to_sample_mapping, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1625\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to longformer-large-4096-hf-finetuned-ner/checkpoint-5463\n",
      "Configuration saved in longformer-large-4096-hf-finetuned-ner/checkpoint-5463/config.json\n",
      "Model weights saved in longformer-large-4096-hf-finetuned-ner/checkpoint-5463/pytorch_model.bin\n",
      "tokenizer config file saved in longformer-large-4096-hf-finetuned-ner/checkpoint-5463/tokenizer_config.json\n",
      "Special tokens file saved in longformer-large-4096-hf-finetuned-ner/checkpoint-5463/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `LongformerForTokenClassification.forward` and have been ignored: overflow_to_sample_mapping, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1625\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to longformer-large-4096-hf-finetuned-ner/checkpoint-7284\n",
      "Configuration saved in longformer-large-4096-hf-finetuned-ner/checkpoint-7284/config.json\n",
      "Model weights saved in longformer-large-4096-hf-finetuned-ner/checkpoint-7284/pytorch_model.bin\n",
      "tokenizer config file saved in longformer-large-4096-hf-finetuned-ner/checkpoint-7284/tokenizer_config.json\n",
      "Special tokens file saved in longformer-large-4096-hf-finetuned-ner/checkpoint-7284/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `LongformerForTokenClassification.forward` and have been ignored: overflow_to_sample_mapping, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1625\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to longformer-large-4096-hf-finetuned-ner/checkpoint-9105\n",
      "Configuration saved in longformer-large-4096-hf-finetuned-ner/checkpoint-9105/config.json\n",
      "Model weights saved in longformer-large-4096-hf-finetuned-ner/checkpoint-9105/pytorch_model.bin\n",
      "tokenizer config file saved in longformer-large-4096-hf-finetuned-ner/checkpoint-9105/tokenizer_config.json\n",
      "Special tokens file saved in longformer-large-4096-hf-finetuned-ner/checkpoint-9105/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51bb4cd818dc4ce1a655c2dc6cc82ffc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▆▆▆█</td></tr><tr><td>eval/f1</td><td>▃▁▆▆█</td></tr><tr><td>eval/loss</td><td>▃▂▁▃█</td></tr><tr><td>eval/precision</td><td>▃▁▆▆█</td></tr><tr><td>eval/recall</td><td>▁▂█▇█</td></tr><tr><td>eval/runtime</td><td>▁█▁▂▁</td></tr><tr><td>eval/samples_per_second</td><td>█▁▇▇█</td></tr><tr><td>eval/steps_per_second</td><td>█▁▇▇█</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▅▅▆▆███</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▅▅▆▆███</td></tr><tr><td>train/learning_rate</td><td>█▆▅▃▁</td></tr><tr><td>train/loss</td><td>█▅▄▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.81786</td></tr><tr><td>eval/f1</td><td>0.2719</td></tr><tr><td>eval/loss</td><td>0.7263</td></tr><tr><td>eval/precision</td><td>0.21681</td></tr><tr><td>eval/recall</td><td>0.36452</td></tr><tr><td>eval/runtime</td><td>114.3649</td></tr><tr><td>eval/samples_per_second</td><td>14.209</td></tr><tr><td>eval/steps_per_second</td><td>14.209</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>9105</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2595</td></tr><tr><td>train/total_flos</td><td>1.691760218058916e+17</td></tr><tr><td>train/train_loss</td><td>0.50324</td></tr><tr><td>train/train_runtime</td><td>22435.1407</td></tr><tr><td>train/train_samples_per_second</td><td>3.248</td></tr><tr><td>train/train_steps_per_second</td><td>0.406</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">comfy-frog-67</strong>: <a href=\"https://wandb.ai/feedback_prize_michael_and_wilson/feedback_prize_pytorch/runs/eodi9dfg\" target=\"_blank\">https://wandb.ai/feedback_prize_michael_and_wilson/feedback_prize_pytorch/runs/eodi9dfg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20220304_163515-eodi9dfg/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.save_model(model_path)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:13.656546Z",
     "iopub.execute_input": "2021-12-23T23:03:13.656788Z",
     "iopub.status.idle": "2021-12-23T23:03:15.317965Z",
     "shell.execute_reply.started": "2021-12-23T23:03:13.656757Z",
     "shell.execute_reply": "2021-12-23T23:03:15.316868Z"
    },
    "trusted": true
   },
   "execution_count": 117,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to longformer-large-4096-hf-1\n",
      "Configuration saved in longformer-large-4096-hf-1/config.json\n",
      "Model weights saved in longformer-large-4096-hf-1/pytorch_model.bin\n",
      "tokenizer config file saved in longformer-large-4096-hf-1/tokenizer_config.json\n",
      "Special tokens file saved in longformer-large-4096-hf-1/special_tokens_map.json\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def tokenize_for_validation(examples):\n",
    "    o = tokenizer(examples['text'], truncation=True, return_offsets_mapping=True, max_length=4096)\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will help us compute the start_positions and end_positions.\n",
    "    offset_mapping = o[\"offset_mapping\"]\n",
    "    o[\"labels\"] = []\n",
    "    for i in range(len(offset_mapping)):\n",
    "        labels = [l2i['O'] for i in range(len(o['input_ids'][i]))]\n",
    "        for label_start, label_end, label in \\\n",
    "        list(zip(examples['starts'][i], examples['ends'][i], examples['classlist'][i])):\n",
    "            for j in range(len(labels)):\n",
    "                token_start = offset_mapping[i][j][0]\n",
    "                token_end = offset_mapping[i][j][1]\n",
    "                if token_start == label_start: \n",
    "                    labels[j] = l2i[f'B-{label}']    \n",
    "                if token_start > label_start and token_end <= label_end: \n",
    "                    labels[j] = l2i[f'I-{label}']\n",
    "        for k, input_id in enumerate(o['input_ids'][i]):\n",
    "            if input_id in [0,1,2]:\n",
    "                labels[k] = -100\n",
    "        labels = fix_beginnings(labels)\n",
    "        o[\"labels\"].append(labels)\n",
    "    return o"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:15.31952Z",
     "iopub.execute_input": "2021-12-23T23:03:15.319834Z",
     "iopub.status.idle": "2021-12-23T23:03:15.332639Z",
     "shell.execute_reply.started": "2021-12-23T23:03:15.319782Z",
     "shell.execute_reply": "2021-12-23T23:03:15.331235Z"
    },
    "trusted": true
   },
   "execution_count": 118,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenized_val = datasets.map(tokenize_for_validation, batched=True)\n",
    "tokenized_val"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:15.334494Z",
     "iopub.execute_input": "2021-12-23T23:03:15.335669Z",
     "iopub.status.idle": "2021-12-23T23:03:16.652272Z",
     "shell.execute_reply.started": "2021-12-23T23:03:15.335596Z",
     "shell.execute_reply": "2021-12-23T23:03:16.651209Z"
    },
    "trusted": true
   },
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "234cd284904740ffa0311a34907b0c03"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd468b7bb57047dc8d1f431dc03046c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'classlist', 'starts', 'ends', 'predictionstrings', 'text', '__index_level_0__', 'input_ids', 'attention_mask', 'offset_mapping', 'labels'],\n        num_rows: 14034\n    })\n    test: Dataset({\n        features: ['id', 'classlist', 'starts', 'ends', 'predictionstrings', 'text', '__index_level_0__', 'input_ids', 'attention_mask', 'offset_mapping', 'labels'],\n        num_rows: 1560\n    })\n})"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ground truth for validation\n",
    "l = []\n",
    "for example in tokenized_val['test']:\n",
    "    for c, p in list(zip(example['classlist'], example['predictionstrings'])):\n",
    "        l.append({\n",
    "            'id': example['id'],\n",
    "            'discourse_type': c,\n",
    "            'predictionstring': p,\n",
    "        })\n",
    "gt_df = pd.DataFrame(l)\n",
    "gt_df"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:16.654017Z",
     "iopub.execute_input": "2021-12-23T23:03:16.654625Z",
     "iopub.status.idle": "2021-12-23T23:03:16.711036Z",
     "shell.execute_reply.started": "2021-12-23T23:03:16.654567Z",
     "shell.execute_reply": "2021-12-23T23:03:16.710012Z"
    },
    "trusted": true
   },
   "execution_count": 120,
   "outputs": [
    {
     "data": {
      "text/plain": "                 id        discourse_type  \\\n0      7B5F5B33B566                  Lead   \n1      7B5F5B33B566              Position   \n2      7B5F5B33B566              Evidence   \n3      7B5F5B33B566                 Claim   \n4      7B5F5B33B566              Evidence   \n...             ...                   ...   \n14461  B3E4B633261B                 Claim   \n14462  B3E4B633261B              Evidence   \n14463  B3E4B633261B          Counterclaim   \n14464  B3E4B633261B              Rebuttal   \n14465  B3E4B633261B  Concluding Statement   \n\n                                        predictionstring  \n0      0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n1      43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 5...  \n2      69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 8...  \n3      166 167 168 169 170 171 172 173 174 175 176 17...  \n4      180 181 182 183 184 185 186 187 188 189 190 19...  \n...                                                  ...  \n14461  94 95 96 97 98 99 100 101 102 103 104 105 106 ...  \n14462  113 114 115 116 117 118 119 120 121 122 123 12...  \n14463                        126 127 128 129 130 131 132  \n14464  133 134 135 136 137 138 139 140 141 142 143 14...  \n14465  147 148 149 150 151 152 153 154 155 156 157 15...  \n\n[14466 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>discourse_type</th>\n      <th>predictionstring</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7B5F5B33B566</td>\n      <td>Lead</td>\n      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7B5F5B33B566</td>\n      <td>Position</td>\n      <td>43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 5...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7B5F5B33B566</td>\n      <td>Evidence</td>\n      <td>69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 8...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7B5F5B33B566</td>\n      <td>Claim</td>\n      <td>166 167 168 169 170 171 172 173 174 175 176 17...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7B5F5B33B566</td>\n      <td>Evidence</td>\n      <td>180 181 182 183 184 185 186 187 188 189 190 19...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14461</th>\n      <td>B3E4B633261B</td>\n      <td>Claim</td>\n      <td>94 95 96 97 98 99 100 101 102 103 104 105 106 ...</td>\n    </tr>\n    <tr>\n      <th>14462</th>\n      <td>B3E4B633261B</td>\n      <td>Evidence</td>\n      <td>113 114 115 116 117 118 119 120 121 122 123 12...</td>\n    </tr>\n    <tr>\n      <th>14463</th>\n      <td>B3E4B633261B</td>\n      <td>Counterclaim</td>\n      <td>126 127 128 129 130 131 132</td>\n    </tr>\n    <tr>\n      <th>14464</th>\n      <td>B3E4B633261B</td>\n      <td>Rebuttal</td>\n      <td>133 134 135 136 137 138 139 140 141 142 143 14...</td>\n    </tr>\n    <tr>\n      <th>14465</th>\n      <td>B3E4B633261B</td>\n      <td>Concluding Statement</td>\n      <td>147 148 149 150 151 152 153 154 155 156 157 15...</td>\n    </tr>\n  </tbody>\n</table>\n<p>14466 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# visualization with displacy\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "# import spacy\n",
    "# from spacy import displacy\n",
    "# from pylab import cm, matplotlib\n",
    "\n",
    "# this bit throw an error"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:16.712458Z",
     "iopub.execute_input": "2021-12-23T23:03:16.713221Z",
     "iopub.status.idle": "2021-12-23T23:03:16.719502Z",
     "shell.execute_reply.started": "2021-12-23T23:03:16.713168Z",
     "shell.execute_reply": "2021-12-23T23:03:16.718212Z"
    },
    "trusted": true
   },
   "execution_count": 121,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "path = Path('data/train')\n",
    "\n",
    "colors = {\n",
    "            'Lead': '#8000ff',\n",
    "            'Position': '#2b7ff6',\n",
    "            'Evidence': '#2adddd',\n",
    "            'Claim': '#80ffb4',\n",
    "            'Concluding Statement': 'd4dd80',\n",
    "            'Counterclaim': '#ff8042',\n",
    "            'Rebuttal': '#ff0000',\n",
    "            'Other': '#007f00',\n",
    "         }\n",
    "\n",
    "def visualize(df, text):\n",
    "    ents = []\n",
    "    example = df['id'].loc[0]\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        ents.append({\n",
    "                        'start': int(row['discourse_start']), \n",
    "                         'end': int(row['discourse_end']), \n",
    "                         'label': row['discourse_type']\n",
    "                    })\n",
    "\n",
    "    doc2 = {\n",
    "        \"text\": text,\n",
    "        \"ents\": ents,\n",
    "        \"title\": example\n",
    "    }\n",
    "\n",
    "    options = {\"ents\": train.discourse_type.unique().tolist() + ['Other'], \"colors\": colors}\n",
    "    displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:16.721142Z",
     "iopub.execute_input": "2021-12-23T23:03:16.721798Z",
     "iopub.status.idle": "2021-12-23T23:03:16.733508Z",
     "shell.execute_reply.started": "2021-12-23T23:03:16.721753Z",
     "shell.execute_reply": "2021-12-23T23:03:16.732443Z"
    },
    "trusted": true
   },
   "execution_count": 122,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_val['test'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:16.735115Z",
     "iopub.execute_input": "2021-12-23T23:03:16.736247Z",
     "iopub.status.idle": "2021-12-23T23:03:17.621012Z",
     "shell.execute_reply.started": "2021-12-23T23:03:16.736199Z",
     "shell.execute_reply": "2021-12-23T23:03:17.619921Z"
    },
    "trusted": true
   },
   "execution_count": 123,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `LongformerForTokenClassification.forward` and have been ignored: __index_level_0__, offset_mapping, ends, text, classlist, predictionstrings, starts, id.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 1\n",
      "Input ids are automatically padded from 564 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='1560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   1/1560 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 652 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 585 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 639 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 551 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 531 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 529 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 555 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 816 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 552 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1016 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 217 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 547 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 665 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 197 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 636 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 609 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 218 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 197 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 852 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 504 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 447 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 476 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 774 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 532 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 227 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 734 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 722 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 540 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 706 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 960 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 235 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 520 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 439 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 577 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 673 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 179 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 700 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 524 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 386 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 869 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 928 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 665 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 611 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 418 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 531 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1190 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1007 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 871 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 674 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 557 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 627 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 673 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 235 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 361 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 233 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 529 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 516 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1081 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 214 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 569 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 717 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 240 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 522 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 667 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 185 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 596 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 515 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 175 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 370 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 733 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 276 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 904 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1248 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 230 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 216 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 758 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 646 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 550 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 888 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 534 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 733 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 629 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 655 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 517 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 181 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1131 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 816 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 568 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 623 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 777 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 587 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 476 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 517 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 187 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 704 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 611 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 552 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 524 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 530 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 548 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 955 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 204 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 677 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 557 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 706 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 623 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 791 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 525 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 827 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 688 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 690 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 590 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1090 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 540 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 786 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 880 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 377 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 895 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 626 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 820 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 517 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 191 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 262 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 963 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 773 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 439 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 639 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 792 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 967 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 522 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 276 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 484 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 641 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1575 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 544 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 613 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 215 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 553 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 280 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 640 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 553 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 574 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 210 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 857 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 206 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 762 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 516 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 524 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 552 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 725 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 557 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 540 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 617 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 766 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 590 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 276 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 873 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1273 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 484 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 767 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 625 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 515 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 205 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 553 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 194 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 739 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 559 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 690 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 524 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 705 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 878 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 975 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 788 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 476 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 267 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1175 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 524 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1095 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 554 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 520 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 813 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 227 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 522 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 200 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 777 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 547 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 704 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 585 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 280 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 943 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 484 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 625 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 523 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 528 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1101 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 249 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 584 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 514 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 574 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 240 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1027 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 627 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 195 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 592 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 664 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 232 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 825 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 534 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 331 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 535 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 617 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 625 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 603 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 166 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 504 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 541 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 950 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 188 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 841 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 217 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 919 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 773 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 892 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 847 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 807 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 609 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 803 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 517 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 521 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 829 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 480 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 648 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 771 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 673 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 361 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 596 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 228 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 819 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 750 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 925 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 176 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 907 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 527 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 562 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 938 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 937 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 259 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1045 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 579 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 734 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 388 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 895 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 209 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 202 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 510 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 188 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 655 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 517 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1342 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 524 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 579 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1370 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 218 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 791 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 886 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 615 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 250 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 629 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 808 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 532 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 537 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 754 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 559 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1068 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 348 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1144 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 875 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 859 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 517 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 361 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 803 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 523 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 856 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 888 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 208 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 551 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1238 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 348 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 626 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 552 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 695 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 679 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 727 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 973 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 635 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 971 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 596 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 510 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 574 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 259 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 528 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 540 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1011 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 335 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 233 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 646 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 603 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 205 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1265 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 550 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 956 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 514 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 563 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 636 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1141 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 559 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 571 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 911 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 513 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 249 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 565 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 717 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 553 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 843 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 832 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 538 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1085 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 836 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 780 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 447 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 575 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 206 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 361 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 596 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1462 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 559 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 609 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 617 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 569 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 196 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 917 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 926 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1171 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 514 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 846 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1334 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 622 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 519 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 910 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 730 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 555 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 769 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 553 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 527 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1099 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 535 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 574 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 707 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 377 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 236 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 641 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 599 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 821 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 763 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 539 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 718 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 727 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 620 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 999 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1075 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 746 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 529 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 185 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 629 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1027 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 901 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 921 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 572 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 366 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 202 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 739 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 447 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 966 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 236 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1259 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 230 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 250 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 185 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 217 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 934 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 568 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 608 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1135 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 646 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 484 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 641 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 613 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 214 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 208 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 818 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 574 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1231 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 539 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 957 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 841 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 995 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 716 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 237 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1023 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 228 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 653 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 874 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1141 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 513 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1212 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 676 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 248 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 608 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1196 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 766 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 588 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 569 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1015 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 843 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 550 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 522 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 656 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 280 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 967 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 732 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 795 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 218 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 764 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 189 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 517 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 516 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 711 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 219 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 684 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 520 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 187 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 662 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 728 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 977 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 863 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 598 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 366 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 629 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 521 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 585 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 552 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 551 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 388 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 566 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 580 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 814 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 805 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 586 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1065 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 650 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1022 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 228 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 631 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 210 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1130 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 209 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 957 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 283 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 742 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 959 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1163 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 259 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 530 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 366 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 576 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 185 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 199 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 524 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 626 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 567 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 845 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 219 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1225 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 447 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 529 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 446 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 653 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 536 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 514 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 568 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 769 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 876 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 239 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 529 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 781 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 837 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 780 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 484 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 606 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1044 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 954 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 626 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 567 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 984 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 214 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 955 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 690 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 662 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 267 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 220 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 280 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 742 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 236 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1132 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 557 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 630 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 504 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 798 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 233 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 685 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 201 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 552 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 787 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 584 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 447 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 418 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 567 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 541 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 552 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 198 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 948 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 577 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 535 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 730 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 946 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 581 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 221 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 652 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 233 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 818 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1160 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 335 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 567 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 619 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 639 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 522 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 881 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 629 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 703 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 531 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 190 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 243 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 697 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 642 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 664 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 239 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 564 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 635 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 525 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 557 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 793 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 228 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 219 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 601 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 825 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 744 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 756 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 876 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 514 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 248 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 619 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 173 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 562 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 510 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1311 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 796 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 538 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 747 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 547 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 827 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 576 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 884 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 198 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1026 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 607 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 565 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 741 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 895 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 608 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 763 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 852 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 197 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 745 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1037 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 613 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 418 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 178 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 773 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 236 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 679 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 196 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 653 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 262 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1205 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 539 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 386 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 737 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 564 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 631 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1038 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 565 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 724 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 504 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 583 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 941 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 239 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 221 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 568 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 250 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 656 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 513 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 658 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 673 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 366 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 884 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 199 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1097 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 232 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 484 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 370 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 304 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1142 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 960 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 567 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 537 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 590 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 528 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 583 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 904 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 571 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 211 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 188 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 783 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 800 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 216 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 207 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 838 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 639 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 629 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1267 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 860 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 883 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 576 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 361 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1116 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 513 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 516 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 240 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 773 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 677 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 504 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 537 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 756 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 262 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 786 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 211 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 239 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 548 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 206 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 835 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 673 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 760 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 806 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 582 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 688 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 535 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 196 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 183 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 978 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 518 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 207 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1271 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 276 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 700 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1207 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 874 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 793 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 225 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 238 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 725 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 567 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 196 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 548 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 348 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 576 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 673 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 707 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 386 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 569 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 764 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 900 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 200 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "preds = np.argmax(predictions, axis=-1)\n",
    "preds.shape"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:17.622787Z",
     "iopub.execute_input": "2021-12-23T23:03:17.623357Z",
     "iopub.status.idle": "2021-12-23T23:03:17.632659Z",
     "shell.execute_reply.started": "2021-12-23T23:03:17.623297Z",
     "shell.execute_reply": "2021-12-23T23:03:17.631425Z"
    },
    "trusted": true
   },
   "execution_count": 124,
   "outputs": [
    {
     "data": {
      "text/plain": "(1560, 4096)"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# code that will convert our predictions into prediction strings, and visualize it at the same time this most likely requires some refactoring\n",
    "# mk: ummmm yeah....\n",
    "\n",
    "def get_class(c):\n",
    "    if c == 14: return 'Other'\n",
    "    else: return i2l[c][2:]\n",
    "\n",
    "def pred2span(pred, example, viz=False, test=False):\n",
    "    example_id = example['id']\n",
    "    n_tokens = len(example['input_ids'])\n",
    "    classes = []\n",
    "    all_span = []\n",
    "    for i, c in enumerate(pred.tolist()):\n",
    "        if i == n_tokens-1:\n",
    "            break\n",
    "        if i == 0:\n",
    "            cur_span = example['offset_mapping'][i]\n",
    "            classes.append(get_class(c))\n",
    "        elif i > 0 and (c == pred[i-1] or (c-7) == pred[i-1]):\n",
    "            cur_span[1] = example['offset_mapping'][i][1]\n",
    "        else:\n",
    "            all_span.append(cur_span)\n",
    "            cur_span = example['offset_mapping'][i]\n",
    "            classes.append(get_class(c))\n",
    "    all_span.append(cur_span)\n",
    "    \n",
    "    if test:\n",
    "        text = get_test_text(example_id)  # something wrong here. looks like there should be a function but its missing\n",
    "    else:\n",
    "        text = get_raw_text(example_id)  # obviously this is a function call\n",
    "    \n",
    "    # map token ids to word (whitespace) token ids\n",
    "    predstrings = []\n",
    "    for span in all_span:\n",
    "        span_start = span[0]\n",
    "        span_end = span[1]\n",
    "        before = text[:span_start]\n",
    "        token_start = len(before.split())\n",
    "        if len(before) == 0: token_start = 0\n",
    "        elif before[-1] != ' ': token_start -= 1\n",
    "        num_tkns = len(text[span_start:span_end+1].split())\n",
    "        tkns = [str(x) for x in range(token_start, token_start+num_tkns)]\n",
    "        predstring = ' '.join(tkns)\n",
    "        predstrings.append(predstring)\n",
    "                    \n",
    "    rows = []\n",
    "    for c, span, predstring in zip(classes, all_span, predstrings):\n",
    "        e = {\n",
    "            'id': example_id,\n",
    "            'discourse_type': c,\n",
    "            'predictionstring': predstring,\n",
    "            'discourse_start': span[0],\n",
    "            'discourse_end': span[1],\n",
    "            'discourse': text[span[0]:span[1]+1]\n",
    "        }\n",
    "        rows.append(e)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df['length'] = df['discourse'].apply(lambda t: len(t.split()))\n",
    "    \n",
    "    # short spans are likely to be false positives, we can choose a min number of tokens based on validation\n",
    "    df = df[df.length > min_tokens].reset_index(drop=True)\n",
    "    if viz: visualize(df, text)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:17.634765Z",
     "iopub.execute_input": "2021-12-23T23:03:17.63535Z",
     "iopub.status.idle": "2021-12-23T23:03:17.655065Z",
     "shell.execute_reply.started": "2021-12-23T23:03:17.635228Z",
     "shell.execute_reply": "2021-12-23T23:03:17.653955Z"
    },
    "trusted": true
   },
   "execution_count": 125,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from spacy import displacy\n",
    "pred2span(preds[0], tokenized_val['test'][0], viz=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:17.658868Z",
     "iopub.execute_input": "2021-12-23T23:03:17.659221Z",
     "iopub.status.idle": "2021-12-23T23:03:17.712976Z",
     "shell.execute_reply.started": "2021-12-23T23:03:17.659184Z",
     "shell.execute_reply": "2021-12-23T23:03:17.711747Z"
    },
    "trusted": true
   },
   "execution_count": 126,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><h2 style=\"margin: 0\">7B5F5B33B566</h2>\n\n<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    When people ask for advice\n\n, they sometimes talk to more than one person. Have you ever wounder why that is the case?, if you are a person who does not take advice from another individual , then you could use this skill,\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Lead</span>\n</mark>\n \n<mark class=\"entity\" style=\"background: #2b7ff6; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    taking advice from another person can help\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Position</span>\n</mark>\n you make better life choices, it will make \n<mark class=\"entity\" style=\"background: #80ffb4; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    you understand things more clearly and faster,\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Claim</span>\n</mark>\n and learn from them. \n<mark class=\"entity\" style=\"background: #2adddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Multiple opinions really is a foundation to a job like a mayor of a city, it is vital to take advice from the community in\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Evidence</span>\n</mark>\n order \n<mark class=\"entity\" style=\"background: #2adddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    to address problems and concerns. If it was just a city with no voice in the community then the city would fall into a bad place which no one wants to be in and the key advice is to receive it.\n\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Evidence</span>\n</mark>\n</br>\n<mark class=\"entity\" style=\"background: #2adddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    In the world we live in, we make a lot of important choices in life, some are big and some are small, but they effect our future and success.\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Evidence</span>\n</mark>\n Next, advice from others can make you more wiser when you make your own choices, if</br></br>\n<mark class=\"entity\" style=\"background: #2adddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Abraham\n\nLincoln never saw how African Americans were treated for there skin color he would have never signed the documents for slavery to be band. if he never got advice he mostly likely would have never Andres the problems with having that toxic racism. If it wasnt for him, it would still be here today thanks to out 16th President, which boils down to good thinking and advice. At some point in our lives I know we got advice from someone we look up too,also what ever the advice they gave you, is an add on to bigger things, for example if a parent or guardian gives you some advice to never quit on things or problems that are hard and difficult for you, with just that you will Carey that for the rest of your life. Which eventually make you more wiser and resistant to challenges and in life. You will have a mindset that will help you be more successful and a hard worker. When you are at that point you will know what is right and wrong, from advice from your remodel, father, and mother. Multiple opinions is like trying different types of food from different chefs and if you just try it, you will see something you might like, if you never try something you will never see a different possibility taking advice is just like that. Only instead its intellects\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Evidence</span>\n</mark>\n make the right choice to help you.</br></br>In Conclusion, \n<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    advice is there to help you, opinions are there to guide you to something that gives you smarts to making the right choices in the world we live in live, sure not all opinions hear might not be in your favor. But just hear them out and be the best you that you want to be shaped in, advice will make yo wiser and more aware of scenarios. \n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Concluding Statement</span>\n</mark>\n</div></span>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "             id        discourse_type  \\\n0  7B5F5B33B566                  Lead   \n1  7B5F5B33B566              Position   \n2  7B5F5B33B566                 Claim   \n3  7B5F5B33B566              Evidence   \n4  7B5F5B33B566              Evidence   \n5  7B5F5B33B566              Evidence   \n6  7B5F5B33B566              Evidence   \n7  7B5F5B33B566  Concluding Statement   \n\n                                    predictionstring  discourse_start  \\\n0  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...                0   \n1                               43 44 45 46 47 48 49              222   \n2                               58 59 60 61 62 63 64              308   \n3  69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 8...              376   \n4  95 96 97 98 99 100 101 102 103 104 105 106 107...              505   \n5  135 136 137 138 139 140 141 142 143 144 145 14...              700   \n6  180 181 182 183 184 185 186 187 188 189 190 19...              927   \n7  425 426 427 428 429 430 431 432 433 434 435 43...             2249   \n\n   discourse_end                                          discourse  length  \n0            221  When people ask for advice\\n\\n, they sometimes...      43  \n1            264        taking advice from another person can help        7  \n2            354    you understand things more clearly and faster,        7  \n3            498  Multiple opinions really is a foundation to a ...      25  \n4            699  to address problems and concerns. If it was ju...      41  \n5            841  In the world we live in, we make a lot of impo...      29  \n6           2197  Abraham\\n\\nLincoln never saw how African Ameri...     235  \n7           2587  advice is there to help you, opinions are ther...      68  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>discourse_type</th>\n      <th>predictionstring</th>\n      <th>discourse_start</th>\n      <th>discourse_end</th>\n      <th>discourse</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7B5F5B33B566</td>\n      <td>Lead</td>\n      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n      <td>0</td>\n      <td>221</td>\n      <td>When people ask for advice\\n\\n, they sometimes...</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7B5F5B33B566</td>\n      <td>Position</td>\n      <td>43 44 45 46 47 48 49</td>\n      <td>222</td>\n      <td>264</td>\n      <td>taking advice from another person can help</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7B5F5B33B566</td>\n      <td>Claim</td>\n      <td>58 59 60 61 62 63 64</td>\n      <td>308</td>\n      <td>354</td>\n      <td>you understand things more clearly and faster,</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7B5F5B33B566</td>\n      <td>Evidence</td>\n      <td>69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 8...</td>\n      <td>376</td>\n      <td>498</td>\n      <td>Multiple opinions really is a foundation to a ...</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7B5F5B33B566</td>\n      <td>Evidence</td>\n      <td>95 96 97 98 99 100 101 102 103 104 105 106 107...</td>\n      <td>505</td>\n      <td>699</td>\n      <td>to address problems and concerns. If it was ju...</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>7B5F5B33B566</td>\n      <td>Evidence</td>\n      <td>135 136 137 138 139 140 141 142 143 144 145 14...</td>\n      <td>700</td>\n      <td>841</td>\n      <td>In the world we live in, we make a lot of impo...</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7B5F5B33B566</td>\n      <td>Evidence</td>\n      <td>180 181 182 183 184 185 186 187 188 189 190 19...</td>\n      <td>927</td>\n      <td>2197</td>\n      <td>Abraham\\n\\nLincoln never saw how African Ameri...</td>\n      <td>235</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7B5F5B33B566</td>\n      <td>Concluding Statement</td>\n      <td>425 426 427 428 429 430 431 432 433 434 435 43...</td>\n      <td>2249</td>\n      <td>2587</td>\n      <td>advice is there to help you, opinions are ther...</td>\n      <td>68</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "pred2span(preds[1], tokenized_val['test'][1], viz=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:17.71609Z",
     "iopub.execute_input": "2021-12-23T23:03:17.716626Z",
     "iopub.status.idle": "2021-12-23T23:03:17.757272Z",
     "shell.execute_reply.started": "2021-12-23T23:03:17.716588Z",
     "shell.execute_reply": "2021-12-23T23:03:17.756227Z"
    },
    "trusted": true
   },
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><h2 style=\"margin: 0\">3CF52C3ED074</h2>\n\n<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    All students do is waste their time and i'm tired of it. Students complain that school is like a prison but that is only because they don't realize the fun part of school.\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Lead</span>\n</mark>\n \n<mark class=\"entity\" style=\"background: #2b7ff6; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    I agree with the principal that all students should participate in at least one extracurricular activity. Students are too lazy instead of wasting time\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Position</span>\n</mark>\n they should participate in after school activities. For students it could benefit a lot, give kids good memories, and make school more fun.\n<mark class=\"entity\" style=\"background: #007f00; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    \n\nMy first reason is, It could benefit a lot.\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Other</span>\n</mark>\n \n<mark class=\"entity\" style=\"background: #2adddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Doing extracurricular activity can give an idea of what you want to do in the future. Sports is a very popular career choice for students but sports takes skills maybe sports could be your future. If your not very athletic thats ok many people enjoy debate club many politicians debate and if that is your skill maybe that can be your career. These after school activities can even help you learn about yourself. Make sure to participate in something you enjoy or try something you never done before. the best part about learning about yourself is finding your hidden talent in any type of activities.\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Evidence</span>\n</mark>\n</br></br>My next reasoning is, giving kids good memory's. \n<mark class=\"entity\" style=\"background: #2adddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    In competitions there is always a reward, and good remember able thing is achieving that reward for your team and school. The highest achievement to accomplish is winning a tournament with your team. Even if you dont win at least you made it to the the finals. The most important part about extracurricular activity is having a fun time. No matter how popular you are you dont know everyone so get to know your teammates and make new friends. From my experience I played basketball and it was a really fun time but i'll never forget when i scored my first point. I promise you things are a lot more memorable when you do something exciting. Everyday you learn something new which can help you get better at what you activity you enjoy to do, and get to know your aquantince.\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Evidence</span>\n</mark>\n</br></br>My last reasoning is, it would make school way more fun. \n<mark class=\"entity\" style=\"background: #2adddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Tell your friends and family what you did today. share with them your achievements, And what made your day. Make coming to more exciting. come to school prepared for after school activities. Now this might seem like too much pressure, but bring your school a trophy!\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Evidence</span>\n</mark>\n</br></br>In conclusion, \n<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    i strongly agree with the principal decision that all students participate. Our school has a lot of activities that will interest all students. School goes by really fast so at least make the most of your years here. Because students are too lazy instead i believe all students should participate in after school activities. It would be beneficial for students, give students something to remember, and enjoy school more.\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Concluding Statement</span>\n</mark>\n</div></span>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "             id        discourse_type  \\\n0  3CF52C3ED074                  Lead   \n1  3CF52C3ED074              Position   \n2  3CF52C3ED074                 Other   \n3  3CF52C3ED074              Evidence   \n4  3CF52C3ED074              Evidence   \n5  3CF52C3ED074              Evidence   \n6  3CF52C3ED074  Concluding Statement   \n\n                                    predictionstring  discourse_start  \\\n0  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...                0   \n1  33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 4...              172   \n2                         79 80 81 82 83 84 85 86 87              463   \n3  89 90 91 92 93 94 95 96 97 98 99 100 101 102 1...              509   \n4  202 203 204 205 206 207 208 209 210 211 212 21...             1161   \n5  354 355 356 357 358 359 360 361 362 363 364 36...             1994   \n6  402 403 404 405 406 407 408 409 410 411 412 41...             2277   \n\n   discourse_end                                          discourse  length  \n0            171  All students do is waste their time and i'm ti...      33  \n1            323  I agree with the principal that all students s...      24  \n2            508   \\n\\nMy first reason is, It could benefit a lot.        9  \n3           1110  Doing extracurricular activity can give an ide...     105  \n4           1935  In competitions there is always a reward, and ...     141  \n5           2260  Tell your friends and family what you did toda...      46  \n6           2698  i strongly agree with the principal decision t...      69  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>discourse_type</th>\n      <th>predictionstring</th>\n      <th>discourse_start</th>\n      <th>discourse_end</th>\n      <th>discourse</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3CF52C3ED074</td>\n      <td>Lead</td>\n      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n      <td>0</td>\n      <td>171</td>\n      <td>All students do is waste their time and i'm ti...</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3CF52C3ED074</td>\n      <td>Position</td>\n      <td>33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 4...</td>\n      <td>172</td>\n      <td>323</td>\n      <td>I agree with the principal that all students s...</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3CF52C3ED074</td>\n      <td>Other</td>\n      <td>79 80 81 82 83 84 85 86 87</td>\n      <td>463</td>\n      <td>508</td>\n      <td>\\n\\nMy first reason is, It could benefit a lot.</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3CF52C3ED074</td>\n      <td>Evidence</td>\n      <td>89 90 91 92 93 94 95 96 97 98 99 100 101 102 1...</td>\n      <td>509</td>\n      <td>1110</td>\n      <td>Doing extracurricular activity can give an ide...</td>\n      <td>105</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3CF52C3ED074</td>\n      <td>Evidence</td>\n      <td>202 203 204 205 206 207 208 209 210 211 212 21...</td>\n      <td>1161</td>\n      <td>1935</td>\n      <td>In competitions there is always a reward, and ...</td>\n      <td>141</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3CF52C3ED074</td>\n      <td>Evidence</td>\n      <td>354 355 356 357 358 359 360 361 362 363 364 36...</td>\n      <td>1994</td>\n      <td>2260</td>\n      <td>Tell your friends and family what you did toda...</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3CF52C3ED074</td>\n      <td>Concluding Statement</td>\n      <td>402 403 404 405 406 407 408 409 410 411 412 41...</td>\n      <td>2277</td>\n      <td>2698</td>\n      <td>i strongly agree with the principal decision t...</td>\n      <td>69</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dfs = []\n",
    "for i in range(len(tokenized_val['test'])):\n",
    "    dfs.append(pred2span(preds[i], tokenized_val['test'][i]))\n",
    "\n",
    "pred_df = pd.concat(dfs, axis=0)\n",
    "pred_df['class'] = pred_df['discourse_type']\n",
    "pred_df"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:17.759337Z",
     "iopub.execute_input": "2021-12-23T23:03:17.760071Z",
     "iopub.status.idle": "2021-12-23T23:03:17.883329Z",
     "shell.execute_reply.started": "2021-12-23T23:03:17.760003Z",
     "shell.execute_reply": "2021-12-23T23:03:17.8822Z"
    },
    "trusted": true
   },
   "execution_count": 128,
   "outputs": [
    {
     "data": {
      "text/plain": "              id        discourse_type  \\\n0   7B5F5B33B566                  Lead   \n1   7B5F5B33B566              Position   \n2   7B5F5B33B566                 Claim   \n3   7B5F5B33B566              Evidence   \n4   7B5F5B33B566              Evidence   \n..           ...                   ...   \n6   B3E4B633261B              Evidence   \n7   B3E4B633261B              Evidence   \n8   B3E4B633261B              Evidence   \n9   B3E4B633261B              Evidence   \n10  B3E4B633261B  Concluding Statement   \n\n                                     predictionstring  discourse_start  \\\n0   0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...                0   \n1                                43 44 45 46 47 48 49              222   \n2                                58 59 60 61 62 63 64              308   \n3   69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 8...              376   \n4   95 96 97 98 99 100 101 102 103 104 105 106 107...              505   \n..                                                ...              ...   \n6   72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 8...              396   \n7   102 103 104 105 106 107 108 109 110 111 112 11...              555   \n8             116 117 118 119 120 121 122 123 124 125              634   \n9                     133 134 135 136 137 138 139 140              727   \n10  146 147 148 149 150 151 152 153 154 155 156 15...              795   \n\n    discourse_end                                          discourse  length  \\\n0             221  When people ask for advice\\n\\n, they sometimes...      43   \n1             264        taking advice from another person can help        7   \n2             354    you understand things more clearly and faster,        7   \n3             498  Multiple opinions really is a foundation to a ...      25   \n4             699  to address problems and concerns. If it was ju...      41   \n..            ...                                                ...     ...   \n6             548  They will see all the people they helped all t...      29   \n7             628  for communities, people, plants an animals an ...      13   \n8             688  this world a better place for the generations ...      10   \n9             765            but they will thank you for it latter.        8   \n10            906  So i encourages you to require all students to...      21   \n\n                   class  \n0                   Lead  \n1               Position  \n2                  Claim  \n3               Evidence  \n4               Evidence  \n..                   ...  \n6               Evidence  \n7               Evidence  \n8               Evidence  \n9               Evidence  \n10  Concluding Statement  \n\n[15964 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>discourse_type</th>\n      <th>predictionstring</th>\n      <th>discourse_start</th>\n      <th>discourse_end</th>\n      <th>discourse</th>\n      <th>length</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7B5F5B33B566</td>\n      <td>Lead</td>\n      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n      <td>0</td>\n      <td>221</td>\n      <td>When people ask for advice\\n\\n, they sometimes...</td>\n      <td>43</td>\n      <td>Lead</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7B5F5B33B566</td>\n      <td>Position</td>\n      <td>43 44 45 46 47 48 49</td>\n      <td>222</td>\n      <td>264</td>\n      <td>taking advice from another person can help</td>\n      <td>7</td>\n      <td>Position</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7B5F5B33B566</td>\n      <td>Claim</td>\n      <td>58 59 60 61 62 63 64</td>\n      <td>308</td>\n      <td>354</td>\n      <td>you understand things more clearly and faster,</td>\n      <td>7</td>\n      <td>Claim</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7B5F5B33B566</td>\n      <td>Evidence</td>\n      <td>69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 8...</td>\n      <td>376</td>\n      <td>498</td>\n      <td>Multiple opinions really is a foundation to a ...</td>\n      <td>25</td>\n      <td>Evidence</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7B5F5B33B566</td>\n      <td>Evidence</td>\n      <td>95 96 97 98 99 100 101 102 103 104 105 106 107...</td>\n      <td>505</td>\n      <td>699</td>\n      <td>to address problems and concerns. If it was ju...</td>\n      <td>41</td>\n      <td>Evidence</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>B3E4B633261B</td>\n      <td>Evidence</td>\n      <td>72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 8...</td>\n      <td>396</td>\n      <td>548</td>\n      <td>They will see all the people they helped all t...</td>\n      <td>29</td>\n      <td>Evidence</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>B3E4B633261B</td>\n      <td>Evidence</td>\n      <td>102 103 104 105 106 107 108 109 110 111 112 11...</td>\n      <td>555</td>\n      <td>628</td>\n      <td>for communities, people, plants an animals an ...</td>\n      <td>13</td>\n      <td>Evidence</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>B3E4B633261B</td>\n      <td>Evidence</td>\n      <td>116 117 118 119 120 121 122 123 124 125</td>\n      <td>634</td>\n      <td>688</td>\n      <td>this world a better place for the generations ...</td>\n      <td>10</td>\n      <td>Evidence</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>B3E4B633261B</td>\n      <td>Evidence</td>\n      <td>133 134 135 136 137 138 139 140</td>\n      <td>727</td>\n      <td>765</td>\n      <td>but they will thank you for it latter.</td>\n      <td>8</td>\n      <td>Evidence</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>B3E4B633261B</td>\n      <td>Concluding Statement</td>\n      <td>146 147 148 149 150 151 152 153 154 155 156 15...</td>\n      <td>795</td>\n      <td>906</td>\n      <td>So i encourages you to require all students to...</td>\n      <td>21</td>\n      <td>Concluding Statement</td>\n    </tr>\n  </tbody>\n</table>\n<p>15964 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# source: https://www.kaggle.com/robikscube/student-writing-competition-twitch#Competition-Metric-Code\n",
    "\n",
    "def calc_overlap(row):\n",
    "    \"\"\"\n",
    "    Calculates the overlap between prediction and\n",
    "    ground truth and overlap percentages used for determining\n",
    "    true positives.\n",
    "    \"\"\"\n",
    "    set_pred = set(row.predictionstring_pred.split(\" \"))\n",
    "    set_gt = set(row.predictionstring_gt.split(\" \"))\n",
    "    # Length of each and intersection\n",
    "    len_gt = len(set_gt)\n",
    "    len_pred = len(set_pred)\n",
    "    inter = len(set_gt.intersection(set_pred))\n",
    "    overlap_1 = inter / len_gt\n",
    "    overlap_2 = inter / len_pred\n",
    "    return [overlap_1, overlap_2]\n",
    "\n",
    "\n",
    "def score_feedback_comp_micro(pred_df, gt_df):\n",
    "    \"\"\"\n",
    "    A function that scores for the kaggle\n",
    "        Student Writing Competition\n",
    "\n",
    "    Uses the steps in the evaluation page here:\n",
    "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
    "    \"\"\"\n",
    "    gt_df = (\n",
    "        gt_df[[\"id\", \"discourse_type\", \"predictionstring\"]]\n",
    "        .reset_index(drop=True)\n",
    "        .copy()\n",
    "    )\n",
    "    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n",
    "    pred_df[\"pred_id\"] = pred_df.index\n",
    "    gt_df[\"gt_id\"] = gt_df.index\n",
    "    # Step 1. all ground truths and predictions for a given class are compared.\n",
    "    joined = pred_df.merge(\n",
    "        gt_df,\n",
    "        left_on=[\"id\", \"class\"],\n",
    "        right_on=[\"id\", \"discourse_type\"],\n",
    "        how=\"outer\",\n",
    "        suffixes=(\"_pred\", \"_gt\"),\n",
    "    )\n",
    "    joined[\"predictionstring_gt\"] = joined[\"predictionstring_gt\"].fillna(\" \")\n",
    "    joined[\"predictionstring_pred\"] = joined[\"predictionstring_pred\"].fillna(\" \")\n",
    "\n",
    "    joined[\"overlaps\"] = joined.apply(calc_overlap, axis=1)\n",
    "\n",
    "    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n",
    "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
    "    # the prediction is a match and considered a true positive.\n",
    "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "    joined[\"overlap1\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[0])\n",
    "    joined[\"overlap2\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[1])\n",
    "\n",
    "    joined[\"potential_TP\"] = (joined[\"overlap1\"] >= 0.5) & (joined[\"overlap2\"] >= 0.5)\n",
    "    joined[\"max_overlap\"] = joined[[\"overlap1\", \"overlap2\"]].max(axis=1)\n",
    "    tp_pred_ids = (\n",
    "        joined.query(\"potential_TP\")\n",
    "        .sort_values(\"max_overlap\", ascending=False)\n",
    "        .groupby([\"id\", \"predictionstring_gt\"])\n",
    "        .first()[\"pred_id\"]\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    # 3. Any unmatched ground truths are false negatives\n",
    "    # and any unmatched predictions are false positives.\n",
    "    fp_pred_ids = [p for p in joined[\"pred_id\"].unique() if p not in tp_pred_ids]\n",
    "\n",
    "    matched_gt_ids = joined.query(\"potential_TP\")[\"gt_id\"].unique()\n",
    "    unmatched_gt_ids = [c for c in joined[\"gt_id\"].unique() if c not in matched_gt_ids]\n",
    "\n",
    "    # Get numbers of each type\n",
    "    TP = len(tp_pred_ids)\n",
    "    FP = len(fp_pred_ids)\n",
    "    FN = len(unmatched_gt_ids)\n",
    "    # calc microf1\n",
    "    my_f1_score = TP / (TP + 0.5 * (FP + FN))\n",
    "    return my_f1_score\n",
    "\n",
    "\n",
    "def score_feedback_comp(pred_df, gt_df, return_class_scores=False):\n",
    "    class_scores = {}\n",
    "    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n",
    "    for discourse_type, gt_subset in gt_df.groupby(\"discourse_type\"):\n",
    "        pred_subset = (\n",
    "            pred_df.loc[pred_df[\"class\"] == discourse_type]\n",
    "            .reset_index(drop=True)\n",
    "            .copy()\n",
    "        )\n",
    "        class_score = score_feedback_comp_micro(pred_subset, gt_subset)\n",
    "        class_scores[discourse_type] = class_score\n",
    "    f1 = np.mean([v for v in class_scores.values()])\n",
    "    if return_class_scores:\n",
    "        return f1, class_scores\n",
    "    return f1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:17.885121Z",
     "iopub.execute_input": "2021-12-23T23:03:17.885735Z",
     "iopub.status.idle": "2021-12-23T23:03:17.908285Z",
     "shell.execute_reply.started": "2021-12-23T23:03:17.88567Z",
     "shell.execute_reply": "2021-12-23T23:03:17.907198Z"
    },
    "trusted": true
   },
   "execution_count": 129,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CV Score"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "score_feedback_comp(pred_df, gt_df, return_class_scores=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:17.910018Z",
     "iopub.execute_input": "2021-12-23T23:03:17.910701Z",
     "iopub.status.idle": "2021-12-23T23:03:18.110011Z",
     "shell.execute_reply.started": "2021-12-23T23:03:17.910652Z",
     "shell.execute_reply": "2021-12-23T23:03:18.108723Z"
    },
    "trusted": true
   },
   "execution_count": 130,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.622527447496722,\n {'Claim': 0.5571841453344344,\n  'Concluding Statement': 0.798907476954592,\n  'Counterclaim': 0.4758679085520745,\n  'Evidence': 0.7149005943840951,\n  'Lead': 0.7856782652546647,\n  'Position': 0.6389797253106606,\n  'Rebuttal': 0.38617401668653156})"
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ]
}