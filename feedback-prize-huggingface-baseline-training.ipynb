{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# HuggingFace Training Baseline\n",
    "\n",
    "I wanted to create my own baseline for this competition, and I tried to do so \"without peeking\" at the kernels published by others. Ideally this can be used for training on a Kaggle kernel. Let's see how good we can get. \n",
    "\n",
    "This baseline is based on the following notebook by Sylvain Gugger: https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb\n",
    "\n",
    "I initially started building with Roberta - thanks to Chris Deotte for pointing to Longformer :) The evaluation code is from Rob Mulla.\n",
    "\n",
    "The notebook requires a couple of hours to run, so we'll use W&B to be able to monitor it along the way and keep the record of our experiments. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "SAMPLE = False # set True for debugging"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T22:59:40.43361Z",
     "iopub.execute_input": "2021-12-23T22:59:40.434Z",
     "iopub.status.idle": "2021-12-23T22:59:40.438896Z",
     "shell.execute_reply.started": "2021-12-23T22:59:40.433966Z",
     "shell.execute_reply": "2021-12-23T22:59:40.437857Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install seqeval -qq # evaluation metrics for training (not the competition metric)\n",
    "# !pip install --upgrade wandb -qq # experiment tracking"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T22:59:40.441479Z",
     "iopub.execute_input": "2021-12-23T22:59:40.442112Z",
     "iopub.status.idle": "2021-12-23T23:00:00.091404Z",
     "shell.execute_reply.started": "2021-12-23T22:59:40.442065Z",
     "shell.execute_reply": "2021-12-23T23:00:00.090244Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# setup wandb for experiment tracking\n",
    "# source: https://www.kaggle.com/debarshichanda/pytorch-w-b-jigsaw-starter\n",
    "\n",
    "import wandb\n",
    "\n",
    "# try:\n",
    "#     from kaggle_secrets import UserSecretsClient\n",
    "#     user_secrets = UserSecretsClient()\n",
    "#     api_key = user_secrets.get_secret(\"wandb_api\")\n",
    "#     wandb.login(key=api_key)\n",
    "#     wandb.init(project=\"feedback_prize\", entity=\"darek\")\n",
    "#     anony = None\n",
    "# except:\n",
    "#     anony = \"must\"\n",
    "#     print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:00.094757Z",
     "iopub.execute_input": "2021-12-23T23:00:00.095189Z",
     "iopub.status.idle": "2021-12-23T23:00:08.865381Z",
     "shell.execute_reply.started": "2021-12-23T23:00:00.095139Z",
     "shell.execute_reply": "2021-12-23T23:00:08.86421Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:1s10ofc9) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 47055... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e71457f6e6f648d7973cdb8253b0cb64"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n</div><div class=\"wandb-col\">\n</div></div>\nSynced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">warm-thunder-37</strong>: <a href=\"https://wandb.ai/feedback_prize_michael_and_wilson/feedback_prize_pytorch/runs/1s10ofc9\" target=\"_blank\">https://wandb.ai/feedback_prize_michael_and_wilson/feedback_prize_pytorch/runs/1s10ofc9</a><br/>\nFind logs at: <code>./wandb/run-20220301_001600-1s10ofc9/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:1s10ofc9). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/feedback_prize_michael_and_wilson/feedback_prize_pytorch/runs/2wnz3pjl\" target=\"_blank\">glowing-capybara-38</a></strong> to <a href=\"https://wandb.ai/feedback_prize_michael_and_wilson/feedback_prize_pytorch\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/feedback_prize_michael_and_wilson/feedback_prize_pytorch/runs/2wnz3pjl?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x7f3c5853be80>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb_creds import *\n",
    "\n",
    "wandb.login(key=API_KEY)\n",
    "wandb.init(project=\"feedback_prize_pytorch\", tags=TAGS, entity=\"feedback_prize_michael_and_wilson\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# CONFIG\n",
    "\n",
    "EXP_NUM = 1\n",
    "task = \"ner\"\n",
    "model_checkpoint = \"longformer-base-4096-hf\"\n",
    "max_length = 1024\n",
    "stride = 128\n",
    "min_tokens = 6\n",
    "model_path = f'{model_checkpoint.split(\"/\")[-1]}-{EXP_NUM}'\n",
    "\n",
    "# TRAINING HYPERPARAMS\n",
    "BS = 4\n",
    "GRAD_ACC = 8\n",
    "LR = 5e-5\n",
    "WD = 0.01\n",
    "WARMUP = 0.1\n",
    "N_EPOCHS = 5"
   ],
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:08.872471Z",
     "iopub.execute_input": "2021-12-23T23:00:08.875384Z",
     "iopub.status.idle": "2021-12-23T23:00:09.613866Z",
     "shell.execute_reply.started": "2021-12-23T23:00:08.875328Z",
     "shell.execute_reply": "2021-12-23T23:00:09.612856Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read train data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "train.head(1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:09.615125Z",
     "iopub.execute_input": "2021-12-23T23:00:09.615508Z",
     "iopub.status.idle": "2021-12-23T23:00:11.240349Z",
     "shell.execute_reply.started": "2021-12-23T23:00:09.615458Z",
     "shell.execute_reply": "2021-12-23T23:00:11.239275Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "             id  discourse_id  discourse_start  discourse_end  \\\n0  423A1CA112E2  1.622628e+12              8.0          229.0   \n\n                                      discourse_text discourse_type  \\\n0  Modern humans today are always on their phone....           Lead   \n\n  discourse_type_num                                   predictionstring  \n0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>discourse_id</th>\n      <th>discourse_start</th>\n      <th>discourse_end</th>\n      <th>discourse_text</th>\n      <th>discourse_type</th>\n      <th>discourse_type_num</th>\n      <th>predictionstring</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>8.0</td>\n      <td>229.0</td>\n      <td>Modern humans today are always on their phone....</td>\n      <td>Lead</td>\n      <td>Lead 1</td>\n      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# check unique classes\n",
    "classes = train.discourse_type.unique().tolist()\n",
    "classes"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:11.245598Z",
     "iopub.execute_input": "2021-12-23T23:00:11.248663Z",
     "iopub.status.idle": "2021-12-23T23:00:12.088646Z",
     "shell.execute_reply.started": "2021-12-23T23:00:11.248611Z",
     "shell.execute_reply": "2021-12-23T23:00:12.087709Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "['Lead',\n 'Position',\n 'Evidence',\n 'Claim',\n 'Concluding Statement',\n 'Counterclaim',\n 'Rebuttal']"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# setup label indices\n",
    "\n",
    "from collections import defaultdict\n",
    "tags = defaultdict()\n",
    "\n",
    "for i, c in enumerate(classes):\n",
    "    tags[f'B-{c}'] = i\n",
    "    tags[f'I-{c}'] = i + len(classes)\n",
    "tags[f'O'] = len(classes) * 2\n",
    "tags[f'Special'] = -100\n",
    "    \n",
    "l2i = dict(tags)\n",
    "\n",
    "i2l = defaultdict()\n",
    "for k, v in l2i.items(): \n",
    "    i2l[v] = k\n",
    "i2l[-100] = 'Special'\n",
    "\n",
    "i2l = dict(i2l)\n",
    "\n",
    "N_LABELS = len(i2l) - 1 # not accounting for -100"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:12.090074Z",
     "iopub.execute_input": "2021-12-23T23:00:12.090401Z",
     "iopub.status.idle": "2021-12-23T23:00:12.909927Z",
     "shell.execute_reply.started": "2021-12-23T23:00:12.090357Z",
     "shell.execute_reply": "2021-12-23T23:00:12.908979Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# some helper functions\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path('data/train')\n",
    "\n",
    "def get_raw_text(ids):\n",
    "    with open(path/f'{ids}.txt', 'r') as file: data = file.read()\n",
    "    return data"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:12.913651Z",
     "iopub.execute_input": "2021-12-23T23:00:12.913893Z",
     "iopub.status.idle": "2021-12-23T23:00:13.630498Z",
     "shell.execute_reply.started": "2021-12-23T23:00:12.913861Z",
     "shell.execute_reply": "2021-12-23T23:00:13.629554Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# group training labels by text file\n",
    "\n",
    "df1 = train.groupby('id')['discourse_type'].apply(list).reset_index(name='classlist')\n",
    "df2 = train.groupby('id')['discourse_start'].apply(list).reset_index(name='starts')\n",
    "df3 = train.groupby('id')['discourse_end'].apply(list).reset_index(name='ends')\n",
    "df4 = train.groupby('id')['predictionstring'].apply(list).reset_index(name='predictionstrings')\n",
    "\n",
    "df = pd.merge(df1, df2, how='inner', on='id')\n",
    "df = pd.merge(df, df3, how='inner', on='id')\n",
    "df = pd.merge(df, df4, how='inner', on='id')\n",
    "df['text'] = df['id'].apply(get_raw_text)\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:13.634902Z",
     "iopub.execute_input": "2021-12-23T23:00:13.635138Z",
     "iopub.status.idle": "2021-12-23T23:00:24.829274Z",
     "shell.execute_reply.started": "2021-12-23T23:00:13.635107Z",
     "shell.execute_reply": "2021-12-23T23:00:24.828189Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "             id                                          classlist  \\\n0  0000D23A521A  [Position, Evidence, Evidence, Claim, Counterc...   \n1  00066EA9880D  [Lead, Position, Claim, Evidence, Claim, Evide...   \n2  000E6DE9E817  [Position, Counterclaim, Rebuttal, Evidence, C...   \n3  001552828BD0  [Lead, Evidence, Claim, Claim, Evidence, Claim...   \n4  0016926B079C  [Position, Claim, Claim, Claim, Claim, Evidenc...   \n\n                                              starts  \\\n0  [0.0, 170.0, 358.0, 438.0, 627.0, 722.0, 836.0...   \n1  [0.0, 456.0, 638.0, 738.0, 1399.0, 1488.0, 231...   \n2  [17.0, 64.0, 158.0, 310.0, 438.0, 551.0, 776.0...   \n3  [0.0, 161.0, 872.0, 958.0, 1191.0, 1542.0, 161...   \n4  [0.0, 58.0, 94.0, 206.0, 236.0, 272.0, 542.0, ...   \n\n                                                ends  \\\n0  [170.0, 357.0, 438.0, 626.0, 722.0, 836.0, 101...   \n1  [455.0, 592.0, 738.0, 1398.0, 1487.0, 2219.0, ...   \n2  [56.0, 157.0, 309.0, 422.0, 551.0, 775.0, 961....   \n3  [160.0, 872.0, 957.0, 1190.0, 1541.0, 1612.0, ...   \n4  [57.0, 91.0, 150.0, 235.0, 271.0, 542.0, 650.0...   \n\n                                   predictionstrings  \\\n0  [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1...   \n1  [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1...   \n2  [2 3 4 5 6 7 8, 10 11 12 13 14 15 16 17 18 19 ...   \n3  [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1...   \n4  [0 1 2 3 4 5 6 7 8 9, 10 11 12 13 14 15, 16 17...   \n\n                                                text  \n0  Some people belive that the so called \"face\" o...  \n1  Driverless cars are exaclty what you would exp...  \n2  Dear: Principal\\n\\nI am arguing against the po...  \n3  Would you be able to give your car up? Having ...  \n4  I think that students would benefit from learn...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>classlist</th>\n      <th>starts</th>\n      <th>ends</th>\n      <th>predictionstrings</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000D23A521A</td>\n      <td>[Position, Evidence, Evidence, Claim, Counterc...</td>\n      <td>[0.0, 170.0, 358.0, 438.0, 627.0, 722.0, 836.0...</td>\n      <td>[170.0, 357.0, 438.0, 626.0, 722.0, 836.0, 101...</td>\n      <td>[0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1...</td>\n      <td>Some people belive that the so called \"face\" o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00066EA9880D</td>\n      <td>[Lead, Position, Claim, Evidence, Claim, Evide...</td>\n      <td>[0.0, 456.0, 638.0, 738.0, 1399.0, 1488.0, 231...</td>\n      <td>[455.0, 592.0, 738.0, 1398.0, 1487.0, 2219.0, ...</td>\n      <td>[0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1...</td>\n      <td>Driverless cars are exaclty what you would exp...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000E6DE9E817</td>\n      <td>[Position, Counterclaim, Rebuttal, Evidence, C...</td>\n      <td>[17.0, 64.0, 158.0, 310.0, 438.0, 551.0, 776.0...</td>\n      <td>[56.0, 157.0, 309.0, 422.0, 551.0, 775.0, 961....</td>\n      <td>[2 3 4 5 6 7 8, 10 11 12 13 14 15 16 17 18 19 ...</td>\n      <td>Dear: Principal\\n\\nI am arguing against the po...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>001552828BD0</td>\n      <td>[Lead, Evidence, Claim, Claim, Evidence, Claim...</td>\n      <td>[0.0, 161.0, 872.0, 958.0, 1191.0, 1542.0, 161...</td>\n      <td>[160.0, 872.0, 957.0, 1190.0, 1541.0, 1612.0, ...</td>\n      <td>[0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1...</td>\n      <td>Would you be able to give your car up? Having ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016926B079C</td>\n      <td>[Position, Claim, Claim, Claim, Claim, Evidenc...</td>\n      <td>[0.0, 58.0, 94.0, 206.0, 236.0, 272.0, 542.0, ...</td>\n      <td>[57.0, 91.0, 150.0, 235.0, 271.0, 542.0, 650.0...</td>\n      <td>[0 1 2 3 4 5 6 7 8 9, 10 11 12 13 14 15, 16 17...</td>\n      <td>I think that students would benefit from learn...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# debugging\n",
    "if SAMPLE: df = df.sample(n=100).reset_index(drop=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:24.831063Z",
     "iopub.execute_input": "2021-12-23T23:00:24.831421Z",
     "iopub.status.idle": "2021-12-23T23:00:25.596595Z",
     "shell.execute_reply.started": "2021-12-23T23:00:24.831375Z",
     "shell.execute_reply": "2021-12-23T23:00:25.595633Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# we will use HuggingFace datasets\n",
    "from datasets import Dataset, load_metric\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "datasets = ds.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "datasets"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:25.59961Z",
     "iopub.execute_input": "2021-12-23T23:00:25.600322Z",
     "iopub.status.idle": "2021-12-23T23:00:26.415085Z",
     "shell.execute_reply.started": "2021-12-23T23:00:25.600259Z",
     "shell.execute_reply": "2021-12-23T23:00:26.413987Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'classlist', 'starts', 'ends', 'predictionstrings', 'text', '__index_level_0__'],\n        num_rows: 14034\n    })\n    test: Dataset({\n        features: ['id', 'classlist', 'starts', 'ends', 'predictionstrings', 'text', '__index_level_0__'],\n        num_rows: 1560\n    })\n})"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:26.416852Z",
     "iopub.execute_input": "2021-12-23T23:00:26.417192Z",
     "iopub.status.idle": "2021-12-23T23:00:31.722501Z",
     "shell.execute_reply.started": "2021-12-23T23:00:26.417127Z",
     "shell.execute_reply": "2021-12-23T23:00:31.721572Z"
    },
    "trusted": true
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Not sure if this is needed, but in case we create a span with certain class without starting token of that class,\n",
    "# let's convert the first token to be the starting token.\n",
    "\n",
    "e = [0,7,7,7,1,1,8,8,8,9,9,9,14,4,4,4]\n",
    "\n",
    "def fix_beginnings(labels):\n",
    "    for i in range(1,len(labels)):\n",
    "        curr_lab = labels[i]\n",
    "        prev_lab = labels[i-1]\n",
    "        if curr_lab in range(7,14):\n",
    "            if prev_lab != curr_lab and prev_lab != curr_lab - 7:\n",
    "                labels[i] = curr_lab -7\n",
    "    return labels\n",
    "\n",
    "fix_beginnings(e)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:31.724112Z",
     "iopub.execute_input": "2021-12-23T23:00:31.724482Z",
     "iopub.status.idle": "2021-12-23T23:00:32.494243Z",
     "shell.execute_reply.started": "2021-12-23T23:00:31.724438Z",
     "shell.execute_reply": "2021-12-23T23:00:32.49297Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 7, 7, 7, 1, 1, 8, 8, 8, 2, 9, 9, 14, 4, 4, 4]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# tokenize and add labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "\n",
    "    o = tokenizer(examples['text'], truncation=True, padding=True, return_offsets_mapping=True, max_length=max_length, stride=stride, return_overflowing_tokens=True)\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = o[\"overflow_to_sample_mapping\"]\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = o[\"offset_mapping\"]\n",
    "    \n",
    "    o[\"labels\"] = []\n",
    "\n",
    "    for i in range(len(offset_mapping)):\n",
    "                   \n",
    "        sample_index = sample_mapping[i]\n",
    "\n",
    "        labels = [l2i['O'] for i in range(len(o['input_ids'][i]))]\n",
    "\n",
    "        for label_start, label_end, label in \\\n",
    "        list(zip(examples['starts'][sample_index], examples['ends'][sample_index], examples['classlist'][sample_index])):\n",
    "            for j in range(len(labels)):\n",
    "                token_start = offset_mapping[i][j][0]\n",
    "                token_end = offset_mapping[i][j][1]\n",
    "                if token_start == label_start: \n",
    "                    labels[j] = l2i[f'B-{label}']    \n",
    "                if token_start > label_start and token_end <= label_end: \n",
    "                    labels[j] = l2i[f'I-{label}']\n",
    "\n",
    "        for k, input_id in enumerate(o['input_ids'][i]):\n",
    "            if input_id in [0,1,2]:\n",
    "                labels[k] = -100\n",
    "\n",
    "        labels = fix_beginnings(labels)\n",
    "                   \n",
    "        o[\"labels\"].append(labels)\n",
    "        \n",
    "    return o"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:32.495836Z",
     "iopub.execute_input": "2021-12-23T23:00:32.496208Z",
     "iopub.status.idle": "2021-12-23T23:00:33.263669Z",
     "shell.execute_reply.started": "2021-12-23T23:00:32.49614Z",
     "shell.execute_reply": "2021-12-23T23:00:33.262629Z"
    },
    "trusted": true
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True, batch_size=20000, remove_columns=datasets[\"train\"].column_names)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:33.265142Z",
     "iopub.execute_input": "2021-12-23T23:00:33.265646Z",
     "iopub.status.idle": "2021-12-23T23:00:35.856612Z",
     "shell.execute_reply.started": "2021-12-23T23:00:33.265601Z",
     "shell.execute_reply": "2021-12-23T23:00:35.855589Z"
    },
    "trusted": true
   },
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe5a2199417045adbd4062bac3dbd387"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "660c193207f348ddb6bfb47847add939"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tokenized_datasets"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:35.858326Z",
     "iopub.execute_input": "2021-12-23T23:00:35.858635Z",
     "iopub.status.idle": "2021-12-23T23:00:36.592654Z",
     "shell.execute_reply.started": "2021-12-23T23:00:35.85859Z",
     "shell.execute_reply": "2021-12-23T23:00:36.591606Z"
    },
    "trusted": true
   },
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['attention_mask', 'input_ids', 'labels', 'offset_mapping', 'overflow_to_sample_mapping'],\n        num_rows: 14574\n    })\n    test: Dataset({\n        features: ['attention_mask', 'input_ids', 'labels', 'offset_mapping', 'overflow_to_sample_mapping'],\n        num_rows: 1625\n    })\n})"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model and Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# we will use auto model for token classification\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=N_LABELS)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:36.59433Z",
     "iopub.execute_input": "2021-12-23T23:00:36.594634Z",
     "iopub.status.idle": "2021-12-23T23:00:40.685632Z",
     "shell.execute_reply.started": "2021-12-23T23:00:36.594593Z",
     "shell.execute_reply": "2021-12-23T23:00:40.684693Z"
    },
    "trusted": true
   },
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at longformer-base-4096-hf were not used when initializing LongformerForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LongformerForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForTokenClassification were not initialized from the model checkpoint at longformer-base-4096-hf and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    logging_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BS,\n",
    "    per_device_eval_batch_size=BS,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    weight_decay=WD,\n",
    "    report_to='wandb', \n",
    "    gradient_accumulation_steps=GRAD_ACC,\n",
    "    warmup_ratio=WARMUP\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:40.690854Z",
     "iopub.execute_input": "2021-12-23T23:00:40.693718Z",
     "iopub.status.idle": "2021-12-23T23:00:41.535273Z",
     "shell.execute_reply.started": "2021-12-23T23:00:40.693672Z",
     "shell.execute_reply": "2021-12-23T23:00:41.534215Z"
    },
    "trusted": true
   },
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noone/anaconda3/envs/pytorch_NLP/lib/python3.9/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at  /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1640869844479/work/c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:41.53676Z",
     "iopub.execute_input": "2021-12-23T23:00:41.537608Z",
     "iopub.status.idle": "2021-12-23T23:00:42.282789Z",
     "shell.execute_reply.started": "2021-12-23T23:00:41.537572Z",
     "shell.execute_reply": "2021-12-23T23:00:42.281853Z"
    },
    "trusted": true
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# this is not the competition metric, but for now this will be better than nothing...\n",
    "\n",
    "metric = load_metric(\"seqeval\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:42.284192Z",
     "iopub.execute_input": "2021-12-23T23:00:42.284501Z",
     "iopub.status.idle": "2021-12-23T23:00:43.656933Z",
     "shell.execute_reply.started": "2021-12-23T23:00:42.284458Z",
     "shell.execute_reply": "2021-12-23T23:00:43.655937Z"
    },
    "trusted": true
   },
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/2.48k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fcb12d992ed34a97a9e41bd2a602cfdc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [i2l[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [i2l[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:43.658571Z",
     "iopub.execute_input": "2021-12-23T23:00:43.658881Z",
     "iopub.status.idle": "2021-12-23T23:00:44.386693Z",
     "shell.execute_reply.started": "2021-12-23T23:00:43.658824Z",
     "shell.execute_reply": "2021-12-23T23:00:44.385607Z"
    },
    "trusted": true
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, \n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:44.388421Z",
     "iopub.execute_input": "2021-12-23T23:00:44.388744Z",
     "iopub.status.idle": "2021-12-23T23:00:45.313179Z",
     "shell.execute_reply.started": "2021-12-23T23:00:44.38869Z",
     "shell.execute_reply": "2021-12-23T23:00:45.312215Z"
    },
    "trusted": true
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")  # new addition\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()\n",
    "wandb.log()  # new additions\n",
    "wandb.watch(model)  # new additions\n",
    "wandb.finish()  #"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:00:45.314663Z",
     "iopub.execute_input": "2021-12-23T23:00:45.318411Z",
     "iopub.status.idle": "2021-12-23T23:03:13.651205Z",
     "shell.execute_reply.started": "2021-12-23T23:00:45.318345Z",
     "shell.execute_reply": "2021-12-23T23:03:13.650259Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `LongformerForTokenClassification.forward` and have been ignored: overflow_to_sample_mapping, offset_mapping.\n",
      "/home/noone/anaconda3/envs/pytorch_NLP/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14574\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 2275\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='2275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/2275 : < :, Epoch 0.00/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.save_model(model_path)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:13.656546Z",
     "iopub.execute_input": "2021-12-23T23:03:13.656788Z",
     "iopub.status.idle": "2021-12-23T23:03:15.317965Z",
     "shell.execute_reply.started": "2021-12-23T23:03:13.656757Z",
     "shell.execute_reply": "2021-12-23T23:03:15.316868Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def tokenize_for_validation(examples):\n",
    "\n",
    "    o = tokenizer(examples['text'], truncation=True, return_offsets_mapping=True, max_length=4096)\n",
    "\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = o[\"offset_mapping\"]\n",
    "    \n",
    "    o[\"labels\"] = []\n",
    "\n",
    "    for i in range(len(offset_mapping)):\n",
    "                   \n",
    "        labels = [l2i['O'] for i in range(len(o['input_ids'][i]))]\n",
    "\n",
    "        for label_start, label_end, label in \\\n",
    "        list(zip(examples['starts'][i], examples['ends'][i], examples['classlist'][i])):\n",
    "            for j in range(len(labels)):\n",
    "                token_start = offset_mapping[i][j][0]\n",
    "                token_end = offset_mapping[i][j][1]\n",
    "                if token_start == label_start: \n",
    "                    labels[j] = l2i[f'B-{label}']    \n",
    "                if token_start > label_start and token_end <= label_end: \n",
    "                    labels[j] = l2i[f'I-{label}']\n",
    "\n",
    "        for k, input_id in enumerate(o['input_ids'][i]):\n",
    "            if input_id in [0,1,2]:\n",
    "                labels[k] = -100\n",
    "\n",
    "        labels = fix_beginnings(labels)\n",
    "                   \n",
    "        o[\"labels\"].append(labels)\n",
    "        \n",
    "    return o"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:15.31952Z",
     "iopub.execute_input": "2021-12-23T23:03:15.319834Z",
     "iopub.status.idle": "2021-12-23T23:03:15.332639Z",
     "shell.execute_reply.started": "2021-12-23T23:03:15.319782Z",
     "shell.execute_reply": "2021-12-23T23:03:15.331235Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenized_val = datasets.map(tokenize_for_validation, batched=True)\n",
    "tokenized_val"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:15.334494Z",
     "iopub.execute_input": "2021-12-23T23:03:15.335669Z",
     "iopub.status.idle": "2021-12-23T23:03:16.652272Z",
     "shell.execute_reply.started": "2021-12-23T23:03:15.335596Z",
     "shell.execute_reply": "2021-12-23T23:03:16.651209Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ground truth for validation\n",
    "\n",
    "l = []\n",
    "for example in tokenized_val['test']:\n",
    "    for c, p in list(zip(example['classlist'], example['predictionstrings'])):\n",
    "        l.append({\n",
    "            'id': example['id'],\n",
    "            'discourse_type': c,\n",
    "            'predictionstring': p,\n",
    "        })\n",
    "    \n",
    "gt_df = pd.DataFrame(l)\n",
    "gt_df"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:16.654017Z",
     "iopub.execute_input": "2021-12-23T23:03:16.654625Z",
     "iopub.status.idle": "2021-12-23T23:03:16.711036Z",
     "shell.execute_reply.started": "2021-12-23T23:03:16.654567Z",
     "shell.execute_reply": "2021-12-23T23:03:16.710012Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# visualization with displacy\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from pylab import cm, matplotlib"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:16.712458Z",
     "iopub.execute_input": "2021-12-23T23:03:16.713221Z",
     "iopub.status.idle": "2021-12-23T23:03:16.719502Z",
     "shell.execute_reply.started": "2021-12-23T23:03:16.713168Z",
     "shell.execute_reply": "2021-12-23T23:03:16.718212Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "path = Path('../input/feedback-prize-2021/train')\n",
    "\n",
    "colors = {\n",
    "            'Lead': '#8000ff',\n",
    "            'Position': '#2b7ff6',\n",
    "            'Evidence': '#2adddd',\n",
    "            'Claim': '#80ffb4',\n",
    "            'Concluding Statement': 'd4dd80',\n",
    "            'Counterclaim': '#ff8042',\n",
    "            'Rebuttal': '#ff0000',\n",
    "            'Other': '#007f00',\n",
    "         }\n",
    "\n",
    "def visualize(df, text):\n",
    "    ents = []\n",
    "    example = df['id'].loc[0]\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        ents.append({\n",
    "                        'start': int(row['discourse_start']), \n",
    "                         'end': int(row['discourse_end']), \n",
    "                         'label': row['discourse_type']\n",
    "                    })\n",
    "\n",
    "    doc2 = {\n",
    "        \"text\": text,\n",
    "        \"ents\": ents,\n",
    "        \"title\": example\n",
    "    }\n",
    "\n",
    "    options = {\"ents\": train.discourse_type.unique().tolist() + ['Other'], \"colors\": colors}\n",
    "    displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:16.721142Z",
     "iopub.execute_input": "2021-12-23T23:03:16.721798Z",
     "iopub.status.idle": "2021-12-23T23:03:16.733508Z",
     "shell.execute_reply.started": "2021-12-23T23:03:16.721753Z",
     "shell.execute_reply": "2021-12-23T23:03:16.732443Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_val['test'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:16.735115Z",
     "iopub.execute_input": "2021-12-23T23:03:16.736247Z",
     "iopub.status.idle": "2021-12-23T23:03:17.621012Z",
     "shell.execute_reply.started": "2021-12-23T23:03:16.736199Z",
     "shell.execute_reply": "2021-12-23T23:03:17.619921Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "preds = np.argmax(predictions, axis=-1)\n",
    "preds.shape"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:17.622787Z",
     "iopub.execute_input": "2021-12-23T23:03:17.623357Z",
     "iopub.status.idle": "2021-12-23T23:03:17.632659Z",
     "shell.execute_reply.started": "2021-12-23T23:03:17.623297Z",
     "shell.execute_reply": "2021-12-23T23:03:17.631425Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# code that will convert our predictions into prediction strings, and visualize it at the same time this most likely requires some refactoring\n",
    "# mk: ummmm yeah....\n",
    "\n",
    "def get_class(c):\n",
    "    if c == 14: return 'Other'\n",
    "    else: return i2l[c][2:]\n",
    "\n",
    "def pred2span(pred, example, viz=False, test=False):\n",
    "    example_id = example['id']\n",
    "    n_tokens = len(example['input_ids'])\n",
    "    classes = []\n",
    "    all_span = []\n",
    "    for i, c in enumerate(pred.tolist()):\n",
    "        if i == n_tokens-1:\n",
    "            break\n",
    "        if i == 0:\n",
    "            cur_span = example['offset_mapping'][i]\n",
    "            classes.append(get_class(c))\n",
    "        elif i > 0 and (c == pred[i-1] or (c-7) == pred[i-1]):\n",
    "            cur_span[1] = example['offset_mapping'][i][1]\n",
    "        else:\n",
    "            all_span.append(cur_span)\n",
    "            cur_span = example['offset_mapping'][i]\n",
    "            classes.append(get_class(c))\n",
    "    all_span.append(cur_span)\n",
    "    \n",
    "    if test: text = get_test_text(example_id)  # something wrong here\n",
    "    else: text = get_raw_text(example_id)\n",
    "    \n",
    "    # abra ka dabra se soli fanta ko pelo\n",
    "    \n",
    "    # map token ids to word (whitespace) token ids\n",
    "    predstrings = []\n",
    "    for span in all_span:\n",
    "        span_start = span[0]\n",
    "        span_end = span[1]\n",
    "        before = text[:span_start]\n",
    "        token_start = len(before.split())\n",
    "        if len(before) == 0: token_start = 0\n",
    "        elif before[-1] != ' ': token_start -= 1\n",
    "        num_tkns = len(text[span_start:span_end+1].split())\n",
    "        tkns = [str(x) for x in range(token_start, token_start+num_tkns)]\n",
    "        predstring = ' '.join(tkns)\n",
    "        predstrings.append(predstring)\n",
    "                    \n",
    "    rows = []\n",
    "    for c, span, predstring in zip(classes, all_span, predstrings):\n",
    "        e = {\n",
    "            'id': example_id,\n",
    "            'discourse_type': c,\n",
    "            'predictionstring': predstring,\n",
    "            'discourse_start': span[0],\n",
    "            'discourse_end': span[1],\n",
    "            'discourse': text[span[0]:span[1]+1]\n",
    "        }\n",
    "        rows.append(e)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df['length'] = df['discourse'].apply(lambda t: len(t.split()))\n",
    "    \n",
    "    # short spans are likely to be false positives, we can choose a min number of tokens based on validation\n",
    "    df = df[df.length > min_tokens].reset_index(drop=True)\n",
    "    if viz: visualize(df, text)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:17.634765Z",
     "iopub.execute_input": "2021-12-23T23:03:17.63535Z",
     "iopub.status.idle": "2021-12-23T23:03:17.655065Z",
     "shell.execute_reply.started": "2021-12-23T23:03:17.635228Z",
     "shell.execute_reply": "2021-12-23T23:03:17.653955Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "pred2span(preds[0], tokenized_val['test'][0], viz=True)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:17.658868Z",
     "iopub.execute_input": "2021-12-23T23:03:17.659221Z",
     "iopub.status.idle": "2021-12-23T23:03:17.712976Z",
     "shell.execute_reply.started": "2021-12-23T23:03:17.659184Z",
     "shell.execute_reply": "2021-12-23T23:03:17.711747Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "pred2span(preds[1], tokenized_val['test'][1], viz=True)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:17.71609Z",
     "iopub.execute_input": "2021-12-23T23:03:17.716626Z",
     "iopub.status.idle": "2021-12-23T23:03:17.757272Z",
     "shell.execute_reply.started": "2021-12-23T23:03:17.716588Z",
     "shell.execute_reply": "2021-12-23T23:03:17.756227Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "dfs = []\nfor i in range(len(tokenized_val['test'])):\n    dfs.append(pred2span(preds[i], tokenized_val['test'][i]))\n\npred_df = pd.concat(dfs, axis=0)\npred_df['class'] = pred_df['discourse_type']\npred_df",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:17.759337Z",
     "iopub.execute_input": "2021-12-23T23:03:17.760071Z",
     "iopub.status.idle": "2021-12-23T23:03:17.883329Z",
     "shell.execute_reply.started": "2021-12-23T23:03:17.760003Z",
     "shell.execute_reply": "2021-12-23T23:03:17.8822Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# source: https://www.kaggle.com/robikscube/student-writing-competition-twitch#Competition-Metric-Code\n\ndef calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(\" \"))\n    set_gt = set(row.predictionstring_gt.split(\" \"))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter / len_gt\n    overlap_2 = inter / len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp_micro(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n\n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = (\n        gt_df[[\"id\", \"discourse_type\", \"predictionstring\"]]\n        .reset_index(drop=True)\n        .copy()\n    )\n    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n    pred_df[\"pred_id\"] = pred_df.index\n    gt_df[\"gt_id\"] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(\n        gt_df,\n        left_on=[\"id\", \"class\"],\n        right_on=[\"id\", \"discourse_type\"],\n        how=\"outer\",\n        suffixes=(\"_pred\", \"_gt\"),\n    )\n    joined[\"predictionstring_gt\"] = joined[\"predictionstring_gt\"].fillna(\" \")\n    joined[\"predictionstring_pred\"] = joined[\"predictionstring_pred\"].fillna(\" \")\n\n    joined[\"overlaps\"] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined[\"overlap1\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[0])\n    joined[\"overlap2\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[1])\n\n    joined[\"potential_TP\"] = (joined[\"overlap1\"] >= 0.5) & (joined[\"overlap2\"] >= 0.5)\n    joined[\"max_overlap\"] = joined[[\"overlap1\", \"overlap2\"]].max(axis=1)\n    tp_pred_ids = (\n        joined.query(\"potential_TP\")\n        .sort_values(\"max_overlap\", ascending=False)\n        .groupby([\"id\", \"predictionstring_gt\"])\n        .first()[\"pred_id\"]\n        .values\n    )\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined[\"pred_id\"].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query(\"potential_TP\")[\"gt_id\"].unique()\n    unmatched_gt_ids = [c for c in joined[\"gt_id\"].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    # calc microf1\n    my_f1_score = TP / (TP + 0.5 * (FP + FN))\n    return my_f1_score\n\n\ndef score_feedback_comp(pred_df, gt_df, return_class_scores=False):\n    class_scores = {}\n    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n    for discourse_type, gt_subset in gt_df.groupby(\"discourse_type\"):\n        pred_subset = (\n            pred_df.loc[pred_df[\"class\"] == discourse_type]\n            .reset_index(drop=True)\n            .copy()\n        )\n        class_score = score_feedback_comp_micro(pred_subset, gt_subset)\n        class_scores[discourse_type] = class_score\n    f1 = np.mean([v for v in class_scores.values()])\n    if return_class_scores:\n        return f1, class_scores\n    return f1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:17.885121Z",
     "iopub.execute_input": "2021-12-23T23:03:17.885735Z",
     "iopub.status.idle": "2021-12-23T23:03:17.908285Z",
     "shell.execute_reply.started": "2021-12-23T23:03:17.88567Z",
     "shell.execute_reply": "2021-12-23T23:03:17.907198Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## CV Score",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "score_feedback_comp(pred_df, gt_df, return_class_scores=True)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-23T23:03:17.910018Z",
     "iopub.execute_input": "2021-12-23T23:03:17.910701Z",
     "iopub.status.idle": "2021-12-23T23:03:18.110011Z",
     "shell.execute_reply.started": "2021-12-23T23:03:17.910652Z",
     "shell.execute_reply": "2021-12-23T23:03:18.108723Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## End\n\nI'll appreciate every upvote or comment!",
   "metadata": {}
  }
 ]
}