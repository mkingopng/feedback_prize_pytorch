{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"tez\")\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tez\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from joblib import Parallel, delayed\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2022-02-23T03:55:07.129053Z",
     "iopub.execute_input": "2022-02-23T03:55:07.129638Z",
     "iopub.status.idle": "2022-02-23T03:55:09.119692Z",
     "shell.execute_reply.started": "2022-02-23T03:55:07.129602Z",
     "shell.execute_reply": "2022-02-23T03:55:09.11895Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "target_id_map = {\n",
    "    \"B-Lead\": 0,\n",
    "    \"I-Lead\": 1,\n",
    "    \"B-Position\": 2,\n",
    "    \"I-Position\": 3,\n",
    "    \"B-Evidence\": 4,\n",
    "    \"I-Evidence\": 5,\n",
    "    \"B-Claim\": 6,\n",
    "    \"I-Claim\": 7,\n",
    "    \"B-Concluding Statement\": 8,\n",
    "    \"I-Concluding Statement\": 9,\n",
    "    \"B-Counterclaim\": 10,\n",
    "    \"I-Counterclaim\": 11,\n",
    "    \"B-Rebuttal\": 12,\n",
    "    \"I-Rebuttal\": 13,\n",
    "    \"O\": 14,\n",
    "    \"PAD\": -100,\n",
    "}\n",
    "\n",
    "\n",
    "id_target_map = {v: k for k, v in target_id_map.items()}\n",
    "\n",
    "class args1:\n",
    "    input_path = \"\"\n",
    "    model = \"longformer\"\n",
    "    tez_model= \"curiousoath/\"\n",
    "    output = \".\"\n",
    "    batch_size = 8\n",
    "    max_len = 4096\n",
    "    \n",
    "class args2:\n",
    "    input_path = \"\"\n",
    "    model = \"longformer\"\n",
    "    tez_model= \"visionary-cherry/\"\n",
    "    output = \".\"\n",
    "    batch_size = 8\n",
    "    max_len = 4096\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-23T03:55:09.123159Z",
     "iopub.execute_input": "2022-02-23T03:55:09.123356Z",
     "iopub.status.idle": "2022-02-23T03:55:09.132333Z",
     "shell.execute_reply.started": "2022-02-23T03:55:09.123333Z",
     "shell.execute_reply": "2022-02-23T03:55:09.131475Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class FeedbackDataset:\n",
    "    def __init__(self, samples, max_len, tokenizer):\n",
    "        self.samples = samples\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.length = len(samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.samples[idx][\"input_ids\"]\n",
    "        # print(input_ids)\n",
    "        # print(input_labels)\n",
    "\n",
    "        # add start token id to the input_ids\n",
    "        input_ids = [self.tokenizer.cls_token_id] + input_ids\n",
    "\n",
    "        if len(input_ids) > self.max_len - 1:\n",
    "            input_ids = input_ids[: self.max_len - 1]\n",
    "\n",
    "        # add end token id to the input_ids\n",
    "        input_ids = input_ids + [self.tokenizer.sep_token_id]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # padding_length = self.max_len - len(input_ids)\n",
    "        # if padding_length > 0:\n",
    "        #     if self.tokenizer.padding_side == \"right\":\n",
    "        #         input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n",
    "        #         attention_mask = attention_mask + [0] * padding_length\n",
    "        #     else:\n",
    "        #         input_ids = [self.tokenizer.pad_token_id] * padding_length + input_ids\n",
    "        #         attention_mask = [0] * padding_length + attention_mask\n",
    "\n",
    "        # return {\n",
    "        #     \"ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        #     \"mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "        # }\n",
    "\n",
    "        return {\n",
    "            \"ids\": input_ids,\n",
    "            \"mask\": attention_mask,\n",
    "        }"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-23T03:55:09.1336Z",
     "iopub.execute_input": "2022-02-23T03:55:09.133915Z",
     "iopub.status.idle": "2022-02-23T03:55:09.143727Z",
     "shell.execute_reply.started": "2022-02-23T03:55:09.133878Z",
     "shell.execute_reply": "2022-02-23T03:55:09.142808Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Collate:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n",
    "        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n",
    "            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n",
    "        else:\n",
    "            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n",
    "            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n",
    "        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-23T03:55:09.145167Z",
     "iopub.execute_input": "2022-02-23T03:55:09.145682Z",
     "iopub.status.idle": "2022-02-23T03:55:09.156608Z",
     "shell.execute_reply.started": "2022-02-23T03:55:09.145645Z",
     "shell.execute_reply": "2022-02-23T03:55:09.155874Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class FeedbackModel(tez.Model):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.num_labels = num_labels\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "        hidden_dropout_prob: float = 0.1\n",
    "        layer_norm_eps: float = 1e-7\n",
    "        config.update(\n",
    "            {\n",
    "                \"output_hidden_states\": True,\n",
    "                \"hidden_dropout_prob\": hidden_dropout_prob,\n",
    "                \"layer_norm_eps\": layer_norm_eps,\n",
    "                \"add_pooling_layer\": False,\n",
    "            }\n",
    "        )\n",
    "        self.transformer = AutoModel.from_config(config)\n",
    "        self.output = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        transformer_out = self.transformer(ids, mask)\n",
    "        sequence_output = transformer_out.last_hidden_state\n",
    "        logits = self.output(sequence_output)\n",
    "        logits = torch.softmax(logits, dim=-1)\n",
    "        return logits, 0, {}"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-23T03:55:09.159075Z",
     "iopub.execute_input": "2022-02-23T03:55:09.159392Z",
     "iopub.status.idle": "2022-02-23T03:55:09.169511Z",
     "shell.execute_reply.started": "2022-02-23T03:55:09.159353Z",
     "shell.execute_reply": "2022-02-23T03:55:09.168689Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def _prepare_test_data_helper(args, tokenizer, ids):\n",
    "    test_samples = []\n",
    "    for idx in ids:\n",
    "        filename = os.path.join(args.input_path, \"test\", idx + \".txt\")\n",
    "        with open(filename, \"r\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        input_ids = encoded_text[\"input_ids\"]\n",
    "        offset_mapping = encoded_text[\"offset_mapping\"]\n",
    "\n",
    "        sample = {\n",
    "            \"id\": idx,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"text\": text,\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "        }\n",
    "\n",
    "        test_samples.append(sample)\n",
    "    return test_samples\n",
    "\n",
    "\n",
    "def prepare_test_data(df, tokenizer, args):\n",
    "    test_samples = []\n",
    "    ids = df[\"id\"].unique()\n",
    "    ids_splits = np.array_split(ids, 4)\n",
    "\n",
    "    results = Parallel(n_jobs=4, backend=\"multiprocessing\")(\n",
    "        delayed(_prepare_test_data_helper)(args, tokenizer, idx) for idx in ids_splits\n",
    "    )\n",
    "    for result in results:\n",
    "        test_samples.extend(result)\n",
    "\n",
    "    return test_samples"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-23T03:55:09.170915Z",
     "iopub.execute_input": "2022-02-23T03:55:09.171165Z",
     "iopub.status.idle": "2022-02-23T03:55:09.180667Z",
     "shell.execute_reply.started": "2022-02-23T03:55:09.171134Z",
     "shell.execute_reply": "2022-02-23T03:55:09.179834Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(os.path.join(\"data\", \"sample_submission.csv\"))\n",
    "df_ids = df[\"id\"].unique()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args1.model)\n",
    "test_samples = prepare_test_data(df, tokenizer, args1)\n",
    "collate = Collate(tokenizer=tokenizer)\n",
    "\n",
    "raw_preds = []\n",
    "for fold_ in range(10):\n",
    "    current_idx = 0\n",
    "    test_dataset = FeedbackDataset(test_samples, args1.max_len, tokenizer)\n",
    "    \n",
    "    if fold_ < 5:\n",
    "        model = FeedbackModel(model_name=args1.model, num_labels=len(target_id_map) - 1)\n",
    "        model.load(os.path.join(args1.tez_model, f\"model_{fold_}.bin\"), weights_only=True)\n",
    "        preds_iter = model.predict(test_dataset, batch_size=args1.batch_size, n_jobs=-1, collate_fn=collate)\n",
    "    else:\n",
    "        model = FeedbackModel(model_name=args2.model, num_labels=len(target_id_map) - 1)\n",
    "        model.load(os.path.join(args2.tez_model, f\"model_{fold_-5}.bin\"), weights_only=True)\n",
    "        preds_iter = model.predict(test_dataset, batch_size=args2.batch_size, n_jobs=-1, collate_fn=collate)\n",
    "        \n",
    "    current_idx = 0\n",
    "    \n",
    "    for preds in preds_iter:\n",
    "        preds = preds.astype(np.float16)\n",
    "        preds = preds / 10\n",
    "        if fold_ == 0:\n",
    "            raw_preds.append(preds)\n",
    "        else:\n",
    "            raw_preds[current_idx] += preds\n",
    "            current_idx += 1\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-23T03:55:09.182039Z",
     "iopub.execute_input": "2022-02-23T03:55:09.18282Z",
     "iopub.status.idle": "2022-02-23T03:56:32.15957Z",
     "shell.execute_reply.started": "2022-02-23T03:55:09.182785Z",
     "shell.execute_reply": "2022-02-23T03:56:32.15875Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test/18409261F5C2.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRemoteTraceback\u001B[0m                           Traceback (most recent call last)",
      "\u001B[0;31mRemoteTraceback\u001B[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/noone/anaconda3/envs/pytorch_NLP/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/noone/anaconda3/envs/pytorch_NLP/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/noone/anaconda3/envs/pytorch_NLP/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/noone/anaconda3/envs/pytorch_NLP/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/tmp/ipykernel_26216/2607409901.py\", line 5, in _prepare_test_data_helper\n    with open(filename, \"r\") as f:\nFileNotFoundError: [Errno 2] No such file or directory: 'test/18409261F5C2.txt'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_26216/1551646336.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mtokenizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAutoTokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mtest_samples\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprepare_test_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0mcollate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCollate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_26216/2607409901.py\u001B[0m in \u001B[0;36mprepare_test_data\u001B[0;34m(df, tokenizer, args)\u001B[0m\n\u001B[1;32m     30\u001B[0m     \u001B[0mids_splits\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray_split\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m4\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m     results = Parallel(n_jobs=4, backend=\"multiprocessing\")(\n\u001B[0m\u001B[1;32m     33\u001B[0m         \u001B[0mdelayed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_prepare_test_data_helper\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0midx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mids_splits\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m     )\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_NLP/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1054\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1055\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mretrieval_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1056\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mretrieve\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1057\u001B[0m             \u001B[0;31m# Make sure that we get a last message telling us we are done\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1058\u001B[0m             \u001B[0melapsed_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_start_time\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_NLP/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36mretrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    933\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    934\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'supports_timeout'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 935\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_output\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjob\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    936\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    937\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_output\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjob\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch_NLP/lib/python3.9/multiprocessing/pool.py\u001B[0m in \u001B[0;36mget\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    769\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_value\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    770\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 771\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_value\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    772\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    773\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_set\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'test/18409261F5C2.txt'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "final_preds = []\n",
    "final_scores = []\n",
    "\n",
    "for rp in raw_preds:\n",
    "    pred_class = np.argmax(rp, axis=2)\n",
    "    pred_scrs = np.max(rp, axis=2)\n",
    "    for pred, pred_scr in zip(pred_class, pred_scrs):\n",
    "        pred = pred.tolist()\n",
    "        pred_scr = pred_scr.tolist()\n",
    "        final_preds.append(pred)\n",
    "        final_scores.append(pred_scr)\n",
    "\n",
    "for j in range(len(test_samples)):\n",
    "    tt = [id_target_map[p] for p in final_preds[j][1:]]\n",
    "    tt_score = final_scores[j][1:]\n",
    "    test_samples[j][\"preds\"] = tt\n",
    "    test_samples[j][\"pred_scores\"] = tt_score"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-23T03:56:32.161694Z",
     "iopub.execute_input": "2022-02-23T03:56:32.162176Z",
     "iopub.status.idle": "2022-02-23T03:56:32.172894Z",
     "shell.execute_reply.started": "2022-02-23T03:56:32.162136Z",
     "shell.execute_reply": "2022-02-23T03:56:32.172177Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def jn(pst, start, end):\n",
    "    return \" \".join([str(x) for x in pst[start:end]])\n",
    "\n",
    "\n",
    "def link_evidence(oof):\n",
    "    thresh = 1\n",
    "    idu = oof['id'].unique()\n",
    "    idc = idu[1]\n",
    "    eoof = oof[oof['class'] == \"Evidence\"]\n",
    "    neoof = oof[oof['class'] != \"Evidence\"]\n",
    "    for thresh2 in range(26,27, 1):\n",
    "        retval = []\n",
    "        for idv in idu:\n",
    "            for c in  ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n",
    "                   'Counterclaim', 'Rebuttal']:\n",
    "                q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n",
    "                if len(q) == 0:\n",
    "                    continue\n",
    "                pst = []\n",
    "                for i,r in q.iterrows():\n",
    "                    pst = pst +[-1] + [int(x) for x in r['predictionstring'].split()]\n",
    "                start = 1\n",
    "                end = 1\n",
    "                for i in range(2,len(pst)):\n",
    "                    cur = pst[i]\n",
    "                    end = i\n",
    "                    #if pst[start] == 205:\n",
    "                    #   print(cur, pst[start], cur - pst[start])\n",
    "                    if (cur == -1 and c != 'Evidence') or ((cur == -1) and ((pst[i+1] > pst[end-1] + thresh) or (pst[i+1] - pst[start] > thresh2))):\n",
    "                        retval.append((idv, c, jn(pst, start, end)))\n",
    "                        start = i + 1\n",
    "                v = (idv, c, jn(pst, start, end+1))\n",
    "                #print(v)\n",
    "                retval.append(v)\n",
    "        roof = pd.DataFrame(retval, columns = ['id', 'class', 'predictionstring']) \n",
    "        roof = roof.merge(neoof, how='outer')\n",
    "        return roof\n",
    "    "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-23T03:56:32.173946Z",
     "iopub.execute_input": "2022-02-23T03:56:32.174201Z",
     "iopub.status.idle": "2022-02-23T03:56:32.187279Z",
     "shell.execute_reply.started": "2022-02-23T03:56:32.174166Z",
     "shell.execute_reply": "2022-02-23T03:56:32.186382Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "proba_thresh = {\n",
    "    \"Lead\": 0.670,  #\n",
    "    \"Position\": 0.520,  #\n",
    "    \"Evidence\": 0.620,  #\n",
    "    \"Claim\": 0.520,  #\n",
    "    \"Concluding Statement\": 0.670,  #\n",
    "    \"Counterclaim\": 0.520,  #\n",
    "    \"Rebuttal\": 0.520,  #\n",
    "}\n",
    "\n",
    "min_thresh = {\n",
    "    \"Lead\": 9,\n",
    "    \"Position\": 5,\n",
    "    \"Evidence\": 14,\n",
    "    \"Claim\": 3,\n",
    "    \"Concluding Statement\": 11,\n",
    "    \"Counterclaim\": 6,\n",
    "    \"Rebuttal\": 4,\n",
    "}\n",
    "\n",
    "submission = []\n",
    "for sample_idx, sample in enumerate(test_samples):\n",
    "    preds = sample[\"preds\"]\n",
    "    offset_mapping = sample[\"offset_mapping\"]\n",
    "    sample_id = sample[\"id\"]\n",
    "    sample_text = sample[\"text\"]\n",
    "    sample_input_ids = sample[\"input_ids\"]\n",
    "    sample_pred_scores = sample[\"pred_scores\"]\n",
    "    sample_preds = []\n",
    "\n",
    "    if len(preds) < len(offset_mapping):\n",
    "        preds = preds + [\"O\"] * (len(offset_mapping) - len(preds))\n",
    "        sample_pred_scores = sample_pred_scores + [0] * (len(offset_mapping) - len(sample_pred_scores))\n",
    "    \n",
    "    idx = 0\n",
    "    phrase_preds = []\n",
    "    while idx < len(offset_mapping):\n",
    "        start, _ = offset_mapping[idx]\n",
    "        if preds[idx] != \"O\":\n",
    "            label = preds[idx][2:]\n",
    "        else:\n",
    "            label = \"O\"\n",
    "        phrase_scores = []\n",
    "        phrase_scores.append(sample_pred_scores[idx])\n",
    "        idx += 1\n",
    "        while idx < len(offset_mapping):\n",
    "            if label == \"O\":\n",
    "                matching_label = \"O\"\n",
    "            else:\n",
    "                matching_label = f\"I-{label}\"\n",
    "            if preds[idx] == matching_label:\n",
    "                _, end = offset_mapping[idx]\n",
    "                phrase_scores.append(sample_pred_scores[idx])\n",
    "                idx += 1\n",
    "            else:\n",
    "                break\n",
    "        if \"end\" in locals():\n",
    "            phrase = sample_text[start:end]\n",
    "            phrase_preds.append((phrase, start, end, label, phrase_scores))\n",
    "\n",
    "    temp_df = []\n",
    "    for phrase_idx, (phrase, start, end, label, phrase_scores) in enumerate(phrase_preds):\n",
    "        word_start = len(sample_text[:start].split())\n",
    "        word_end = word_start + len(sample_text[start:end].split())\n",
    "        word_end = min(word_end, len(sample_text.split()))\n",
    "        ps = \" \".join([str(x) for x in range(word_start, word_end)])\n",
    "        if label != \"O\":\n",
    "            if sum(phrase_scores) / len(phrase_scores) >= proba_thresh[label]:\n",
    "                if len(ps.split()) >= min_thresh[label]:\n",
    "                    temp_df.append((sample_id, label, ps))\n",
    "    \n",
    "    temp_df = pd.DataFrame(temp_df, columns=[\"id\", \"class\", \"predictionstring\"])\n",
    "    submission.append(temp_df)\n",
    "\n",
    "submission = pd.concat(submission).reset_index(drop=True)\n",
    "submission = link_evidence(submission)\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-23T03:56:32.188649Z",
     "iopub.execute_input": "2022-02-23T03:56:32.189183Z",
     "iopub.status.idle": "2022-02-23T03:56:32.257087Z",
     "shell.execute_reply.started": "2022-02-23T03:56:32.189145Z",
     "shell.execute_reply": "2022-02-23T03:56:32.256346Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "submission.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-23T03:56:32.258377Z",
     "iopub.execute_input": "2022-02-23T03:56:32.258669Z",
     "iopub.status.idle": "2022-02-23T03:56:32.27364Z",
     "shell.execute_reply.started": "2022-02-23T03:56:32.25863Z",
     "shell.execute_reply": "2022-02-23T03:56:32.272852Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}